@article{ahmed2024,
  title = {Seal {{Pipeline}}: {{Enhancing Dynamic Object Detection}} and {{Tracking}} for {{Autonomous Unmanned Surface Vehicles}} in {{Maritime Environments}}},
  shorttitle = {Seal {{Pipeline}}},
  author = {Ahmed, Mohamed and Rasheed, Bader and Salloum, Hadi and Hegazy, Mostafa and Bahrami, Mohammad Reza and Chuchkalov, Mikhail},
  year = {2024},
  month = oct,
  journal = {Drones},
  volume = {8},
  number = {10},
  pages = {561},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2504-446X},
  doi = {10.3390/drones8100561},
  urldate = {2025-03-14},
  abstract = {This study addresses the dynamic object detection problem for Unmanned Surface Vehicles (USVs) in marine environments, which is complicated by boat tilting and camera illumination sensitivity. A novel pipeline named ``Seal'' is proposed to enhance detection accuracy and reliability. The approach begins with an innovative preprocessing stage that integrates data from the Inertial Measurement Unit (IMU) with LiDAR sensors to correct tilt-induced distortions in LiDAR point cloud data and reduce ripple effects around objects. The adjusted data are grouped using clustering algorithms and bounding boxes for precise object localization. Additionally, a specialized Kalman filter tailored for maritime environments mitigates object discontinuities between successive frames and addresses data sparsity caused by boat tilting. The methodology was evaluated using the VRX simulator, with experiments conducted on the Volga River using real USVs. The preprocessing effectiveness was assessed using the Root Mean Square Error (RMSE) and tracking accuracy was evaluated through detection rate metrics. The results demonstrate a 25\% to 30\% improvement in detection accuracy and show that the pipeline can aid industry even with sparse object representation across different frames. This study highlights the potential of integrating sensor fusion with specialized tracking for accurate dynamic object detection in maritime settings, establishing a new benchmark for USV navigation systems' accuracy and reliability.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {dynamic object detection,Kalman filter,LiDAR point cloud,unmanned surface vehicles},
  file = {/Users/dplane/Zotero/storage/CSB4LEXB/Ahmed et al_2024_Seal Pipeline.pdf}
}

@article{bai2022,
  title = {A {{Review}} of {{Current Research}} and {{Advances}} in {{Unmanned Surface Vehicles}}},
  author = {Bai, Xiangen and Li, Bohan and Xu, Xiaofeng and Xiao, Yingjie},
  year = {2022},
  month = jun,
  journal = {Journal of Marine Science and Application},
  volume = {21},
  number = {2},
  pages = {47--58},
  issn = {1993-5048},
  doi = {10.1007/s11804-022-00276-9},
  urldate = {2025-03-14},
  abstract = {Following developments in artificial intelligence and big data technology, the level of intelligence in intelligent vessels has been improved. Intelligent vessels are being developed into unmanned surface vehicles (USVs), which have widely interested scholars in the shipping industry due to their safety, high efficiency, and energy-saving qualities. Considering the current development of USVs, the types of USVs and applications domestically and internationally are being investigated. USVs emerged with technological developments and their characteristics show some differences from traditional vessels, which brings some problems and advantages for their application. Certain maritime regulations are not applicable to USVs and must be changed. The key technologies in the current development of USVs are being investigated. While the level of intelligence is improving, the protection of cargo cannot be neglected. An innovative approach to the internal structure of USVs is proposed, where the inner hull can automatically recover its original state in case of outer hull tilting. Finally, we summarize the development status of USVs, which are an inevitable direction of development in the marine field.},
  langid = {english},
  keywords = {Artificial Intelligence,Intelligent vessel,Internal structure,Maritime supervision,Ship automation level,Shipping industry,Unmanned surface vehicle},
  file = {/Users/dplane/Zotero/storage/K76MBVJN/Bai et al_2022_A Review of Current Research and Advances in Unmanned Surface Vehicles.pdf}
}

@inproceedings{biasutti2019,
  title = {{{LU-Net}}: {{An Efficient Network}} for {{3D LiDAR Point Cloud Semantic Segmentation Based}} on {{End-to-End-Learned 3D Features}} and {{U-Net}}},
  shorttitle = {{{LU-Net}}},
  booktitle = {2019 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision Workshop}} ({{ICCVW}})},
  author = {Biasutti, Pierre and Lepetit, Vincent and Aujol, Jean-Fran{\c c}ois and Br{\'e}dif, Mathieu and Bugeau, Aur{\'e}lie},
  year = {2019},
  month = oct,
  pages = {942--950},
  issn = {2473-9944},
  doi = {10.1109/ICCVW.2019.00123},
  urldate = {2025-02-17},
  abstract = {We propose LU-Net - for LiDAR U-Net, a new method for the semantic segmentation of a 3D LiDAR point cloud. Instead of applying some global 3D segmentation method such as PointNet, we propose an end-to-end architecture for LiDAR point cloud semantic segmentation that efficiently solves the problem as an image processing problem. We first extract high-level 3D features for each point given its 3D neighbors. Then, these features are projected into a 2D multichannel range-image by considering the topology of the sensor. Thanks to these learned features and this projection, we can finally perform the segmentation using a simple U-Net segmentation network, which performs very well while being very efficient. In this way, we can exploit both the 3D nature of the data and the specificity of the LiDAR sensor. This approach outperforms the state-of-the-art by a large margin on the KITTI dataset, as our experiments show. Moreover, this approach operates at 24fps on a single GPU. This is above the acquisition rate of common LiDAR sensors which makes it suitable for real-time applications.},
  keywords = {3D point cloud,cnn,deep learning,Feature extraction,Image segmentation,Laser radar,LiDAR,Robot sensing systems,semantic segmentation,Semantics,Three-dimensional displays,Two dimensional displays},
  file = {/Users/dplane/Zotero/storage/MJR2QDS3/Biasutti et al. - 2019 - LU-Net An Efficient Network for 3D LiDAR Point Cl.pdf;/Users/dplane/Zotero/storage/ZB336YVH/9022291.html}
}

@inproceedings{carthel2007,
  title = {Multisensor Tracking and Fusion for Maritime Surveillance},
  booktitle = {2007 10th {{International Conference}} on {{Information Fusion}}},
  author = {Carthel, Craig and Coraluppi, Stefano and Grignan, Patrick},
  year = {2007},
  month = jul,
  pages = {1--6},
  doi = {10.1109/ICIF.2007.4408025},
  urldate = {2025-02-15},
  abstract = {Over the past several years, the NATO Undersea Research Centre has conducted extensive research in multisensor networks for undersea surveillance, culminating in the development of the DMHT tracker. In this paper, we discuss upgrades to this technology and its application to maritime surveillance.},
  keywords = {Anomaly Detection,Automatic,Coastal Radar,Fusion,Fusion power generation,Identification System (AIS),Imagery,Maritime Surveillance,multisensor,Multisensor Fusion and Tracking,Radar detection,Radar imaging,Radar tracking,SAR,Sea measurements,Signal processing,Sonar equipment,Surveillance,Synthetic aperture radar,Target tracking,Track},
  file = {/Users/dplane/Zotero/storage/KC479BI2/Carthel et al_2007_Multisensor tracking and fusion for maritime surveillance.pdf;/Users/dplane/Zotero/storage/625RP48V/4408025.html}
}

@inproceedings{chen2017,
  title = {Multi-View {{3D Object Detection Network}} for {{Autonomous Driving}}},
  booktitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Chen, Xiaozhi and Ma, Huimin and Wan, Ji and Li, Bo and Xia, Tian},
  year = {2017},
  month = jul,
  pages = {6526--6534},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2017.691},
  urldate = {2025-03-16},
  abstract = {This paper aims at high-accuracy 3D object detection in autonomous driving scenario. We propose Multi-View 3D networks (MV3D), a sensory-fusion framework that takes both LIDAR point cloud and RGB images as input and predicts oriented 3D bounding boxes. We encode the sparse 3D point cloud with a compact multi-view representation. The network is composed of two subnetworks: one for 3D object proposal generation and another for multi-view feature fusion. The proposal network generates 3D candidate boxes efficiently from the birds eye view representation of 3D point cloud. We design a deep fusion scheme to combine region-wise features from multiple views and enable interactions between intermediate layers of different paths. Experiments on the challenging KITTI benchmark show that our approach outperforms the state-of-the-art by around 25\% and 30\% AP on the tasks of 3D localization and 3D detection. In addition, for 2D detection, our approach obtains 14.9\% higher AP than the state-of-the-art on the hard data among the LIDAR-based methods.},
  keywords = {Birds,Laser radar,Object detection,Proposals,Three-dimensional displays,Two dimensional displays},
  file = {/Users/dplane/Zotero/storage/7495ACUV/Chen et al. - 2017 - Multi-view 3D Object Detection Network for Autonom.pdf;/Users/dplane/Zotero/storage/76YSMPB6/Chen et al_2017_Multi-view 3D Object Detection Network for Autonomous Driving.pdf;/Users/dplane/Zotero/storage/UX966S6R/8100174.html}
}

@inproceedings{clunie2021,
  title = {Development of a {{Perception System}} for an {{Autonomous Surface Vehicle}} Using {{Monocular Camera}}, {{LIDAR}}, and {{Marine RADAR}}},
  booktitle = {2021 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Clunie, Thomas and DeFilippo, Michael and Sacarny, Michael and Robinette, Paul},
  year = {2021},
  month = may,
  pages = {14112--14119},
  issn = {2577-087X},
  doi = {10.1109/ICRA48506.2021.9561275},
  urldate = {2025-02-15},
  abstract = {This paper describes a set of software modules and algorithms for maritime object detection and tracking. The approach described here is designed to work in conjunction with various sensors from a maritime surface vessel (e.g. marine RADAR, LIDAR, camera). The described system identifies obstacles from the input sensors, estimates their state, and fuses the obstacle data into a consolidated report. The system is verified using experiments conducted on a live system and successfully demonstrates the ability to detect and track obstacles up to 450m away while operating at 7 fps. The software is open source and available at https://github.com/uml-marine-robotics/asv\_perception.},
  keywords = {autonomous surface vehicles,Autonomous systems,calibration,Cameras,classification,intelligent robots,Laser radar,LIDAR,marine RADAR,marine robotics,mobile agents unmanned autonomous vehicles,object detection,Object detection,Radar tracking,Robot sensing systems,segmentation,sensor fusion,Sensor fusion,Software algorithms},
  file = {/Users/dplane/Zotero/storage/ELDJNRXK/Clunie et al_2021_Development of a Perception System for an Autonomous Surface Vehicle using.pdf;/Users/dplane/Zotero/storage/DLQXEKQU/9561275.html}
}

@misc{coyleE,
  title = {Efficient {{Grid-Based Clustering}} and {{Concave Hull Extraction}} for {{Unstructured Point Clouds}}},
  author = {Coyle, Eric},
  urldate = {2023-11-09},
  abstract = {This paper presents a situational awareness technique for point clouds, called Grid-Based Clustering and Concave Hull Extraction (GB-CACHE), which is shown to be suitable for real-time implementation. GB-CACHE is able to efficiently segment and extract concave hulls from unstructured point clouds by sub-sampling these point clouds to a structured grid. The technique is specifically design for unmanned systems perceiving objects that are assumed to lie on a flat surface, such as the ground, water, or sea bed. In addition to segmentation and hull extraction, the technique is shown to enable the implementation of optional processes of point filtering, object classification, mapping, and object tracking. Example GB-CACHE results are shown from four separate unmanned case studies covering three different domains (surface, aerial, underwater), using three different sensing modalities (LiDAR, radar, sonar). Dense LiDAR point clouds are then used to analyze the computational efficiency of each main processes of GB-CACHE. Collectively, GB-CACHE is shown to be efficient enough for real-time implementation even with these dense point clouds and low to mid-grade computing solutions.},
  keywords = {Global Positioning System,Laser radar,LiDAR,mapping,object segmentation,Object segmentation,occupancy grid,Path planning,Sea surface,Sensors},
  file = {/Users/dplane/Zotero/storage/P9WUJSX4/Efficient_Grid_Based_Clustering_and_Concave_Hull_Extraction_for_Unsegmented_Point_Clouds.pdf;/Users/dplane/Zotero/storage/VWKZE93P/8656488.html}
}

@article{cui2022,
  title = {Deep {{Learning}} for {{Image}} and {{Point Cloud Fusion}} in {{Autonomous Driving}}: {{A Review}}},
  shorttitle = {Deep {{Learning}} for {{Image}} and {{Point Cloud Fusion}} in {{Autonomous Driving}}},
  author = {Cui, Yaodong and Chen, Ren and Chu, Wenbo and Chen, Long and Tian, Daxin and Li, Ying and Cao, Dongpu},
  year = {2022},
  month = feb,
  journal = {IEEE Transactions on Intelligent Transportation Systems},
  volume = {23},
  number = {2},
  pages = {722--739},
  issn = {1558-0016},
  doi = {10.1109/TITS.2020.3023541},
  urldate = {2025-03-16},
  abstract = {Autonomous vehicles were experiencing rapid development in the past few years. However, achieving full autonomy is not a trivial task, due to the nature of the complex and dynamic driving environment. Therefore, autonomous vehicles are equipped with a suite of different sensors to ensure robust, accurate environmental perception. In particular, the camera-LiDAR fusion is becoming an emerging research theme. However, so far there has been no critical review that focuses on deep-learning-based camera-LiDAR fusion methods. To bridge this gap and motivate future research, this article devotes to review recent deep-learning-based data fusion approaches that leverage both image and point cloud. This review gives a brief overview of deep learning on image and point cloud data processing. Followed by in-depth reviews of camera-LiDAR fusion methods in depth completion, object detection, semantic segmentation, tracking and online cross-sensor calibration, which are organized based on their respective fusion levels. Furthermore, we compare these methods on publicly available datasets. Finally, we identified gaps and over-looked challenges between current academic researches and real-world applications. Based on these observations, we provide our insights and point out promising research directions.},
  keywords = {Camera-LiDAR fusion,Convolution,deep learning,Deep learning,depth completion,Feature extraction,Geometry,Laser radar,object detection,semantic segmentation,Semantics,sensor fusion,Three-dimensional displays,tracking},
  file = {/Users/dplane/Zotero/storage/YR2LPNE4/Cui et al_2022_Deep Learning for Image and Point Cloud Fusion in Autonomous Driving.pdf;/Users/dplane/Zotero/storage/ZBD64QLB/9380166.html}
}

@inproceedings{das2022,
  title = {Sensor Fusion in Autonomous Vehicle Using {{LiDAR}} and Camera {{Sensor}}},
  booktitle = {2022 {{IEEE}} 10th {{Region}} 10 {{Humanitarian Technology Conference}} ({{R10-HTC}})},
  author = {Das, Diptadip and Adhikary, Nabanita and Chaudhury, Saurabh},
  year = {2022},
  month = sep,
  pages = {336--341},
  issn = {2572-7621},
  doi = {10.1109/R10-HTC54060.2022.9929588},
  urldate = {2025-02-15},
  abstract = {This paper presents a sensor fusion methodology for autonomous vehicles (AVs) using Light Detection and Ranging (LiDAR) and camera. Mostly sensors like camera or LiDAR is used only as the sensor for visual perception in AVs. But the hindrance comes during bad weather conditions, dim light or night time. To alleviate this problem, a method which combines both LiDAR and camera sensor using odometry is explored in this paper. The study also attempts to employ Extended Kalman Filter (EKF) to reduce error in position estimate of the vehicle.},
  keywords = {Autonomous vehicles,Camera,Cameras,Distance measurement,GPS,Kalman Filter,Kalman filters,Laser radar,LiDAR,Odometry,Point cloud compression,Real-time systems,Sensor fusion},
  file = {/Users/dplane/Zotero/storage/U7P8B57B/Das et al_2022_Sensor fusion in autonomous vehicle using LiDAR and camera Sensor.pdf;/Users/dplane/Zotero/storage/8EY6A4VX/9929588.html}
}

@misc{eckstein2024,
  title = {{{US Navy}}'s Four Unmanned Ships Return from {{Pacific}} Deployment},
  author = {Eckstein, Megan},
  year = {2024},
  month = jan,
  journal = {Defense News},
  urldate = {2025-02-14},
  abstract = {Four medium USV prototypes spent five months in the region working with the Navy-Marine team and allies to push the limits of concepts of operations.},
  chapter = {name},
  howpublished = {https://www.defensenews.com/naval/2024/01/16/us-navys-four-unmanned-ships-return-from-pacific-deployment/},
  langid = {english},
  keywords = {News Article},
  file = {/Users/dplane/Zotero/storage/VKARHDCY/us-navys-four-unmanned-ships-return-from-pacific-deployment.html}
}

@misc{eckstein2024a,
  title = {{{US Navy}}'s Four Unmanned Ships Return from {{Pacific}} Deployment},
  author = {Eckstein, Megan},
  year = {2024},
  month = jan,
  journal = {Defense News},
  urldate = {2025-03-25},
  abstract = {Four medium USV prototypes spent five months in the region working with the Navy-Marine team and allies to push the limits of concepts of operations.},
  chapter = {name},
  howpublished = {https://www.defensenews.com/naval/2024/01/16/us-navys-four-unmanned-ships-return-from-pacific-deployment/},
  langid = {english},
  file = {/Users/dplane/Zotero/storage/U54A5TSN/us-navys-four-unmanned-ships-return-from-pacific-deployment.html}
}

@inproceedings{farahnakian2018,
  title = {Object {{Detection Based}} on {{Multi-sensor Proposal Fusion}} in {{Maritime Environment}}},
  booktitle = {2018 17th {{IEEE International Conference}} on {{Machine Learning}} and {{Applications}} ({{ICMLA}})},
  author = {Farahnakian, Fahimeh and Haghbayan, Mohammad-Hashem and Poikonen, Jonne and Laurinen, Markus and Nevalainen, Paavo and Heikkonen, Jukka},
  year = {2018},
  month = dec,
  pages = {971--976},
  publisher = {IEEE},
  address = {Orlando, FL},
  doi = {10.1109/ICMLA.2018.00158},
  urldate = {2025-02-15},
  abstract = {In this paper, we propose an effective object detection framework based on proposal fusion of multiple sensors such as infrared camera, RGB cameras, radar and LiDAR. Our framework first applies the Selective Search (SS) method on RGB image data to extract possible candidate proposals which likely contain the objects of interest. Then it uses the information from other sensors in order to reduce the number of generated proposals by SS and find more dense proposals. Finally, the class of objects within the final proposals are identified by Convolutional Neural Network (CNN). Experimental results on real dataset demonstrate that our framework can precisely detect meaningful object regions using a smaller number of proposals than other object proposals methods. Further, our framework can achieve reliable object detection and classification results in maritime environments.},
  isbn = {978-1-5386-6805-4},
  langid = {english},
  keywords = {multisensor},
  file = {/Users/dplane/Zotero/storage/T6N9CJ4I/Farahnakian et al. - 2018 - Object Detection Based on Multi-sensor Proposal Fu.pdf;/Users/dplane/Zotero/storage/3VHX9UEK/8614183.html}
}

@article{farahnakian2020,
  title = {Deep {{Learning Based Multi-Modal Fusion Architectures}} for {{Maritime Vessel Detection}}},
  author = {Farahnakian, Fahimeh and Heikkonen, Jukka},
  year = {2020},
  month = jan,
  journal = {Remote Sensing},
  volume = {12},
  number = {16},
  pages = {2509},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2072-4292},
  doi = {10.3390/rs12162509},
  urldate = {2025-02-15},
  abstract = {Object detection is a fundamental computer vision task for many real-world applications. In the maritime environment, this task is challenging due to varying light, view distances, weather conditions, and sea waves. In addition, light reflection, camera motion and illumination changes may cause to false detections. To address this challenge, we present three fusion architectures to fuse two imaging modalities: visible and infrared. These architectures can provide complementary information from two modalities in different levels: pixel-level, feature-level, and decision-level. They employed deep learning for performing fusion and detection. We investigate the performance of the proposed architectures conducting a real marine image dataset, which is captured by color and infrared cameras on-board a vessel in the Finnish archipelago. The cameras are employed for developing autonomous ships, and collect data in a range of operation and climatic conditions. Experiments show that feature-level fusion architecture outperforms the state-of-the-art other fusion level architectures.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {autonomous vehicles,convolutional neural networks,deep learning,favorite,marine environment,multi-sensor fusion,object detection},
  file = {/Users/dplane/Zotero/storage/ZT9I8YBH/Farahnakian_Heikkonen_2020_Deep Learning Based Multi-Modal Fusion Architectures for Maritime Vessel.pdf;/Users/dplane/Zotero/storage/V3MP3HIL/2509.html}
}

@article{feng2021,
  title = {Deep {{Multi-Modal Object Detection}} and {{Semantic Segmentation}} for {{Autonomous Driving}}: {{Datasets}}, {{Methods}}, and {{Challenges}}},
  shorttitle = {Deep {{Multi-Modal Object Detection}} and {{Semantic Segmentation}} for {{Autonomous Driving}}},
  author = {Feng, Di and {Haase-Sch{\"u}tz}, Christian and Rosenbaum, Lars and Hertlein, Heinz and Gl{\"a}ser, Claudius and Timm, Fabian and Wiesbeck, Werner and Dietmayer, Klaus},
  year = {2021},
  month = mar,
  journal = {IEEE Transactions on Intelligent Transportation Systems},
  volume = {22},
  number = {3},
  pages = {1341--1360},
  issn = {1558-0016},
  doi = {10.1109/TITS.2020.2972974},
  urldate = {2025-03-16},
  abstract = {Recent advancements in perception for autonomous driving are driven by deep learning. In order to achieve robust and accurate scene understanding, autonomous vehicles are usually equipped with different sensors (e.g. cameras, LiDARs, Radars), and multiple sensing modalities can be fused to exploit their complementary properties. In this context, many methods have been proposed for deep multi-modal perception problems. However, there is no general guideline for network architecture design, and questions of ``what to fuse'', ``when to fuse'', and ``how to fuse'' remain open. This review paper attempts to systematically summarize methodologies and discuss challenges for deep multi-modal object detection and semantic segmentation in autonomous driving. To this end, we first provide an overview of on-board sensors on test vehicles, open datasets, and background information for object detection and semantic segmentation in autonomous driving research. We then summarize the fusion methodologies and discuss challenges and open questions. In the appendix, we provide tables that summarize topics and methods. We also provide an interactive online platform to navigate each reference: https://boschresearch.github.io/multimodalperception/.},
  keywords = {autonomous driving,Autonomous vehicles,Cameras,deep learning,Fuses,Laser radar,Multi-modality,object detection,Object detection,semantic segmentation,Sensors},
  file = {/Users/dplane/Zotero/storage/UHGPBCSA/Feng et al_2021_Deep Multi-Modal Object Detection and Semantic Segmentation for Autonomous.pdf;/Users/dplane/Zotero/storage/MS47RZGG/9000872.html}
}

@article{fischer2022,
  title = {A {{RoboStack Tutorial}}: {{Using}} the {{Robot Operating System Alongside}} the {{Conda}} and {{Jupyter Data Science Ecosystems}}},
  shorttitle = {A {{RoboStack Tutorial}}},
  author = {Fischer, Tobias and Vollprecht, Wolf and Traversaro, Silvio and Yen, Sean and Herrero, Carlos and Milford, Michael},
  year = {2022},
  month = jun,
  journal = {IEEE Robotics \& Automation Magazine},
  volume = {29},
  number = {2},
  pages = {65--74},
  issn = {1558-223X},
  doi = {10.1109/MRA.2021.3128367},
  urldate = {2025-05-22},
  abstract = {The Robot Operating System (ROS) has become the de facto standard middleware in the robotics community [1]. ROS bundles everything, from low-level drivers to tools that transform among coordinate systems, to state-of-the-art perception and control algorithms. One of ROS's key merits is the rich ecosystem of standardized tools to build and distribute ROS-based software.},
  keywords = {Control systems,Ecosystems,Middleware,Operating systems,Robot kinematics,Software engineering,Tutorials},
  file = {/Users/dplane/Zotero/storage/J3J5RRGW/Fischer et al_2022_A RoboStack Tutorial.pdf}
}

@inproceedings{garcia-garcia2016,
  title = {{{PointNet}}: {{A 3D Convolutional Neural Network}} for Real-Time Object Class Recognition},
  shorttitle = {{{PointNet}}},
  booktitle = {2016 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  author = {{Garcia-Garcia}, A. and {Gomez-Donoso}, F. and {Garcia-Rodriguez}, J. and {Orts-Escolano}, S. and Cazorla, M. and {Azorin-Lopez}, J.},
  year = {2016},
  month = jul,
  pages = {1578--1584},
  issn = {2161-4407},
  doi = {10.1109/IJCNN.2016.7727386},
  urldate = {2025-02-22},
  abstract = {During the last few years, Convolutional Neural Networks are slowly but surely becoming the default method solve many computer vision related problems. This is mainly due to the continuous success that they have achieved when applied to certain tasks such as image, speech, or object recognition. Despite all the efforts, object class recognition methods based on deep learning techniques still have room for improvement. Most of the current approaches do not fully exploit 3D information, which has been proven to effectively improve the performance of other traditional object recognition methods. In this work, we propose PointNet, a new approach inspired by VoxNet and 3D ShapeNets, as an improvement over the existing methods by using density occupancy grids representations for the input data, and integrating them into a supervised Convolutional Neural Network architecture. An extensive experimentation was carried out, using ModelNet - a large-scale 3D CAD models dataset - to train and test the system, to prove that our approach is on par with state-of-the-art methods in terms of accuracy while being able to perform recognition under real-time constraints.},
  keywords = {Computer architecture,Machine learning,Neural networks,Object recognition,PointNet,Solid modeling,Three-dimensional displays,Two dimensional displays},
  file = {/Users/dplane/Zotero/storage/B5E8SE2I/Garcia-Garcia et al_2016_PointNet.pdf;/Users/dplane/Zotero/storage/QY3UAIDX/7727386.html}
}

@inproceedings{geiger2012,
  title = {Are We Ready for Autonomous Driving? {{The KITTI}} Vision Benchmark Suite},
  shorttitle = {Are We Ready for Autonomous Driving?},
  booktitle = {2012 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Geiger, Andreas and Lenz, Philip and Urtasun, Raquel},
  year = {2012},
  month = jun,
  pages = {3354--3361},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2012.6248074},
  urldate = {2025-03-16},
  abstract = {Today, visual recognition systems are still rarely employed in robotics applications. Perhaps one of the main reasons for this is the lack of demanding benchmarks that mimic such scenarios. In this paper, we take advantage of our autonomous driving platform to develop novel challenging benchmarks for the tasks of stereo, optical flow, visual odometry/SLAM and 3D object detection. Our recording platform is equipped with four high resolution video cameras, a Velodyne laser scanner and a state-of-the-art localization system. Our benchmarks comprise 389 stereo and optical flow image pairs, stereo visual odometry sequences of 39.2 km length, and more than 200k 3D object annotations captured in cluttered scenarios (up to 15 cars and 30 pedestrians are visible per image). Results from state-of-the-art algorithms reveal that methods ranking high on established datasets such as Middlebury perform below average when being moved outside the laboratory to the real world. Our goal is to reduce this bias by providing challenging benchmarks with novel difficulties to the computer vision community. Our benchmarks are available online at: www.cvlibs.net/datasets/kitti},
  keywords = {Benchmark testing,Cameras,Measurement,Optical imaging,Optical sensors,Visualization},
  file = {/Users/dplane/Zotero/storage/Y666P9UW/Geiger et al_2012_Are we ready for autonomous driving.pdf;/Users/dplane/Zotero/storage/7DKIWKUK/6248074.html}
}

@inproceedings{girshick2014,
  title = {Rich {{Feature Hierarchies}} for {{Accurate Object Detection}} and {{Semantic Segmentation}}},
  booktitle = {2014 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
  year = {2014},
  month = jun,
  pages = {580--587},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2014.81},
  urldate = {2025-02-14},
  abstract = {Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30\% relative to the previous best result on VOC 2012 -- achieving a mAP of 53.3\%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also present experiments that provide insight into what the network learns, revealing a rich hierarchy of image features. Source code for the complete system is available at http://www.cs.berkeley.edu/ rbg/rcnn.},
  keywords = {Feature extraction,nn_core,object deteciton,Object detection,Proposals,Support vector machines,Training,Vectors,Visualization},
  file = {/Users/dplane/Zotero/storage/CJRLEN2A/Girshick et al_2014_Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation.pdf;/Users/dplane/Zotero/storage/N9STWHI6/6909475.html}
}

@inproceedings{guo2023,
  title = {Investigating the~{{Transferability}} of~{{YOLOv5-Based Water Surface Object Detection Model}} in~{{Maritime Applications}}},
  booktitle = {International {{Conference}} on {{Neural Computing}} for {{Advanced Applications}}},
  author = {Guo, Yu and Chen, Zhuo and Wang, Qi and Bao, Tao and Zhou, Zexing},
  editor = {Zhang, Haijun and Ke, Yinggen and Wu, Zhou and Hao, Tianyong and Zhang, Zhao and Meng, Weizhi and Mu, Yuanyuan},
  year = {2023},
  pages = {103--115},
  publisher = {Springer Nature},
  address = {Singapore},
  doi = {10.1007/978-981-99-5847-4_8},
  abstract = {Object detection on the water surface is crucial for unmanned surface vehicles in maritime environments. Despite the challenges posed by variable lighting and ocean conditions, advancements in this field are necessary. In this paper, we investigate the transferability of YOLOv5-based water surface object detection models in cross-domain scenarios. The evaluation is based on publicly available datasets and two newly proposed datasets, Taihu Trial Dataset(TTD) and Fuxian Trial Dataset(FTD), which contain similar target classes but distinct scene and features. Results from extensive experiments indicate that zero-shot transfer is challenging, but a limited number of samples from the target domain can greatly enhance model performance.},
  isbn = {978-981-9958-47-4},
  langid = {english},
  keywords = {Intelligent Perception,Maritime,Model Transferability,Object detection,Yolo},
  file = {/Users/dplane/Zotero/storage/L6EJCEGE/Guo et al_2023_Investigating the Transferability of YOLOv5-Based Water Surface Object.pdf}
}

@inproceedings{haghbayan2018,
  title = {An {{Efficient Multi-sensor Fusion Approach}} for {{Object Detection}} in {{Maritime Environments}}},
  booktitle = {2018 21st {{International Conference}} on {{Intelligent Transportation Systems}} ({{ITSC}})},
  author = {Haghbayan, Mohammad-Hashem and Farahnakian, Fahimeh and Poikonen, Jonne and Laurinen, Markus and Nevalainen, Paavo and Plosila, Juha and Heikkonen, Jukka},
  year = {2018},
  month = nov,
  pages = {2163--2170},
  issn = {2153-0017},
  doi = {10.1109/ITSC.2018.8569890},
  urldate = {2025-02-14},
  abstract = {Robust real-time object detection and tracking are challenging problems in autonomous transportation systems due to operation of algorithms in inherently uncertain and dynamic environments and rapid movement of objects. Therefore, tracking and detection algorithms must cooperate with each other to achieve smooth tracking of detected objects that later can be used by the navigation system. In this paper, we first present an efficient multi-sensor fusion approach based on the probabilistic data association method in order to achieve accurate object detection and tracking results. The proposed approach fuses the detection results obtained independently from four main sensors: radar, LiDAR, RGB camera and infrared camera. It generates object region proposals based on the fused detection result. Then, a Convolutional Neural Network (CNN) approach is used to identify the object categories within these regions. The CNN is trained on a real dataset from different ferry driving scenarios. The experimental results of tracking and classification on real datasets show that the proposed approach provides reliable object detection and classification results in maritime environments.},
  keywords = {autonomous vessel,Cameras,convolutional neural networks,favorite,Feature extraction,Fusion,Infrared,LiDAR,maritime environment,multi-sensor fusion,multisensor,object detection,Object detection,Proposals,Radar,region proposals,RGB Camera,Sensor fusion},
  file = {/Users/dplane/Zotero/storage/XGTZHTAP/Haghbayan et al_2018_An Efficient Multi-sensor Fusion Approach for Object Detection in Maritime.pdf;/Users/dplane/Zotero/storage/3XDUSZAU/8569890.html}
}

@inproceedings{he2016,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  booktitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = {2016},
  month = jun,
  pages = {770--778},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2016.90},
  urldate = {2025-02-14},
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8{\texttimes} deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  keywords = {Complexity theory,Degradation,DNN,Image recognition,Image segmentation,Neural networks,nn_core,Training,Visualization},
  file = {/Users/dplane/Zotero/storage/5PUFB45X/He et al_2016_Deep Residual Learning for Image Recognition.pdf;/Users/dplane/Zotero/storage/I79LPQ6F/7780459.html}
}

@inproceedings{helgesen2019,
  title = {Sensor {{Combinations}} in {{Heterogeneous Multi-sensor Fusion}} for {{Maritime Target Tracking}}},
  booktitle = {2019 22th {{International Conference}} on {{Information Fusion}} ({{FUSION}})},
  author = {Helgesen, {\O}ystein Kaarstad and Brekke, Edmund F{$\phi$}rland and Helgesen, H{\aa}kon Hagen and Engelhardtsen, {\O}ystein},
  year = {2019},
  month = jul,
  pages = {1--9},
  doi = {10.23919/FUSION43075.2019.9011297},
  urldate = {2025-02-15},
  abstract = {Safe navigation for autonomous surface vehicles requires a robust and reliable tracking system that maintains and estimates position and velocity of other vessels. This paper demonstrates a measurement level sensor fusion system for tracking in a maritime environment using lidar, radar, electrooptical and infrared cameras. The backbone of the system is a multi-sensor version of the Joint Integrated Probabilistic Data Association (JIPDA) with both existence and visibility probabilities. Using reference targets equipped with GPS receivers, the performance of different sensors and sensor combinations are evaluated for autonomous surface vehicles (ASVs), Several interesting observations are made, among them that passive sensors can help resolve merged measurements issues in radar tracking, and that the choice between radar and lidar may boil down to a trade-off between fast track initiation and large numbers of false tracks.},
  keywords = {Cameras,Detectors,Radar tracking,Robot sensing systems,Sensor fusion,target tracking,Target tracking,unmanned surface vehicle},
  file = {/Users/dplane/Zotero/storage/HPC86MI5/Helgesen et al_2019_Sensor Combinations in Heterogeneous Multi-sensor Fusion for Maritime Target.pdf;/Users/dplane/Zotero/storage/UWXSBENQ/9011297.html}
}

@article{holland2024,
  title = {Design of the Minion Research Platform for the 2022 Maritime RobotX Challenge},
  author = {Holland, David and Landaeta, Erasmo and Montagnoli, Charles and Ayars, Taylor and Barnes, Jamie and Barthelemy, Kesmir and Brown, Robert and Delp, Grady and Garnier, Thomas and Halleran, Juan and Helms, Matthew and Hendrickson, James and Kay, James and Kuennen, Kurt and Lane, Katherine and Lachguar, Adam and Perskin, Jennifer and Sarkar, Sagar and Schoener, Marco and Thomas, Ryan and Thompson, David and Vail, Devon and Coyle, Eric J. and Currier, Patrick N. and Reinholtz, Charles F.},
  year = {2024},
  month = sep,
  journal = {Naval Engineers Journal},
  volume = {136},
  number = {3},
  pages = {125--137},
  abstract = {For the 2022 Maritime RobotX Challenge Embry-Riddle Aeronautical University (ERAU) made significant improvements to their fully autonomous research platform, Minion. To complete mission tasks, Minion uses sophisticated sensory and perception algorithms fusing data from a suite consisting of multi-beam LiDARs, multi-modal imagery sensors, and a high precision GPS/INS. This data feeds decision-making algorithms that include neural network visual detection, long-range LiDAR-based object detection, and dynamic path planning. These processes are all tied together using a unique tasking system designed to initiate tasks when perception queues are received and select a task order to maximum time usage. The research team has also developed an autonomous drone platform, for completing the tasks that require aerial reconnaissance. All of Minion's systems are rated and tested to survive operations in adverse weather conditions, including high temperature, high humidity, and heavy precipitation. Minion was thoroughly tested using simulations, recorded data, and dozens of hours of in-water testing. The result of this is an advanced platform that is robust, reliable, and readily upgradable.},
  file = {/Users/dplane/Zotero/storage/8NR3LJFC/Holland et al. - Design of the Minion Research Platform for the 202.pdf}
}

@misc{huang2024,
  title = {Multi-Modal {{Sensor Fusion}} for {{Auto Driving Perception}}: {{A Survey}}},
  shorttitle = {Multi-Modal {{Sensor Fusion}} for {{Auto Driving Perception}}},
  author = {Huang, Keli and Shi, Botian and Li, Xiang and Li, Xin and Huang, Siyuan and Li, Yikang},
  year = {2024},
  month = dec,
  number = {arXiv:2202.02703},
  eprint = {2202.02703},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2202.02703},
  urldate = {2025-03-19},
  abstract = {Multi-modal fusion is a fundamental task for the perception of an autonomous driving system, which has recently intrigued many researchers. However, achieving a rather good performance is not an easy task due to the noisy raw data, underutilized information, and the misalignment of multi-modal sensors. In this paper, we provide a literature review of the existing multi-modal-based methods for perception tasks in autonomous driving. Generally, we make a detailed analysis including over 50 papers leveraging perception sensors including LiDAR and camera trying to solve object detection and semantic segmentation tasks. Different from traditional fusion methodology for categorizing fusion models, we propose an innovative way that divides them into two major classes, four minor classes by a more reasonable taxonomy in the view of the fusion stage. Moreover, we dive deep into the current fusion methods, focusing on the remaining problems and open-up discussions on the potential research opportunities. In conclusion, what we expect to do in this paper is to present a new taxonomy of multi-modal fusion methods for the autonomous driving perception tasks and provoke thoughts of the fusion-based techniques in the future.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/dplane/Zotero/storage/3ADB7SG6/Huang et al_2024_Multi-modal Sensor Fusion for Auto Driving Perception.pdf;/Users/dplane/Zotero/storage/3D5U7J8Q/2202.html}
}

@article{jun-hwa2022,
  title = {Object {{Detection}} and {{Classification Based}} on {{YOLO-V5}} with {{Improved Maritime Dataset}}},
  author = {{Jun-Hwa}, Kim and Kim, Namho and Park, Yong Woon and Won, Chee Sun},
  year = {2022},
  journal = {Journal of Marine Science and Engineering},
  volume = {10},
  number = {3},
  pages = {377},
  publisher = {MDPI AG},
  address = {Basel, Switzerland},
  doi = {10.3390/jmse10030377},
  urldate = {2025-02-14},
  abstract = {SMD (Singapore Maritime Dataset) is a public dataset with annotated videos, and it is almost unique in the training of deep neural networks (DNN) for the recognition of maritime objects. However, there are noisy labels and imprecisely located bounding boxes in the ground truth of the SMD. In this paper, for the benchmark of DNN algorithms, we correct the annotations of the SMD dataset and present an improved version, which we coined SMD-Plus. We also propose augmentation techniques designed especially for the SMD-Plus. More specifically, an online transformation of training images via Copy \& Paste is applied to solve the class-imbalance problem in the training dataset. Furthermore, the mix-up technique is adopted in addition to the basic augmentation techniques for YOLO-V5. Experimental results show that the detection and classification performance of the modified YOLO-V5 with the SMD-Plus has improved in comparison to the original YOLO-V5. The ground truth of the SMD-Plus and our experimental results are available for download.},
  copyright = {{\copyright} 2022 by the authors.  Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (https://creativecommons.org/licenses/by/4.0/).  Notwithstanding the ProQuest Terms and Conditions, you may use this content in accordance with the terms of the License.},
  langid = {english},
  keywords = {Algorithms,Annotations,Artificial neural networks,Augmentation,Boxes,Classification,data relabel,Datasets,deep learning,Deep learning,Detection,Environmental,maritime dataset,Neural networks,object detection,Object detection,Object recognition,Training},
  file = {/Users/dplane/Zotero/storage/QEHUDF5F/Jun-Hwa et al_2022_Object Detection and Classification Based on YOLO-V5 with Improved Maritime.pdf}
}

@article{krizhevsky2017,
  title = {{{ImageNet}} Classification with Deep Convolutional Neural Networks},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
  year = {2017},
  month = may,
  journal = {Communications of the ACM},
  volume = {60},
  number = {6},
  pages = {84--90},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/3065386},
  urldate = {2025-02-14},
  abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called ``dropout'' that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
  langid = {english},
  keywords = {nn_core alexnet},
  file = {/Users/dplane/Zotero/storage/H7KD8E5V/Krizhevsky et al. - 2017 - ImageNet classification with deep convolutional ne.pdf}
}

@article{kumar2020,
  title = {{{LiDAR}} and {{Camera Fusion Approach}} for {{Object Distance Estimation}} in {{Self-Driving Vehicles}}},
  author = {Kumar, G. Ajay and Lee, Jin Hee and Hwang, Jongrak and Park, Jaehyeong and Youn, Sung Hoon and Kwon, Soon},
  year = {2020},
  month = feb,
  journal = {Symmetry},
  volume = {12},
  number = {2},
  pages = {324},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2073-8994},
  doi = {10.3390/sym12020324},
  urldate = {2025-02-21},
  abstract = {The fusion of light detection and ranging (LiDAR) and camera data in real-time is known to be a crucial process in many applications, such as in autonomous driving, industrial automation, and robotics. Especially in the case of autonomous vehicles, the efficient fusion of data from these two types of sensors is important to enabling the depth of objects as well as the detection of objects at short and long distances. As both the sensors are capable of capturing the different attributes of the environment simultaneously, the integration of those attributes with an efficient fusion approach greatly benefits the reliable and consistent perception of the environment. This paper presents a method to estimate the distance (depth) between a self-driving car and other vehicles, objects, and signboards on its path using the accurate fusion approach. Based on the geometrical transformation and projection, low-level sensor fusion was performed between a camera and LiDAR using a 3D marker. Further, the fusion information is utilized to estimate the distance of objects detected by the RefineDet detector. Finally, the accuracy and performance of the sensor fusion and distance estimation approach were evaluated in terms of quantitative and qualitative analysis by considering real road and simulation environment scenarios. Thus the proposed low-level sensor fusion, based on the computational geometric transformation and projection for object distance estimation proves to be a promising solution for enabling reliable and consistent environment perception ability for autonomous vehicles.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {autonomous vehicle,computational geometry transformation,depth sensing,image source,point cloud to image mapping,projection,self-driving vehicle,sensor calibration,sensor fusion},
  file = {/Users/dplane/Zotero/storage/MYAW7BJS/Kumar et al_2020_LiDAR and Camera Fusion Approach for Object Distance Estimation in Self-Driving.pdf}
}

@article{kunz2005,
  title = {Detection of Small Targets in a Marine Environment Using Laser Radar},
  author = {Kunz, Gerard J. and Bekman, Herman H. P. T. and Benoist, Koen W. and Coen, Leo H. and Van Den Heuvel, Johan C. and Van Putten, Frank J. M.},
  editor = {Frouin, Robert J. and Babin, Marcel and Sathyendranath, Shubha},
  year = {2005},
  month = aug,
  volume = {5885},
  pages = {58850F},
  doi = {10.1117/12.614914},
  urldate = {2025-02-15},
  abstract = {Small maritime targets, e.g., periscope tubes, jet skies, swimmers and small boats, are potential threats for naval ships under many conditions, but are difficult to detect with current radar systems due to their limited radar cross section and the presence of sea clutter. On the other hand, applications of lidar systems have shown that the reflections from small targets are significantly stronger than reflections from the sea surface. As a result, dedicated lidar systems are potential tools for the detection of small maritime targets. A geometric approach is used to compare the diffuse reflection properties of cylinders and spheres with flat surfaces, which is used to estimate the maximum detectable range of such objects for a given lidar system. Experimental results using lasers operating at 1.06 {\textmu}m and 1.57 {\textmu}m confirm this theory and are discussed. Small buoys near Scheveningen harbor could be detected under adverse weather over more than 9 km. Extrapolation of these results indicates that small targets can be detected out to ranges of approximately 20 km.},
  langid = {english},
  keywords = {multiseonsor},
  file = {/Users/dplane/Zotero/storage/LM29JWG2/Kunz et al. - 2005 - Detection of small targets in a marine environment.pdf}
}

@article{lachguar,
  title = {Minion: {{Design}} and {{Competition Strategy}} for the 2024 {{Maritime RobotX Challenge Minion}}},
  author = {Lachguar, Adam and Ucles, Giovanna and Sarkar, Sagar and Lane, Dan and Liebergall, Erik and Aggarwal, Sarthak and Proper, Willis and Saravis, Michael and Dcruz, Dean and Kay, Isaac and Abe, Matis and Jagwani, Bharat and Vinnakota, Rohit and Prudencio, Mateus and Yelvington, Marshall and Young, Jacob and Pathi, Neel and Park, Brian and Anand, Aarambh and Lam, Andrew and Jones, Thomas and Coyle, Dr Eric and Currier, Dr Patrick},
  abstract = {Embry-Riddle Aeronautical University's Team Minion is returning to RobotX with significant improvements to its defending champion fully autonomous surface vessel (ASV), Minion. Team Minion's new design strategy and systems engineering approach, called Minion Process, has allowed a balance of academics, research, and team objectives throughout the team. This design strategy combined with a rigorous multistep testing process that values safety and innovation has led to an ever-improving toolset for Minion and its Uncrewed Aerial Vehicle (UAV), Kevin. These include software enhancements of a new patent-pending control scheme and better integration of computer vision throughout the system, as well as hardware improvements of azimuthing motor control, new UAV capabilities, and a new ball launcher.},
  langid = {english},
  file = {/Users/dplane/Zotero/storage/UJDUIAW2/Lachguar et al. - Minion Design and Competition Strategy for the 20.pdf}
}

@misc{landaeta,
  title = {Assessing {{High Dynamic Range Imagery Performance}} for {{Object Detection}} in {{Maritime Environments}}},
  author = {Landaeta, Erasmo},
  year = {2024},
  month = apr,
  langid = {english},
  keywords = {disertation},
  file = {/Users/dplane/Zotero/storage/E56CK556/Landaeta - Assessing High Dynamic Range Imagery Performance f.pdf}
}

@article{lecun1998,
  title = {Gradient-Based Learning Applied to Document Recognition},
  author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
  year = {1998},
  month = nov,
  journal = {Proceedings of the IEEE},
  volume = {86},
  number = {11},
  pages = {2278--2324},
  issn = {1558-2256},
  doi = {10.1109/5.726791},
  urldate = {2025-02-14},
  abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day.},
  keywords = {Character recognition,Feature extraction,gradient descent,Hidden Markov models,Machine learning,Multi-layer neural network,Neural networks,nn_core,Optical character recognition software,Optical computing,Pattern recognition,Principal component analysis},
  file = {/Users/dplane/Zotero/storage/3UZN4EJL/Lecun et al_1998_Gradient-based learning applied to document recognition.pdf;/Users/dplane/Zotero/storage/YBE8NWC2/726791.html}
}

@article{li2020a,
  title = {Compact {{Antenna}} for {{Picosatellites Using}} a {{Meandered Folded-Shorted Patch Array}}},
  author = {Li, Yuepei and Podilchak, Symon K. and Anagnostou, Dimitris E. and Constantinides, Constantin and Walkinshaw, Tom},
  year = {2020},
  month = mar,
  journal = {IEEE Antennas and Wireless Propagation Letters},
  volume = {19},
  number = {3},
  pages = {477--481},
  issn = {1548-5757},
  doi = {10.1109/LAWP.2020.2966088},
  urldate = {2025-03-16},
  abstract = {The design and operation of a compact antenna array offering circularly polarized (CP) radiation for picosat and other small satellites for communication applications is presented. The proposed array combines the folded-shorted patches (FSPs) and meandering for antenna miniaturization. Realization of CP is achieved by a compact and planar feed circuit consisting of a network of meander-shaped 90{$^\circ$} and 180{$^\circ$} hybrid couplers, which can provide quadrature feeding of the FSP elements and can be integrated onto the backside of the antenna ground plane, which is only 5 {\texttimes} 5 cm2. Agreement in terms of the simulations and measurements is observed with realized gain values of more than 3 dBic at 1.065 GHz and with an antenna size of 0.17{$\lambda$}0 {\texttimes} 0.17{$\lambda$}0.},
  keywords = {Antenna arrays,Antenna feeds,Antenna measurements,Circular polarisation (CP),Delays,folded-shorted patch (FSP) antenna,Gain,meandering,Resonant frequency,satellite communications,sequentially rotated arrays},
  file = {/Users/dplane/Zotero/storage/Q35TZ7P8/Li et al_2020_Compact Antenna for Picosatellites Using a Meandered Folded-Shorted Patch Array.pdf;/Users/dplane/Zotero/storage/GQI48D7Q/8957242.html}
}

@article{li2023,
  title = {A Survey of Maritime Unmanned Search System: {{Theory}}, Applications and Future Directions},
  shorttitle = {A Survey of Maritime Unmanned Search System},
  author = {Li, Jiqiang and Zhang, Guoqing and Jiang, Changyan and Zhang, Weidong},
  year = {2023},
  month = oct,
  journal = {Ocean Engineering},
  volume = {285},
  pages = {115359},
  issn = {0029-8018},
  doi = {10.1016/j.oceaneng.2023.115359},
  urldate = {2025-03-14},
  abstract = {The rising frequency of ocean activities, such as ocean transportation and marine resources development, inevitably leads to a higher incidence of sudden accidents at sea. Maritime search and rescue (MSAR) plays a crucial role in marine transportation. Specifically, maritime search serves as a preparatory mission before executing the actual rescue operations. In 2018, the International Maritime Organization (IMO) made significant legal amendments to the international convention for the Safety of Life at Sea (SOLAS), emphasizing the critical importance of MSAR operations. Nevertheless, the conventional maritime search systems, such as independent surface ships, underwater vehicles, or aerial platforms, fall short of meeting the demands of modern shipping and can impose significant energy and economic burdens. Meanwhile, unmanned systems are gaining prominence in maritime search due to their notable advantages, including high efficiency, cost-effectiveness, and rapid deployment. One observes that the research of unmanned maritime search is still in an early stage. This paper provides a comprehensive survey of the state of the art of maritime unmanned search system development with four dimensions, including the unmanned aerial vehicle (UAV), unmanned surface vessel (USV) and underwater unmanned vehicle (UUV) and its cooperative heterogeneous vehicles. Firstly, the search types, the superiorities and the application scenarios of the unmanned search system are summarized. Then, the theoretical progress, engineering applications and limitations for the unmanned search system are investigated. To further discuss the advantages of the unmanned search system, a novel cooperative platform is established that uses an USV and two UAVs. In the proposed platform, the sensor, communication, guidance and control subsystems are described to illustrate the convenience and the effectiveness of the proposed cooperative system. Finally, the future research directions are examined to expedite the practical implementation of theoretical advancements in the field of maritime unmanned search.},
  keywords = {Autonomous guidance and control,Cooperative unmanned systems,Maritime unmanned search system,Underwater unmanned vehicle,Unmanned aerial vehicle,Unmanned surface vessel},
  file = {/Users/dplane/Zotero/storage/9PQ6WKBM/Li et al_2023_A survey of maritime unmanned search system.pdf;/Users/dplane/Zotero/storage/TDL5W3SE/S0029801823017432.html}
}

@article{liebergall,
  title = {Comparing {{Standard}} and {{High Dynamic Range Imagery}} for {{Maritime Object Detection}}},
  author = {Liebergall, Erik and Landaeta, Erasmo and Coyle, Dr Eric},
  year = {2024},
  month = sep,
  journal = {Naval Engineers Journal},
  volume = {136},
  number = {3},
  pages = {117--124},
  langid = {english},
  keywords = {disertation},
  file = {/Users/dplane/Zotero/storage/FPCS7BMN/Liebergall et al. - Comparing Standard and High Dynamic Range Imagery .pdf}
}

@article{liu2023a,
  title = {Real Time Object Detection Using {{LiDAR}} and Camera Fusion for Autonomous Driving},
  author = {Liu, Haibin and Wu, Chao and Wang, Huanjie},
  year = {2023},
  month = may,
  journal = {Scientific Reports},
  volume = {13},
  number = {1},
  pages = {8056},
  publisher = {Nature Publishing Group},
  issn = {2045-2322},
  doi = {10.1038/s41598-023-35170-z},
  urldate = {2025-03-16},
  abstract = {Autonomous driving has been widely applied in commercial and industrial applications, along with the upgrade of environmental awareness systems. Tasks such as path planning, trajectory tracking, and obstacle avoidance are strongly dependent on the ability to perform real-time object detection and position regression. Among the most commonly used sensors, camera provides dense semantic information but lacks accurate distance information to the target, while LiDAR provides accurate depth information but with sparse resolution. In this paper, a LiDAR-camera-based fusion algorithm is proposed to improve the above-mentioned trade-off problems by constructing a Siamese network for object detection. Raw point clouds are converted to camera planes to obtain a 2D depth image. By designing a cross feature fusion block to connect the depth and RGB processing branches, the feature-layer fusion strategy is applied to integrate multi-modality data. The proposed fusion algorithm is evaluated on the KITTI dataset. Experimental results demonstrate that our algorithm has superior performance and real-time efficiency. Remarkably, it outperforms other state-of-the-art algorithms at the most important moderate level and achieves excellent performance at the easy and hard levels.},
  copyright = {2023 The Author(s)},
  langid = {english},
  keywords = {Civil engineering,Mechanical engineering},
  file = {/Users/dplane/Zotero/storage/56MBH3R2/Liu et al_2023_Real time object detection using LiDAR and camera fusion for autonomous driving.pdf}
}

@article{ma2024,
  title = {Multi-Modal Information Fusion for {{LiDAR-based 3D}} Object Detection Framework},
  author = {Ma, Ruixin and Yin, Yong and Chen, Jing and Chang, Rihao},
  year = {2024},
  month = jan,
  journal = {Multimedia Tools and Applications},
  volume = {83},
  number = {3},
  pages = {7995--8012},
  issn = {1573-7721},
  doi = {10.1007/s11042-023-15452-4},
  urldate = {2025-03-14},
  abstract = {With the rapid development of water transportation, ship safety supervision is facing more severe pressures and challenges. Precise and efficient detection of ship targets is becoming more and more important, which urgently requires intelligent detection methods to ultimately improves shipping management efficiency. However, the surveillance video of waterway transportation is often influenced by fog and rain, which can affect the performance of object detection and reduce the efficiency of management. The current traditional object approaches are hard to handle these problems. In this paper, we propose a novel multi-modal information fusion method to handle multi-object detection in waterway transportation, which introduces the LiDAR (Light Detection And Ranging) dataset to add spatial information and handle the interference of fog and rain. The target ROI (Region Of Interest) point cloud and image data are initially fused in the pre-fusion stage. This phase can efficiently direct the network's attention to the region with the highest target probability, increasing the target recall rate. The 3D bounding box in the point cloud and 2D bounding boxes in the image retrieved are then fused in the post-fusion stage to improve target precision and enrich target detection information. Finally, using time synchronization and a space transformation matrix, the detection result is transferred to the picture coordinate system to create a ship image target with 3D depth information. This technique overcomes the constraints of single-sensor environment perception, adapts to the detection of ship targets in a variety of situations, and is more precise and robust. The algorithm's superiority is also demonstrated by the experiments.},
  langid = {english},
  keywords = {Computer vision,Data fusion,LiDAR,Multi-modality,Object detection},
  file = {/Users/dplane/Zotero/storage/2CPN2KY2/Ma et al_2024_Multi-modal information fusion for LiDAR-based 3D object detection framework.pdf}
}

@article{norbye,
  title = {Camera-{{Lidar}} Sensor Fusion in Real Time for Autonomous Surface Vehicles},
  author = {Norbye, H{\aa}kon Gjertsen},
  langid = {english},
  keywords = {favorite},
  file = {/Users/dplane/Zotero/storage/CB8XH9TV/Norbye - Camera-Lidar sensor fusion in real time for autono.pdf}
}

@inproceedings{pang2020,
  title = {{{CLOCs}}: {{Camera-LiDAR Object Candidates Fusion}} for {{3D Object Detection}}},
  shorttitle = {{{CLOCs}}},
  booktitle = {2020 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Pang, Su and Morris, Daniel and Radha, Hayder},
  year = {2020},
  month = oct,
  pages = {10386--10393},
  publisher = {IEEE Press},
  address = {Las Vegas, NV, USA},
  doi = {10.1109/IROS45743.2020.9341791},
  urldate = {2025-03-16},
  abstract = {There have been significant advances in neural networks for both 3D object detection using LiDAR and 2D object detection using video. However, it has been surprisingly difficult to train networks to effectively use both modalities in a way that demonstrates gain over single-modality networks. In this paper, we propose a novel Camera-LiDAR Object Candidates (CLOCs) fusion network. CLOCs fusion provides a low-complexity multi-modal fusion framework that significantly improves the performance of single-modality detectors. CLOCs operates on the combined output candidates before Non-Maximum Suppression (NMS) of any 2D and any 3D detector, and is trained to leverage their geometric and semantic consistencies to produce more accurate final 3D and 2D detection results. Our experimental evaluation on the challenging KITTI object detection benchmark, including 3D and bird's eye view metrics, shows significant improvements, especially at long distance, over the state-of-the-art fusion based methods. At time of submission, CLOCs ranks the highest among all the fusion-based methods in the official KITTI leaderboard. We will release our code upon acceptance.},
  file = {/Users/dplane/Zotero/storage/H4P9JDYX/Pang et al_2020_CLOCs.pdf}
}

@article{prasad2017,
  title = {Video {{Processing From Electro-Optical Sensors}} for {{Object Detection}} and {{Tracking}} in a {{Maritime Environment}}: {{A Survey}}},
  shorttitle = {Video {{Processing From Electro-Optical Sensors}} for {{Object Detection}} and {{Tracking}} in a {{Maritime Environment}}},
  author = {Prasad, Dilip K. and Rajan, Deepu and Rachmawati, Lily and Rajabally, Eshan and Quek, Chai},
  year = {2017},
  month = aug,
  journal = {IEEE Transactions on Intelligent Transportation Systems},
  volume = {18},
  number = {8},
  pages = {1993--2016},
  issn = {1558-0016},
  doi = {10.1109/TITS.2016.2634580},
  urldate = {2025-02-15},
  abstract = {We present a survey on maritime object detection and tracking approaches, which are essential for the development of a navigational system for autonomous ships. The electro-optical (EO) sensor considered here is a video camera that operates in the visible or the infrared spectra, which conventionally complements radar and sonar for situational awareness at sea and has demonstrated its effectiveness over the last few years. This paper provides a comprehensive overview of various approaches of video processing for object detection and tracking in the maritime environment. We follow an approach-based taxonomy wherein the advantages and limitations of each approach are compared. The object detection system consists of the following modules: horizon detection, static background subtraction, and foreground segmentation. Each of these has been studied extensively in maritime situations and has been shown to be challenging due to the presence of background motion especially due to waves and wakes. The key processes involved in object tracking include video frame registration, dynamic background subtraction, and the object tracking algorithm itself. The challenges for robust tracking arise due to camera motion, dynamic background, and low contrast of tracked object, possibly due to environmental degradation. The survey also discusses multisensor approaches and commercial maritime systems that use EO sensors. The survey also highlights methods from computer vision research, which hold promise to perform well in maritime EO data processing. Performance of several maritime and computer vision techniques is evaluated on Singapore Maritime Dataset.},
  keywords = {autonomous automobiles,Cameras,computer vision,Image edge detection,Intelligent sensors,Marine vehicles,maritime navigation,Maritime vehicles,Object detection,Radar tracking,video signal processing},
  file = {/Users/dplane/Zotero/storage/2BXQTRNP/Prasad et al_2017_Video Processing From Electro-Optical Sensors for Object Detection and Tracking.pdf;/Users/dplane/Zotero/storage/XGZHX9U5/7812788.html}
}

@inproceedings{qi2017,
  title = {{{PointNet}}: {{Deep Learning}} on {{Point Sets}} for {{3D Classification}} and {{Segmentation}}},
  shorttitle = {{{PointNet}}},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Qi, Charles R. and Su, Hao and Mo, Kaichun and Guibas, Leonidas J.},
  year = {2017},
  pages = {652--660},
  urldate = {2025-02-17},
  file = {/Users/dplane/Zotero/storage/FPZM6YA5/Qi et al. - 2017 - PointNet Deep Learning on Point Sets for 3D Class.pdf}
}

@misc{rekavandi2022,
  title = {A {{Guide}} to {{Image}} and {{Video}} Based {{Small Object Detection}} Using {{Deep Learning}} : {{Case Study}} of {{Maritime Surveillance}}},
  shorttitle = {A {{Guide}} to {{Image}} and {{Video}} Based {{Small Object Detection}} Using {{Deep Learning}}},
  author = {Rekavandi, Aref Miri and Xu, Lian and Boussaid, Farid and Seghouane, Abd-Krim and Hoefs, Stephen and Bennamoun, Mohammed},
  year = {2022},
  month = jul,
  number = {arXiv:2207.12926},
  eprint = {2207.12926},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2207.12926},
  urldate = {2025-02-15},
  abstract = {Small object detection (SOD) in optical images and videos is a challenging problem that even state-of-the-art generic object detection methods fail to accurately localize and identify such objects. Typically, small objects appear in real-world due to large camera-object distance. Because small objects occupy only a small area in the input image (e.g., less than 10\%), the information extracted from such a small area is not always rich enough to support decision making. Multidisciplinary strategies are being developed by researchers working at the interface of deep learning and computer vision to enhance the performance of SOD deep learning based methods. In this paper, we provide a comprehensive review of over 160 research papers published between 2017 and 2022 in order to survey this growing subject. This paper summarizes the existing literature and provide a taxonomy that illustrates the broad picture of current research. We investigate how to improve the performance of small object detection in maritime environments, where increasing performance is critical. By establishing a connection between generic and maritime SOD research, future directions have been identified. In addition, the popular datasets that have been used for SOD for generic and maritime applications are discussed, and also well-known evaluation metrics for the state-of-the-art methods on some of the datasets are provided.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/dplane/Zotero/storage/WA2PRAAR/Rekavandi et al_2022_A Guide to Image and Video based Small Object Detection using Deep Learning.pdf;/Users/dplane/Zotero/storage/V2C82P7U/2207.html}
}

@article{roriz2022,
  title = {Automotive {{LiDAR Technology}}: {{A Survey}}},
  shorttitle = {Automotive {{LiDAR Technology}}},
  author = {Roriz, Ricardo and Cabral, Jorge and Gomes, Tiago},
  year = {2022},
  month = jul,
  journal = {IEEE Transactions on Intelligent Transportation Systems},
  volume = {23},
  number = {7},
  pages = {6282--6297},
  issn = {1558-0016},
  doi = {10.1109/TITS.2021.3086804},
  urldate = {2025-03-19},
  abstract = {Nowadays, and more than a decade after the first steps towards autonomous driving, we keep heading to achieve fully autonomous vehicles on our roads, with LiDAR sensors being a key instrument for the success of this technology. Such advances trigger the emergence of new players in the automotive industry, and along with car manufacturers, this sector represents a multibillion-dollar market where everyone wants to take a share. To understand recent advances and technologies behind LiDAR, this article presents a survey on LiDAR sensors for the automotive industry. With this work, we show the measurement principles and imaging techniques currently being used, going through a review of commercial systems and development solutions available in the market today. Furthermore, we highlight the current and future challenges, providing insights on how both research and industry can step towards better LiDAR solutions.},
  keywords = {3D imaging,Automobiles,Automotive engineering,Autonomous vehicles,Laser radar,LiDAR,Sensors,Three-dimensional displays,ToF,Wavelength measurement},
  file = {/Users/dplane/Zotero/storage/H76ZAHCL/Roriz et al_2022_Automotive LiDAR Technology.pdf;/Users/dplane/Zotero/storage/TL2LHVVW/9455394.html}
}

@inproceedings{shan2020,
  title = {{{LIO-SAM}}: {{Tightly-coupled Lidar Inertial Odometry}} via {{Smoothing}} and {{Mapping}}},
  shorttitle = {{{LIO-SAM}}},
  booktitle = {2020 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Shan, Tixiao and Englot, Brendan and Meyers, Drew and Wang, Wei and Ratti, Carlo and Rus, Daniela},
  year = {2020},
  month = oct,
  pages = {5135--5142},
  issn = {2153-0866},
  doi = {10.1109/IROS45743.2020.9341176},
  urldate = {2025-03-16},
  abstract = {We propose a framework for tightly-coupled lidar inertial odometry via smoothing and mapping, LIO-SAM, that achieves highly accurate, real-time mobile robot trajectory estimation and map-building. LIO-SAM formulates lidar-inertial odometry atop a factor graph, allowing a multitude of relative and absolute measurements, including loop closures, to be incorporated from different sources as factors into the system. The estimated motion from inertial measurement unit (IMU) pre-integration de-skews point clouds and produces an initial guess for lidar odometry optimization. The obtained lidar odometry solution is used to estimate the bias of the IMU. To ensure high performance in real-time, we marginalize old lidar scans for pose optimization, rather than matching lidar scans to a global map. Scan-matching at a local scale instead of a global scale significantly improves the real-time performance of the system, as does the selective introduction of keyframes, and an efficient sliding window approach that registers a new keyframe to a fixed-size set of prior "sub-keyframes." The proposed method is extensively evaluated on datasets gathered from three platforms over various scales and environments.},
  keywords = {Laser radar,Optimization,Real-time systems,Registers,Smoothing methods,Three-dimensional displays,Trajectory},
  file = {/Users/dplane/Zotero/storage/AREX9CDQ/Shan et al_2020_LIO-SAM.pdf;/Users/dplane/Zotero/storage/PM83DYPZ/9341176.html}
}

@article{shao2022,
  title = {Multi-{{Scale Object Detection Model}} for {{Autonomous Ship Navigation}} in {{Maritime Environment}}},
  author = {Shao, Zeyuan and Lyu, Hongguang and Yin, Yong and Cheng, Tao and Gao, Xiaowei and Zhang, Wenjun and Jing, Qianfeng and Zhao, Yanjie and Zhang, Lunping},
  year = {2022},
  month = nov,
  journal = {Journal of Marine Science and Engineering},
  volume = {10},
  number = {11},
  pages = {1783--1803},
  issn = {2077-1312},
  doi = {10.3390/jmse10111783},
  urldate = {2025-02-14},
  abstract = {Accurate detection of sea-surface objects is vital for the safe navigation of autonomous ships. With the continuous development of artificial intelligence, electro-optical (EO) sensors such as video cameras are used to supplement marine radar to improve the detection of objects that produce weak radar signals and small sizes. In this study, we propose an enhanced convolutional neural network (CNN) named VarifocalNet * that improves object detection in harsh maritime environments. Specifically, the feature representation and learning ability of the VarifocalNet model are improved by using a deformable convolution module, redesigning the loss function, introducing a soft non-maximum suppression algorithm, and incorporating multi-scale prediction methods. These strategies improve the accuracy and reliability of our CNN-based detection results under complex sea conditions, such as in turbulent waves, sea fog, and water reflection. Experimental results under different maritime conditions show that our method significantly outperforms similar methods (such as SSD, YOLOv3, RetinaNet, Faster R-CNN, Cascade R-CNN) in terms of the detection accuracy and robustness for small objects. The maritime obstacle detection results were obtained under harsh imaging conditions to demonstrate the performance of our network model.},
  copyright = {https://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {/Users/dplane/Zotero/storage/45Y2JPIU/Shao et al. - 2022 - Multi-Scale Object Detection Model for Autonomous .pdf}
}

@inproceedings{shi2019,
  title = {{{PointRCNN}}: {{3D Object Proposal Generation}} and {{Detection From Point Cloud}}},
  shorttitle = {{{PointRCNN}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Shi, Shaoshuai and Wang, Xiaogang and Li, Hongsheng},
  year = {2019},
  pages = {770--779},
  urldate = {2025-03-16},
  file = {/Users/dplane/Zotero/storage/TXCK35WL/Shi et al_2019_PointRCNN.pdf}
}

@misc{shi2021,
  title = {{{PV-RCNN}}: {{Point-Voxel Feature Set Abstraction}} for {{3D Object Detection}}},
  shorttitle = {{{PV-RCNN}}},
  author = {Shi, Shaoshuai and Guo, Chaoxu and Jiang, Li and Wang, Zhe and Shi, Jianping and Wang, Xiaogang and Li, Hongsheng},
  year = {2021},
  month = apr,
  number = {arXiv:1912.13192},
  eprint = {1912.13192},
  primaryclass = {cs, eess},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1912.13192},
  urldate = {2024-05-19},
  abstract = {We present a novel and high-performance 3D object detection framework, named PointVoxel-RCNN (PV-RCNN), for accurate 3D object detection from point clouds. Our proposed method deeply integrates both 3D voxel Convolutional Neural Network (CNN) and PointNet-based set abstraction to learn more discriminative point cloud features. It takes advantages of efficient learning and high-quality proposals of the 3D voxel CNN and the flexible receptive fields of the PointNet-based networks. Specifically, the proposed framework summarizes the 3D scene with a 3D voxel CNN into a small set of keypoints via a novel voxel set abstraction module to save follow-up computations and also to encode representative scene features. Given the high-quality 3D proposals generated by the voxel CNN, the RoI-grid pooling is proposed to abstract proposal-specific features from the keypoints to the RoI-grid points via keypoint set abstraction with multiple receptive fields. Compared with conventional pooling operations, the RoI-grid feature points encode much richer context information for accurately estimating object confidences and locations. Extensive experiments on both the KITTI dataset and the Waymo Open dataset show that our proposed PV-RCNN surpasses state-of-the-art 3D detection methods with remarkable margins by using only point clouds. Code is available at https://github.com/open-mmlab/OpenPCDet.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {/Users/dplane/Zotero/storage/MU9GRBXN/Shi et al. - 2021 - PV-RCNN Point-Voxel Feature Set Abstraction for 3.pdf;/Users/dplane/Zotero/storage/YVMUCYY7/1912.html}
}

@article{su2023,
  title = {A Survey of Maritime Vision Datasets},
  author = {Su, Li and Chen, Yusheng and Song, Hao and Li, Wanyi},
  year = {2023},
  month = aug,
  journal = {Multimedia Tools and Applications},
  volume = {82},
  number = {19},
  pages = {28873--28893},
  issn = {1573-7721},
  doi = {10.1007/s11042-023-14756-9},
  urldate = {2025-02-15},
  abstract = {The field of computer vision has been applied in many topics and scenes, especially in the shipping business which occupies a large position in the world trade. With the development of ship intellectualization, the task of detection, tracking, segmentation and classification of interested targets become more and more important. Publicly available dataset is the foundation to promote research in shipping. Based on this intention, we systematically present a review of maritime datasets on maritime perception. In this paper, comparison is made in terms of data type, environment, ground authenticity, and applicable research directions. The aim of writing this paper is to help researchers quickly identify the most suitable dataset for their work.},
  langid = {english},
  keywords = {Computer vision,Maritime perception,Maritime ship dataset,Ship intellectualization},
  file = {/Users/dplane/Zotero/storage/WEQJLWZZ/Su et al_2023_A survey of maritime vision datasets.pdf}
}

@inproceedings{subedi2020,
  title = {Camera-{{LiDAR Data Fusion}} for {{Autonomous Mooring Operation}}},
  booktitle = {2020 15th {{IEEE Conference}} on {{Industrial Electronics}} and {{Applications}} ({{ICIEA}})},
  author = {Subedi, Dipendra and Jha, Ajit and Tyapin, Ilya and Hovland, Geir},
  year = {2020},
  month = nov,
  pages = {1176--1181},
  issn = {2158-2297},
  doi = {10.1109/ICIEA48937.2020.9248089},
  urldate = {2025-02-15},
  abstract = {The use of camera and LiDAR sensors to sense the environment has gained increasing popularity in robotics. Individual sensors, such as cameras and LiDARs, fail to meet the growing challenges in complex autonomous systems. One such scenario is autonomous mooring, where the ship has to be tied to a fixed rigid structure (bollard) to keep it stationary safely. The detection and pose estimation of the bollard based on data fusion from the camera and LiDAR are presented here. Firstly, a single shot extrinsic calibration of LiDAR with the camera is presented. Secondly, the camera-LiDAR data fusion method using camera intrinsic parameters and camera to LiDAR extrinsic parameters is proposed. Finally, the use of an image-based segmentation method to segment the corresponding point cloud from the fused camera-LiDAR data is developed and tailored for its application in autonomous mooring operation.},
  keywords = {autonomous mooring,camera calibration,Cameras,Data integration,favorite,Image segmentation,Laser radar,LiDAR calibration,Robot vision systems,sensor fusion,Sensors,Three-dimensional displays},
  file = {/Users/dplane/Zotero/storage/Z6HBKJUK/Subedi et al_2020_Camera-LiDAR Data Fusion for Autonomous Mooring Operation.pdf;/Users/dplane/Zotero/storage/PU6PNWZ6/9248089.html}
}

@article{sun,
  title = {Sparse {{Voxels Rasterization}}: {{Real-time High-fidelity Radiance Field Rendering}}},
  author = {Sun, Cheng and Choe, Jaesung and Loop, Charles and Ma, Wei-Chiu and Wang, Yu-Chiang Frank},
  langid = {english},
  file = {/Users/dplane/Zotero/storage/MBQ97JZD/Sun et al. - Sparse Voxels Rasterization Real-time High-ﬁdelit.pdf}
}

@misc{tan2020,
  title = {{{EfficientDet}}: {{Scalable}} and {{Efficient Object Detection}}},
  shorttitle = {{{EfficientDet}}},
  author = {Tan, Mingxing and Pang, Ruoming and Le, Quoc V.},
  year = {2020},
  month = jul,
  number = {arXiv:1911.09070},
  eprint = {1911.09070},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1911.09070},
  urldate = {2025-04-24},
  abstract = {Model efficiency has become increasingly important in computer vision. In this paper, we systematically study neural network architecture design choices for object detection and propose several key optimizations to improve efficiency. First, we propose a weighted bi-directional feature pyramid network (BiFPN), which allows easy and fast multiscale feature fusion; Second, we propose a compound scaling method that uniformly scales the resolution, depth, and width for all backbone, feature network, and box/class prediction networks at the same time. Based on these optimizations and better backbones, we have developed a new family of object detectors, called EfficientDet, which consistently achieve much better efficiency than prior art across a wide spectrum of resource constraints. In particular, with single model and single-scale, our EfficientDet-D7 achieves state-of-the-art 55.1 AP on COCO test-dev with 77M parameters and 410B FLOPs, being 4x - 9x smaller and using 13x - 42x fewer FLOPs than previous detectors. Code is available at https://github.com/google/automl/tree/master/efficientdet.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {/Users/dplane/Zotero/storage/XG2PXB4U/Tan et al_2020_EfficientDet.pdf;/Users/dplane/Zotero/storage/Y6Y7A6FU/1911.html}
}

@misc{thompson2017,
  title = {Maritime {{Object Detection}}, {{Tracking}}, and {{Classification Using Lidar}} and {{Vision-Based Sensor Fusion}}},
  author = {Thompson, David},
  year = {2017},
  month = nov,
  keywords = {Masters Thesis},
  file = {/Users/dplane/Zotero/storage/GHDNDT5U/Thompson_2017_Maritime Object Detection, Tracking, and Classification Using Lidar and.pdf;/Users/dplane/Zotero/storage/WRPX37BP/377.html}
}

@article{thompson2019b,
  title = {Efficient {{LiDAR-Based Object Segmentation}} and {{Mapping}} for {{Maritime Environments}}},
  author = {Thompson, David and Coyle, Eric and Brown, Jeremy},
  year = {2019},
  month = apr,
  journal = {IEEE Journal of Oceanic Engineering},
  volume = {44},
  number = {2},
  pages = {352--362},
  issn = {1558-1691},
  doi = {10.1109/JOE.2019.2898762},
  urldate = {2025-02-22},
  abstract = {This paper proposes a method that utilizes a 3-D occupancy grid to efficiently map a large area while retaining simple representations of objects for path planning and provide spatial characteristics of objects, which may be used for object classification. To enable large-scale mapping of objects, a region around the unmanned surface vehicle (USV) is defined where a high density of LiDAR returns is expected, termed the visibility horizon. The polygon intersection between the visibility horizon and the newly detected objects is computed, as well as the polygon subtraction of the visibility horizon from the mapped list of polygons. The two polygon lists are then combined using a polygon union operation, with the objects retaining class designations. The result is a 2-D map that contains polygon representations of objects, where the object is described with a tunable number of vertices and may have an associated object class. Thus, providing necessary information for path planning and tasking. The resultant polygons are shown here to be accurate to 20 cm using a 10-cm occupancy grid and 16-ft-long unmanned surface vehicles with four multibeam LiDAR sensors.},
  keywords = {Global Positioning System,Laser radar,LiDAR,mapping,object segmentation,Object segmentation,occupancy grid,Path planning,Sea surface,Sensors},
  file = {/Users/dplane/Zotero/storage/TVJDZG8K/Thompson et al_2019_Efficient LiDAR-Based Object Segmentation and Mapping for Maritime Environments.pdf;/Users/dplane/Zotero/storage/RIEJ7BRH/8656488.html}
}

@article{thompson2023,
  title = {Neural {{Network Fusion}} of {{Multi-Modal Sensor Data For Autonomous Surface Vessels}}},
  author = {Thompson, David},
  year = {2023},
  month = apr,
  journal = {Doctoral Dissertations and Master's Theses},
  keywords = {Phd Dissertation},
  file = {/Users/dplane/Zotero/storage/VB7LSWXG/Thompson - Neural Network Fusion of Multi-Modal Sensor Data F.pdf;/Users/dplane/Zotero/storage/AULNLBPS/741.html}
}

@article{tufekci2023,
  title = {Review of Lidar-Image Sensor Fusion Methods in {{3D}} Object Detection and the Experiment of Camera-Lidar Fusion in Autonomous Driving},
  author = {Tufekci, Zeynep},
  year = {2023},
  urldate = {2025-02-15},
  abstract = {In Autonomous driving, the LiDAR sensor has great importance to catch 3D shapes and visualize the environment around cars. These cars have also mounted camera systems calibrated with LiDAR sensors. These camera systems have multiple cameras with overlapping fields of view which totally cover 360{$^\circ$}. In this research, we focus on 3D Object Detection and Semantic Segmentation frameworks adopting sensor fusion technologies specifically camera-LiDAR and LiDAR-only frameworks. Also, how the camera view helps LiDAR point cloud is discussed. Early fusion approach by fusing point cloud and pseudo-LiDAR in the early step before the object detection network is experimented within the implementation. The depth map is generated from a single camera view. Depth map from front-view camera image which has forward direction information is adopted in this experiment while merging multi-camera images and merging depth maps are discussed at the same time. Pseudo-LiDAR is generated from a depth map while encountering distortion problems due to an unknown depth shift. To overcome this problem and find exact metric reconstruction, some possible solutions are offered. One possible solution is that pseudo-LiDAR uses a 3D point cloud to learn this unknown depth shift. To align two 3D point clouds, one original and one virtual, into one common base, ground extracted from both point clouds. Pandaset [1] which has 360{$^\circ$} and forward-facing LiDAR point cloud and multi-view images is exploited as an autonomous driving dataset.--Author's abstract},
  keywords = {automotive,fusion},
  file = {/Users/dplane/Zotero/storage/F9GASTS4/Tufekci_2023_Review of lidar-image sensor fusion methods in 3D object detection and the.pdf}
}

@inproceedings{tzeng2017,
  title = {Adversarial {{Discriminative Domain Adaptation}}},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Tzeng, Eric and Hoffman, Judy and Saenko, Kate and Darrell, Trevor},
  year = {2017},
  pages = {7167--7176},
  urldate = {2025-03-16},
  file = {/Users/dplane/Zotero/storage/5PKKI2P9/Tzeng et al_2017_Adversarial Discriminative Domain Adaptation.pdf}
}

@misc{ultralytics,
  author = {Glenn Jocher and Ayush Chaurasia and Jing Qiu},
  title = {Ultralytics YOLOv8},
  version = {8.0.0},
  year = {2023},
  url = {https://github.com/ultralytics/ultralytics},
  orcid = {0000-0001-5950-6979, 0000-0002-7603-6750, 0000-0003-3783-7069},
  license = {AGPL-3.0}
}

@inproceedings{vaswani2017,
  title = {Attention Is {{All}} You {{Need}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and ukasz Kaiser, {\L} and Polosukhin, Illia},
  year = {2017},
  volume = {30},
  publisher = {Curran Associates, Inc.},
  urldate = {2025-03-16},
  abstract = {The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.},
  keywords = {nn_core attention transformers},
  file = {/Users/dplane/Zotero/storage/4RWCHYZN/Vaswani et al_2017_Attention is All you Need.pdf}
}

@article{wang2019a,
  title = {Traffic {{Light Recognition With High Dynamic Range Imaging}} and {{Deep Learning}}},
  author = {Wang, Jian-Gang and Zhou, Lu-Bing},
  year = {2019},
  month = apr,
  journal = {IEEE Transactions on Intelligent Transportation Systems},
  volume = {20},
  number = {4},
  pages = {1341--1352},
  issn = {1558-0016},
  doi = {10.1109/TITS.2018.2849505},
  urldate = {2025-02-14},
  abstract = {Traffic light recognition (TLR) detects the traffic light from an image and then estimates the state of the light signal. TLR is important for autonomous vehicles because running against a red light could cause a deadly car accident. For a practical TLR system, computation time, varying illumination conditions, and false positives are three key challenges. In this paper, a novel real-time method is proposed to recognize a traffic light with high dynamic imaging and deep learning. In our approach, traffic light candidates are robustly detected from low exposure/dark frames and accurately classified using a deep neural network in consecutive high exposure/bright frames. This dual-channel mechanism can make full use of undistorted color and shape information in dark frames as well as the rich context in bright frames. In the dark channel, a non-parametric multi-color saliency model is proposed to simultaneously extract lights with different colors. A multiclass classifier with convolutional neural network (CNN) model is then adopted to reduce the number of false positives in the bright channel. The performance is further boosted by incorporating temporal trajectory tracking. In order to speed up the algorithm, a prior detection mask is generated to limit the potential search regions. Intensive experiments on a large dual-channel dataset show that the proposed approach outperforms the state-of-the-art real-time deep learning object detector, which could cause more false positives because it uses bright images only. The algorithm has been integrated into our autonomous vehicle and can work robustly on real roads.},
  keywords = {automotive,autonomous vehicle,Cameras,deep learning,high dynamic range imaging,Histograms,Image color analysis,Image recognition,Lighting,Machine learning,Robustness,Traffic light recognition},
  file = {/Users/dplane/Zotero/storage/GE3JC7PF/Wang_Zhou_2019_Traffic Light Recognition With High Dynamic Range Imaging and Deep Learning.pdf;/Users/dplane/Zotero/storage/3M6X3JUK/8419782.html}
}

@article{wang2020a,
  title = {Multi-{{Sensor Fusion}} in {{Automated Driving}}: {{A Survey}}},
  shorttitle = {Multi-{{Sensor Fusion}} in {{Automated Driving}}},
  author = {Wang, Zhangjing and Wu, Yu and Niu, Qingqing},
  year = {2020},
  journal = {IEEE Access},
  volume = {8},
  pages = {2847--2868},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2019.2962554},
  urldate = {2025-05-21},
  abstract = {With the significant development of practicability in deep learning and the ultra-high-speed information transmission rate of 5G communication technology will overcome the barrier of data transmission on the Internet of Vehicles, automated driving is becoming a pivotal technology affecting the future industry. Sensors are the key to the perception of the outside world in the automated driving system and whose cooperation performance directly determines the safety of automated driving vehicles. In this survey, we mainly discuss the different strategies of multi-sensor fusion in automated driving in recent years. The performance of conventional sensors and the necessity of multi-sensor fusion are analyzed, including radar, LiDAR, camera, ultrasonic, GPS, IMU, and V2X. According to the differences in the latest studies, we divide the fusion strategies into four categories and point out some shortcomings. Sensor fusion is mainly applied for multi-target tracking and environment reconstruction. We discuss the method of establishing a motion model and data association in multi-target tracking. At the end of the paper, we analyzed the deficiencies in the current studies and put forward some suggestions for further improvement in the future. Through this investigation, we hope to analyze the current situation of multi-sensor fusion in the automated driving process and provide more efficient and reliable fusion strategies.},
  keywords = {Automated driving,Cameras,data association,deep learning,environmental reconstruction,intent analysis,Laser radar,multi-sensor fusion strategy,multi-target tracking,Sensor fusion,Sensor phenomena and characterization,Sensor systems},
  file = {/Users/dplane/Zotero/storage/FLRQEEBL/Wang et al_2020_Multi-Sensor Fusion in Automated Driving.pdf}
}

@article{xie2024,
  title = {Reliable {{LiDAR-based}} Ship Detection and Tracking for {{Autonomous Surface Vehicles}} in Busy Maritime Environments},
  author = {Xie, Yongchang and Nanlal, Cassandra and Liu, Yuanchang},
  year = {2024},
  month = nov,
  journal = {Ocean Engineering},
  volume = {312},
  pages = {119288},
  issn = {0029-8018},
  doi = {10.1016/j.oceaneng.2024.119288},
  urldate = {2025-03-15},
  abstract = {Environmental perception is a crucial requirement of Autonomous Surface Vehicles (ASVs) if required to perform tasks safely in a dynamically complex operational environment. Most existing methods for ship detection rely on camera-based methods, which are sensitive to environmental conditions and cannot directly provide spatial location information related to detected targets. To overcome this limitation, we propose a LiDAR-based ship detection and tracking framework that can be applied to busy maritime environments. The proposed framework consists of two functional modules: a ship detection and multi-object tracking. For ship detection, a modularised network structure was adapted, allowing for ease of switching between different types of detection network to prioritise either detection accuracy, detection speed or a compromise of both, depending on the task requirements. A Kalman Filter-based multi-object tracking method is also implemented to compensate for any detections that may have been missed as a result of ship motions or occlusions, relying solely on the detection results. We also collected the first-ever real-world LiDAR dataset for maritime applications across the River Thames and marinas, including a range of ship types, with lengths ranging from 5 m up to 40 m, and different hull types. The datasets are organised in a similar manner to the KITTI datasets, which can be easily applied to the well-developed point cloud detection networks. Remarkably, our methods achieve an overall detection accuracy of 74.1\% in the collected datasets. The proposed framework and dataset make LiDAR-based environmental perception feasible for implementation in ASVs and support development in the autonomous maritime navigation field.},
  keywords = {Autonomous Surface Vehicle,Deep learning,LiDAR-based perception,Object tracking,Ship detection},
  file = {/Users/dplane/Zotero/storage/L2YDM7K3/Xie et al_2024_Reliable LiDAR-based ship detection and tracking for Autonomous Surface.pdf;/Users/dplane/Zotero/storage/CU2A8UMG/S002980182402626X.html}
}

@article{xu2023,
  title = {{{FusionRCNN}}: {{LiDAR-Camera Fusion}} for {{Two-Stage 3D Object Detection}}},
  shorttitle = {{{FusionRCNN}}},
  author = {Xu, Xinli and Dong, Shaocong and Xu, Tingfa and Ding, Lihe and Wang, Jie and Jiang, Peng and Song, Liqiang and Li, Jianan},
  year = {2023},
  month = jan,
  journal = {Remote Sensing},
  volume = {15},
  number = {7},
  pages = {1839},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2072-4292},
  doi = {10.3390/rs15071839},
  urldate = {2025-03-16},
  abstract = {Accurate and reliable perception systems are essential for autonomous driving and robotics. To achieve this, 3D object detection with multi-sensors is necessary. Existing 3D detectors have significantly improved accuracy by adopting a two-stage paradigm that relies solely on LiDAR point clouds for 3D proposal refinement. However, the sparsity of point clouds, particularly for faraway points, makes it difficult for the LiDAR-only refinement module to recognize and locate objects accurately. To address this issue, we propose a novel multi-modality two-stage approach called FusionRCNN. This approach effectively and efficiently fuses point clouds and camera images in the Regions of Interest (RoI). The FusionRCNN adaptively integrates both sparse geometry information from LiDAR and dense texture information from the camera in a unified attention mechanism. Specifically, FusionRCNN first utilizes RoIPooling to obtain an image set with a unified size and gets the point set by sampling raw points within proposals in the RoI extraction step. Then, it leverages an intra-modality self-attention to enhance the domain-specific features, followed by a well-designed cross-attention to fuse the information from two modalities. FusionRCNN is fundamentally plug-and-play and supports different one-stage methods with almost no architectural changes. Extensive experiments on KITTI and Waymo benchmarks demonstrate that our method significantly boosts the performances of popular detectors. Remarkably, FusionRCNN improves the strong SECOND baseline by 6.14\% mAP on Waymo and outperforms competing two-stage approaches.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {3D object detection,Fusion,LiDAR-camera fusion,RCNN,two-stage},
  file = {/Users/dplane/Zotero/storage/YMESBATK/Xu et al_2023_FusionRCNN.pdf}
}

@article{yang2024,
  title = {A Review of Intelligent Ship Marine Object Detection Based on {{RGB}} Camera},
  author = {Yang, Defu and Solihin, Mahmud Iwan and Zhao, Yawen and Yao, Benchun and Chen, Chaoran and Cai, Bingyu and Machmudah, Affiani},
  year = {2024},
  journal = {IET Image Processing},
  volume = {18},
  number = {2},
  pages = {281--297},
  issn = {1751-9667},
  doi = {10.1049/ipr2.12959},
  urldate = {2025-03-16},
  abstract = {The article presents a comprehensive summary of Intelligent Ship Marine Object Detection (ISMOD) based on the RGB Camera. Marine object detection plays a pivotal role in enabling intelligent ships to acquire crucial data and security assurances for autonomous navigation. Among the various detection sensors, the RGB Camera is an informative and cost-effective tool with a wide range of civil applications. In the beginning, the ISMOD metrics based on the RGB camera is analyzed from three significant aspects, namely accuracy, speed, and robustness. Subsequently, the latest research status and comparative overview are presented, encompassing three mainstream detection methods: traditional detection, deep learning detection, and sensor fusion detection. Finally, the existing challenges of ISMOD are discussed and future development trends are recommended. The results demonstrate that forthcoming development will predominantly concentrate on deep learning approaches, complemented by other techniques. It is imperative to advance detection performance in domains such as deep fusion, multi-feature extraction, multi-fusion technology, and lightweight detection architecture.},
  copyright = {{\copyright} 2023 The Authors. IET Image Processing published by John Wiley \& Sons Ltd on behalf of The Institution of Engineering and Technology.},
  langid = {english},
  keywords = {intelligent transportation systems,learning (artificial intelligence),marine navigation,object detection},
  file = {/Users/dplane/Zotero/storage/A3QJZBQZ/Yang et al_2024_A review of intelligent ship marine object detection based on RGB camera.pdf;/Users/dplane/Zotero/storage/59GKT6H7/ipr2.html}
}

@article{yeong2021,
  title = {Sensor and {{Sensor Fusion Technology}} in {{Autonomous Vehicles}}: {{A Review}}},
  shorttitle = {Sensor and {{Sensor Fusion Technology}} in {{Autonomous Vehicles}}},
  author = {Yeong, De Jong and {Velasco-Hernandez}, Gustavo and Barry, John and Walsh, Joseph},
  year = {2021},
  month = jan,
  journal = {Sensors},
  volume = {21},
  number = {6},
  pages = {2140},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {1424-8220},
  doi = {10.3390/s21062140},
  urldate = {2025-03-25},
  abstract = {With the significant advancement of sensor and communication technology and the reliable application of obstacle detection techniques and algorithms, automated driving is becoming a pivotal technology that can revolutionize the future of transportation and mobility. Sensors are fundamental to the perception of vehicle surroundings in an automated driving system, and the use and performance of multiple integrated sensors can directly determine the safety and feasibility of automated driving vehicles. Sensor calibration is the foundation block of any autonomous system and its constituent sensors and must be performed correctly before sensor fusion and obstacle detection processes may be implemented. This paper evaluates the capabilities and the technical performance of sensors which are commonly employed in autonomous vehicles, primarily focusing on a large selection of vision cameras, LiDAR sensors, and radar sensors and the various conditions in which such sensors may operate in practice. We present an overview of the three primary categories of sensor calibration and review existing open-source calibration packages for multi-sensor calibration and their compatibility with numerous commercial sensors. We also summarize the three main approaches to sensor fusion and review current state-of-the-art multi-sensor fusion techniques and algorithms for object detection in autonomous driving applications. The current paper, therefore, provides an end-to-end review of the hardware and software methods required for sensor fusion object detection. We conclude by highlighting some of the challenges in the sensor fusion field and propose possible future research directions for automated driving systems.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {autonomous vehicles,calibration,camera,lidar,obstacle detection,perception,radar,self-driving cars,sensor fusion},
  file = {/Users/dplane/Zotero/storage/ZMH3DFVN/Yeong et al. - 2021 - Sensor and Sensor Fusion Technology in Autonomous .pdf}
}

@article{zhang2021,
  title = {Survey on {{Deep Learning-Based Marine Object Detection}}},
  author = {Zhang, Ruolan and Li, Shaoxi and Ji, Guanfeng and Zhao, Xiuping and Li, Jing and Pan, Mingyang},
  year = {2021},
  journal = {Journal of Advanced Transportation},
  volume = {2021},
  number = {1},
  pages = {5808206},
  issn = {2042-3195},
  doi = {10.1155/2021/5808206},
  urldate = {2025-02-15},
  abstract = {We present a survey on marine object detection based on deep neural network approaches, which are state-of-the-art approaches for the development of autonomous ship navigation, maritime surveillance, shipping management, and other intelligent transportation system applications in the future. The fundamental task of maritime transportation surveillance and autonomous ship navigation is to construct a reachable visual perception system that requires high efficiency and high accuracy of marine object detection. Therefore, high-performance deep learning-based algorithms and high-quality marine-related datasets need to be summarized. This survey focuses on summarizing the methods and application scenarios of maritime object detection, analyzes the characteristics of different marine-related datasets, highlights the marine detection application of the YOLO series model, and also discusses the current limitations of object detection based on deep learning and possible breakthrough directions. The large-scale, multiscenario industrialized neural network training is an indispensable link to solve the practical application of marine object detection. A widely accepted and standardized large-scale marine object verification dataset should be proposed.},
  copyright = {Copyright {\copyright} 2021 Ruolan Zhang et al.},
  langid = {english},
  file = {/Users/dplane/Zotero/storage/H9VJR5BM/Zhang et al_2021_Survey on Deep Learning-Based Marine Object Detection.pdf;/Users/dplane/Zotero/storage/Y2B7SNZ7/5808206.html}
}

@article{zhang2022a,
  title = {Research of {{Maritime Object Detection Method}} in {{Foggy Environment Based}} on {{Improved Model SRC-YOLO}}},
  author = {Zhang, Yihong and Ge, Hang and Lin, Qin and Zhang, Ming and Sun, Qiantao},
  year = {2022},
  journal = {Sensors},
  volume = {22},
  number = {20},
  pages = {7786},
  publisher = {MDPI AG},
  address = {Basel, Switzerland},
  doi = {10.3390/s22207786},
  urldate = {2025-02-17},
  abstract = {An improved maritime object detection algorithm, SRC-YOLO, based on the YOLOv4-tiny, is proposed in the foggy environment to address the issues of false detection, missed detection, and low detection accuracy in complicated situations. To confirm the model's validity, an ocean dataset containing various concentrations of haze, target angles, and sizes was produced for the research. Firstly, the Single Scale Retinex (SSR) algorithm was applied to preprocess the dataset to reduce the interference of the complex scenes on the ocean. Secondly, in order to increase the model's receptive field, we employed a modified Receptive Field Block (RFB) module in place of the standard convolution in the Neck part of the model. Finally, the Convolutional Block Attention Module (CBAM), which integrates channel and spatial information, was introduced to raise detection performance by expanding the network model's attention to the context information in the feature map and the object location points. The experimental results demonstrate that the improved SRC-YOLO model effectively detects marine targets in foggy scenes by increasing the mean Average Precision (mAP) of detection results from 79.56\% to 86.15\%.},
  copyright = {{\copyright} 2022 by the authors.  Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (https://creativecommons.org/licenses/by/4.0/).  Notwithstanding the ProQuest Terms and Conditions, you may use this content in accordance with the terms of the License.},
  langid = {english},
  keywords = {Accuracy,Algorithms,CNN,convolutional block attention module,Datasets,Deep learning,Detection,Evacuations & rescues,Feature maps,Haze,inclement weather,object detection,Object detection,receptive field block,Spatial data,YOLOv4-tiny},
  file = {/Users/dplane/Zotero/storage/8IGZLRX6/Zhang et al. - 2022 - Research of Maritime Object Detection Method in Fo.pdf}
}

@inproceedings{zhou2018a,
  title = {{{VoxelNet}}: {{End-to-End Learning}} for {{Point Cloud Based 3D Object Detection}}},
  shorttitle = {{{VoxelNet}}},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Zhou, Yin and Tuzel, Oncel},
  year = {2018},
  pages = {4490--4499},
  urldate = {2025-03-16},
  file = {/Users/dplane/Zotero/storage/83ZHREN8/Zhou_Tuzel_2018_VoxelNet.pdf}
}

@article{zhu2024,
  title = {Camera, {{LiDAR}}, and {{IMU Based Multi-Sensor Fusion SLAM}}: {{A Survey}}},
  shorttitle = {Camera, {{LiDAR}}, and {{IMU Based Multi-Sensor Fusion SLAM}}},
  author = {Zhu, Jun and Li, Hongyi and Zhang, Tao},
  year = {2024},
  month = apr,
  journal = {Tsinghua Science and Technology},
  volume = {29},
  number = {2},
  pages = {415--429},
  issn = {1007-0214},
  doi = {10.26599/TST.2023.9010010},
  urldate = {2025-02-15},
  abstract = {In recent years, Simultaneous Localization And Mapping (SLAM) technology has prevailed in a wide range of applications, such as autonomous driving, intelligent robots, Augmented Reality (AR), and Virtual Reality (VR). Multi-sensor fusion using the most popular three types of sensors (e.g., visual sensor, LiDAR sensor, and IMU) is becoming ubiquitous in SLAM, in part because of the complementary sensing capabilities and the inevitable shortages (e.g., low precision and long-term drift) of the stand-alone sensor in challenging environments. In this article, we survey thoroughly the research efforts taken in this field and strive to provide a concise but complete review of the related work. Firstly, a brief introduction of the state estimator formation in SLAM is presented. Secondly, the state-of-the-art algorithms of different multi-sensor fusion algorithms are given. Then we analyze the deficiencies associated with the reviewed approaches and formulate some future research considerations. This paper can be considered as a brief guide to newcomers and a comprehensive reference for experienced researchers and engineers to explore new interesting orientations.},
  keywords = {Data integration,fusion,Laser radar,localization,Location awareness,multi-sensor fusion,navigation,Robot vision systems,Simultaneous localization and mapping,Simultaneous Localization And Mapping (SLAM),survey,Surveys,Visualization},
  file = {/Users/dplane/Zotero/storage/ZFDKWZ9Z/Zhu et al_2024_Camera, LiDAR, and IMU Based Multi-Sensor Fusion SLAM.pdf;/Users/dplane/Zotero/storage/5F6LAGFY/10258154.html}
}

@misc{zotero-1645,
  title = {Autonomous {{Vessels}} Are {{Becoming}} a {{Commercial Reality}}},
  journal = {The Maritime Executive},
  urldate = {2025-02-14},
  abstract = {TheAutonomous Ship Technology Symposium 2021conference brought together the largest public and priva...},
  howpublished = {https://maritime-executive.com/editorials/autonomous-vessels-are-becoming-a-commercial-reality},
  langid = {english},
  keywords = {News Article},
  file = {/Users/dplane/Zotero/storage/3Y3NY458/autonomous-vessels-are-becoming-a-commercial-reality.html}
}

@misc{zotero-1652,
  title = {More {{Data}} with {{Less Effort}} and {{Risk}}},
  urldate = {2025-02-14},
  abstract = {DEA Marine Services, a division of David Evans and Associates, Inc. (DEA), of Vancouver, Wash., has invested in and utilized Sea Machines Robotics\&rsq...},
  howpublished = {https://www.hydro-international.com/case-study/sea-machines-autonomy-enables-dea-marine-services-to-collect-more-data-with-less-effort-and-risk},
  langid = {english},
  keywords = {News Article},
  file = {/Users/dplane/Zotero/storage/AVQXU927/sea-machines-autonomy-enables-dea-marine-services-to-collect-more-data-with-less-effort-and-ris.html}
}

@misc{zotero-1745,
  title = {Camera {{Calibration}}},
  urldate = {2025-02-17},
  howpublished = {https://www.mathworks.com/help/vision/camera-calibration.html},
  keywords = {matlab},
  file = {/Users/dplane/Zotero/storage/XKH2REGE/camera-calibration.html}
}

@ misc{matlab_calibration,
year = {2024},
author = {The MathWorks Inc.},
title = {What Is Camera Calibration},
publisher = {The MathWorks Inc.},
address = {Natick, Massachusetts, United States},
url = {https://www.mathworks.com/help/vision/ug/camera-calibration.html}
}

@misc{matlab_vision,
year = {2024},
author = {The MathWorks Inc.},
title = {Computer Vision Toolbox version: 24.1 (R2024a)},
publisher = {The MathWorks Inc.},
address = {Natick, Massachusetts, United States},
url = {https://www.mathworks.com}
}

@misc{livox_manual,
year = {2019},
author = {Livox},
title = {Livox Horizon User Manual 1.0},
publisher = {Livox Inc.},
url = {https://www.livoxtech.com/horizon}
}

@book{Reinhard2010,
author = {Reinhard, Erik},
address = {Burlington, MA},
booktitle = {High dynamic range imaging : acquisition, display, and image-based lighting},
edition = {2nd ed.},
isbn = {9786612755460},
keywords = {High dynamic range imaging ; Photography -- Digital techniques ; Photography -- Printing processes ; Image processing ; Electroluminescent display systems ; Computer graphics},
language = {eng},
publisher = {Morgan Kaufmann/Elsevier},
series = {[The Morgan Kaufmann series in computer graphics]},
title = {High dynamic range imaging : acquisition, display, and image-based lighting },
year = {2010},
}
