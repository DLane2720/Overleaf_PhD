@article{ptp2008,
  author={},
  journal={IEEE Std 1588-2008 (Revision of IEEE Std 1588-2002)}, 
  title={IEEE Standard for a Precision Clock Synchronization Protocol for Networked Measurement and Control Systems}, 
  year={2008},
  volume={},
  number={},
  pages={1-269},
  keywords={IEEE Standards;Standards;Clocks;Trademarks;Synchronization;Protocols;Patents;1588-2008;boundary clock;clock;distributed system;master clock;measurement and control system;real-time clock;synchronized clock;transparent clock},
  doi={10.1109/IEEESTD.2008.4579760}},
  keywords = {1588-2008,boundary clock,Boundary Clock,clock,Clocks,distributed system,Grandmaster Clock,IEEE 1588,IEEE Standards,management,master clock,measurement and control system,Ordinary Clock,Patents,Protocols,real-time clock,security,Security,Standards,synchronization,Synchronization,synchronized clock,Trademarks,transparent clock,Transparent Clock},
  file = {/Users/dplane/Zotero/storage/DFF4IHWT/2008 - IEEE Standard for a Precision Clock Synchronization Protocol for Networked Measurement and Control S.pdf;/Users/dplane/Zotero/storage/HGT5G7MU/2020 - IEEE Standard for a Precision Clock Synchronization Protocol for Networked Measurement and Control S.pdf}
}

@article{ahmed2024,
  title = {Seal {{Pipeline}}: {{Enhancing Dynamic Object Detection}} and {{Tracking}} for {{Autonomous Unmanned Surface Vehicles}} in {{Maritime Environments}}},
  shorttitle = {Seal {{Pipeline}}},
  author = {Ahmed, Mohamed and Rasheed, Bader and Salloum, Hadi and Hegazy, Mostafa and Bahrami, Mohammad Reza and Chuchkalov, Mikhail},
  year = 2024,
  month = oct,
  journal = {Drones},
  volume = {8},
  number = {10},
  pages = {561},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2504-446X},
  doi = {10.3390/drones8100561},
  urldate = {2025-03-14},
  abstract = {This study addresses the dynamic object detection problem for Unmanned Surface Vehicles (USVs) in marine environments, which is complicated by boat tilting and camera illumination sensitivity. A novel pipeline named ``Seal'' is proposed to enhance detection accuracy and reliability. The approach begins with an innovative preprocessing stage that integrates data from the Inertial Measurement Unit (IMU) with LiDAR sensors to correct tilt-induced distortions in LiDAR point cloud data and reduce ripple effects around objects. The adjusted data are grouped using clustering algorithms and bounding boxes for precise object localization. Additionally, a specialized Kalman filter tailored for maritime environments mitigates object discontinuities between successive frames and addresses data sparsity caused by boat tilting. The methodology was evaluated using the VRX simulator, with experiments conducted on the Volga River using real USVs. The preprocessing effectiveness was assessed using the Root Mean Square Error (RMSE) and tracking accuracy was evaluated through detection rate metrics. The results demonstrate a 25\% to 30\% improvement in detection accuracy and show that the pipeline can aid industry even with sparse object representation across different frames. This study highlights the potential of integrating sensor fusion with specialized tracking for accurate dynamic object detection in maritime settings, establishing a new benchmark for USV navigation systems' accuracy and reliability.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {dynamic object detection,Kalman filter,LiDAR point cloud,unmanned surface vehicles},
  file = {/Users/dplane/Zotero/storage/CSB4LEXB/Ahmed et al_2024_Seal Pipeline.pdf}
}

@article{bae2023,
  title = {Survey on the {{Developments}} of {{Unmanned Marine Vehicles}}: {{Intelligence}} and {{Cooperation}}},
  shorttitle = {Survey on the {{Developments}} of {{Unmanned Marine Vehicles}}},
  author = {Bae, Inyeong and Hong, Jungpyo},
  year = 2023,
  month = jan,
  journal = {Sensors},
  volume = {23},
  number = {10},
  pages = {4643},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {1424-8220},
  doi = {10.3390/s23104643},
  urldate = {2025-10-22},
  abstract = {With the recent development of artificial intelligence (AI) and information and communication technology, manned vehicles operated by humans used on the ground, air, and sea are evolving into unmanned vehicles (UVs) that operate without human intervention. In particular, unmanned marine vehicles (UMVs), including unmanned underwater vehicles (UUVs) and unmanned surface vehicles (USVs), have the potential to complete maritime tasks that are unachievable for manned vehicles, lower the risk of man power, raise the power required to carry out military missions, and reap huge economic benefits. The aim of this review is to identify past and current trends in UMV development and present insights into future UMV development. The review discusses the potential benefits of UMVs, including completing maritime tasks that are unachievable for manned vehicles, lowering the risk of human intervention, and increasing power for military missions and economic benefits. However, the development of UMVs has been relatively tardy compared to that of UVs used on the ground and in the air due to adverse environments for UMV operation. This review highlights the challenges in developing UMVs, particularly in adverse environments, and the need for continued advancements in communication and networking technologies, navigation and sound exploration technologies, and multivehicle mission planning technologies to improve UMV cooperation and intelligence. Furthermore, the review identifies the importance of incorporating AI and machine learning technologies in UMVs to enhance their autonomy and ability to perform complex tasks. Overall, this review provides insights into the current state and future directions for UMV development.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {artificial intelligence,cooperation,swarm,unmanned marine vehicle},
  file = {/Users/dplane/Zotero/storage/4JR8CP7N/Bae and Hong - 2023 - Survey on the Developments of Unmanned Marine Vehicles Intelligence and Cooperation.pdf}
}

@article{bai2022,
  title = {A {{Review}} of {{Current Research}} and {{Advances}} in {{Unmanned Surface Vehicles}}},
  author = {Bai, Xiangen and Li, Bohan and Xu, Xiaofeng and Xiao, Yingjie},
  year = 2022,
  month = jun,
  journal = {Journal of Marine Science and Application},
  volume = {21},
  number = {2},
  pages = {47--58},
  issn = {1993-5048},
  doi = {10.1007/s11804-022-00276-9},
  urldate = {2025-03-14},
  abstract = {Following developments in artificial intelligence and big data technology, the level of intelligence in intelligent vessels has been improved. Intelligent vessels are being developed into unmanned surface vehicles (USVs), which have widely interested scholars in the shipping industry due to their safety, high efficiency, and energy-saving qualities. Considering the current development of USVs, the types of USVs and applications domestically and internationally are being investigated. USVs emerged with technological developments and their characteristics show some differences from traditional vessels, which brings some problems and advantages for their application. Certain maritime regulations are not applicable to USVs and must be changed. The key technologies in the current development of USVs are being investigated. While the level of intelligence is improving, the protection of cargo cannot be neglected. An innovative approach to the internal structure of USVs is proposed, where the inner hull can automatically recover its original state in case of outer hull tilting. Finally, we summarize the development status of USVs, which are an inevitable direction of development in the marine field.},
  langid = {english},
  keywords = {Artificial Intelligence,Intelligent vessel,Internal structure,Maritime supervision,Ship automation level,Shipping industry,Unmanned surface vehicle},
  file = {/Users/dplane/Zotero/storage/K76MBVJN/Bai et al_2022_A Review of Current Research and Advances in Unmanned Surface Vehicles.pdf}
}

@inproceedings{biasutti2019,
  title = {{{LU-Net}}: {{An Efficient Network}} for {{3D LiDAR Point Cloud Semantic Segmentation Based}} on {{End-to-End-Learned 3D Features}} and {{U-Net}}},
  shorttitle = {{{LU-Net}}},
  booktitle = {2019 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision Workshop}} ({{ICCVW}})},
  author = {Biasutti, Pierre and Lepetit, Vincent and Aujol, Jean-Fran{\c c}ois and Br{\'e}dif, Mathieu and Bugeau, Aur{\'e}lie},
  year = 2019,
  month = oct,
  pages = {942--950},
  issn = {2473-9944},
  doi = {10.1109/ICCVW.2019.00123},
  urldate = {2025-02-17},
  abstract = {We propose LU-Net - for LiDAR U-Net, a new method for the semantic segmentation of a 3D LiDAR point cloud. Instead of applying some global 3D segmentation method such as PointNet, we propose an end-to-end architecture for LiDAR point cloud semantic segmentation that efficiently solves the problem as an image processing problem. We first extract high-level 3D features for each point given its 3D neighbors. Then, these features are projected into a 2D multichannel range-image by considering the topology of the sensor. Thanks to these learned features and this projection, we can finally perform the segmentation using a simple U-Net segmentation network, which performs very well while being very efficient. In this way, we can exploit both the 3D nature of the data and the specificity of the LiDAR sensor. This approach outperforms the state-of-the-art by a large margin on the KITTI dataset, as our experiments show. Moreover, this approach operates at 24fps on a single GPU. This is above the acquisition rate of common LiDAR sensors which makes it suitable for real-time applications.},
  keywords = {3D point cloud,cnn,deep learning,Feature extraction,Image segmentation,Laser radar,LiDAR,Robot sensing systems,semantic segmentation,Semantics,Three-dimensional displays,Two dimensional displays},
  file = {/Users/dplane/Zotero/storage/MJR2QDS3/Biasutti et al. - 2019 - LU-Net An Efficient Network for 3D LiDAR Point Cl.pdf;/Users/dplane/Zotero/storage/ZB336YVH/9022291.html}
}

@article{brantner2021,
  title = {Controlling {{Ocean One}}: {{Human}}--Robot Collaboration for Deep-Sea Manipulation},
  shorttitle = {Controlling {{Ocean One}}},
  author = {Brantner, Gerald and Khatib, Oussama},
  year = 2021,
  journal = {Journal of Field Robotics},
  volume = {38},
  number = {1},
  pages = {28--51},
  issn = {1556-4967},
  doi = {10.1002/rob.21960},
  urldate = {2025-10-22},
  abstract = {Deploying robots to explore venues that are inaccessible to humans, or simply inhospitable, has been a longstanding ambition of scientists, engineers, and explorers across numerous fields. The deep sea exemplifies an environment that is largely uncharted and denies human presence. Central to exploration is the capacity to deliver dexterous robotic manipulation to this unstructured environment. Unmanned underwater vehicles (UUVs) are successful in providing passive solutions for observation and mapping but currently are far from capable of delivering human-level dexterity. The ones providing manipulation typically are UUVs coupled with position-controlled hydraulic arms using disjoint controllers for navigation and manipulation that require expert operators. Ocean One is a humanoid underwater robot designed specifically for underwater manipulation. In this paper, we present Ocean One's control architecture that, through a collaboration between this humanoid robot and a human pilot, enables the deployment of dexterous robotic manipulation to the deep sea. We provide detailed descriptions of this architecture's two main components: first, a whole-body controller that creates functional autonomy by coordinating manipulation, posture, and constraint tasks, and second, a set of haptic and visual human interfaces that enable intimate interaction while avoiding micromanagement. We test the presented methods in simulation and validate them in pool experiments and in two field deployments. On its maiden mission into the Mediterranean Sea, Ocean One explored the Lune, a French naval vessel that sank in 1664 off the coast of Toulon, France. In its second expedition, Ocean One assisted human divers in investigating underwater volcanic structures at Santorini, Greece.},
  copyright = {{\copyright} 2020 Wiley Periodicals LLC},
  langid = {english},
  keywords = {control,extreme environments,human-robot interaction,marinerobotics,mobile manipulation},
  file = {/Users/dplane/Zotero/storage/TJYEFVWT/Brantner and Khatib - 2021 - Controlling Ocean One Humanâ€“robot collaboration for deep-sea manipulation.pdf;/Users/dplane/Zotero/storage/ENNP46MM/rob.html}
}

@inproceedings{carthel2007,
  title = {Multisensor Tracking and Fusion for Maritime Surveillance},
  booktitle = {2007 10th {{International Conference}} on {{Information Fusion}}},
  author = {Carthel, Craig and Coraluppi, Stefano and Grignan, Patrick},
  year = 2007,
  month = jul,
  pages = {1--6},
  doi = {10.1109/ICIF.2007.4408025},
  urldate = {2025-02-15},
  abstract = {Over the past several years, the NATO Undersea Research Centre has conducted extensive research in multisensor networks for undersea surveillance, culminating in the development of the DMHT tracker. In this paper, we discuss upgrades to this technology and its application to maritime surveillance.},
  keywords = {Anomaly Detection,Automatic,Coastal Radar,Fusion,Fusion power generation,Identification System (AIS),Imagery,Maritime Surveillance,multisensor,Multisensor Fusion and Tracking,Radar detection,Radar imaging,Radar tracking,SAR,Sea measurements,Signal processing,Sonar equipment,Surveillance,Synthetic aperture radar,Target tracking,Track},
  file = {/Users/dplane/Zotero/storage/KC479BI2/Carthel et al_2007_Multisensor tracking and fusion for maritime surveillance.pdf;/Users/dplane/Zotero/storage/625RP48V/4408025.html}
}

@article{castano-londono2024,
  title = {Evolution of {{Algorithms}} and {{Applications}} for {{Unmanned Surface Vehicles}} in the {{Context}} of {{Small Craft}}: {{A Systematic Review}}},
  shorttitle = {Evolution of {{Algorithms}} and {{Applications}} for {{Unmanned Surface Vehicles}} in the {{Context}} of {{Small Craft}}},
  author = {{Castano-Londono}, Luis and Marrugo Llorente, Stefany del Pilar and {Paipa-Sanabria}, Edwin and {Orozco-Lopez}, Mar{\'i}a Bel{\'e}n and Fuentes Monta{\~n}a, David Ignacio and Gonzalez Montoya, Daniel},
  year = 2024,
  month = jan,
  journal = {Applied Sciences},
  volume = {14},
  number = {21},
  pages = {9693},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2076-3417},
  doi = {10.3390/app14219693},
  urldate = {2025-11-01},
  abstract = {The development of autonomous vessels and unmanned surface vehicles (USVs) has generated great interest in the scientific community due to their potential and advantages for use in various environments and applications. Several literature review papers have been produced from different perspectives, contributing to a better understanding of the topic and to the analysis of advances, challenges, and trends. We hypothesize that the greatest attention has been focused on the development of high-impact applications in the maritime sector. Additionally, we depart from the need to investigate the potential and advances of USVs in fluvial environments, which involve particular operating conditions, where there are different socio-environmental conditions and restrictions in terms of access to conventional energy sources and communication systems. In this sense, the main objective of this work is to study USVs in the particular context of small craft. The search for records was conducted in Scopus and Web of Science databases, covering studies published from 2000 to 16 May 2024. The methodology employed was based on the PRISMA 2020 guidelines, which is a widely recognized protocol that ensures quality and rigor in systematic reviews and bibliometric analyses. To optimize the data collection and selection process, the semaphore technique was additionally implemented, allowing for an efficient categorization of the studies found. This combined methodological approach facilitated a systematic and transparent evaluation of the literature. This study was developed based on three research questions about the evolution of research topics, areas of application, and types of algorithms related to USVs. The study of the evolution of works on USVs was carried out based on the results of the meta-analysis generated with the Bibliometrix tool. The study of applications and developments was carried out based on information obtained from the papers for six study categories: application environment, level of autonomy, application area, algorithm typology, methods, and electronic devices used. For each of the 387 papers identified in the databases, labeling was performed for the 359 screened papers with six study categories according to the availability of information in the title and abstract. In the categories application sector, autonomy level, application area and algorithm type/task, it was identified that most studies are oriented toward the maritime sector, the developments to achieve full autonomy for USVs, the development of designs or algorithms at the modeling and simulation level, and the development and implementation of algorithms for the GNC subsystems. Nevertheless, this research has revealed a much wider range of environments and applications beyond maritime, military, and commercial sectors. In addition, from the mapping of the types of algorithms used in the GNC architecture, the study provides information that can be used to guide the design of the subsystems that enable USV autonomy for civilian use in restricted environments.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {autonomy,bibliometric analysis,GNC architecture,unmanned surface vehicles},
  file = {/Users/dplane/Zotero/storage/ILBC4I3S/Castano-Londono et al. - 2024 - Evolution of Algorithms and Applications for Unmanned Surface Vehicles in the Context of Small Craft.pdf}
}

@inproceedings{chen2017,
  title = {Multi-View {{3D Object Detection Network}} for {{Autonomous Driving}}},
  booktitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Chen, Xiaozhi and Ma, Huimin and Wan, Ji and Li, Bo and Xia, Tian},
  year = 2017,
  month = jul,
  pages = {6526--6534},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2017.691},
  urldate = {2025-03-16},
  abstract = {This paper aims at high-accuracy 3D object detection in autonomous driving scenario. We propose Multi-View 3D networks (MV3D), a sensory-fusion framework that takes both LIDAR point cloud and RGB images as input and predicts oriented 3D bounding boxes. We encode the sparse 3D point cloud with a compact multi-view representation. The network is composed of two subnetworks: one for 3D object proposal generation and another for multi-view feature fusion. The proposal network generates 3D candidate boxes efficiently from the birds eye view representation of 3D point cloud. We design a deep fusion scheme to combine region-wise features from multiple views and enable interactions between intermediate layers of different paths. Experiments on the challenging KITTI benchmark show that our approach outperforms the state-of-the-art by around 25\% and 30\% AP on the tasks of 3D localization and 3D detection. In addition, for 2D detection, our approach obtains 14.9\% higher AP than the state-of-the-art on the hard data among the LIDAR-based methods.},
  keywords = {Birds,Laser radar,Object detection,Proposals,Three-dimensional displays,Two dimensional displays},
  file = {/Users/dplane/Zotero/storage/7495ACUV/Chen et al. - 2017 - Multi-view 3D Object Detection Network for Autonom.pdf;/Users/dplane/Zotero/storage/76YSMPB6/Chen et al_2017_Multi-view 3D Object Detection Network for Autonomous Driving.pdf;/Users/dplane/Zotero/storage/UX966S6R/8100174.html}
}

@article{chitta2023,
  title = {{{TransFuser}}: {{Imitation With Transformer-Based Sensor Fusion}} for {{Autonomous Driving}}},
  shorttitle = {{{TransFuser}}},
  author = {Chitta, Kashyap and Prakash, Aditya and Jaeger, Bernhard and Yu, Zehao and Renz, Katrin and Geiger, Andreas},
  year = 2023,
  month = nov,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {45},
  number = {11},
  pages = {12878--12895},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2022.3200245},
  urldate = {2025-10-22},
  abstract = {How should we integrate representations from complementary sensors for autonomous driving? Geometry-based fusion has shown promise for perception (e.g., object detection, motion forecasting). However, in the context of end-to-end driving, we find that imitation learning based on existing sensor fusion methods underperforms in complex driving scenarios with a high density of dynamic agents. Therefore, we propose TransFuser, a mechanism to integrate image and LiDAR representations using self-attention. Our approach uses transformer modules at multiple resolutions to fuse perspective view and bird's eye view feature maps. We experimentally validate its efficacy on a challenging new benchmark with long routes and dense traffic, as well as the official leaderboard of the CARLA urban driving simulator. At the time of submission, TransFuser outperforms all prior work on the CARLA leaderboard in terms of driving score by a large margin. Compared to geometry-based fusion, TransFuser reduces the average collisions per kilometer by 48\%.},
  keywords = {Attention,autonomous driving,Autonomous vehicles,Cameras,imitation learning,Laser radar,Semantics,sensor fusion,Sensor fusion,Three-dimensional displays,transformers,Transformers},
  file = {/Users/dplane/Zotero/storage/WKTL9E52/Chitta et al. - 2023 - TransFuser Imitation With Transformer-Based Sensor Fusion for Autonomous Driving.pdf}
}

@inproceedings{clunie2021,
  title = {Development of a {{Perception System}} for an {{Autonomous Surface Vehicle}} Using {{Monocular Camera}}, {{LIDAR}}, and {{Marine RADAR}}},
  booktitle = {2021 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Clunie, Thomas and DeFilippo, Michael and Sacarny, Michael and Robinette, Paul},
  year = 2021,
  month = may,
  pages = {14112--14119},
  issn = {2577-087X},
  doi = {10.1109/ICRA48506.2021.9561275},
  urldate = {2025-02-15},
  abstract = {This paper describes a set of software modules and algorithms for maritime object detection and tracking. The approach described here is designed to work in conjunction with various sensors from a maritime surface vessel (e.g. marine RADAR, LIDAR, camera). The described system identifies obstacles from the input sensors, estimates their state, and fuses the obstacle data into a consolidated report. The system is verified using experiments conducted on a live system and successfully demonstrates the ability to detect and track obstacles up to 450m away while operating at 7 fps. The software is open source and available at https://github.com/uml-marine-robotics/asv\_perception.},
  keywords = {autonomous surface vehicles,Autonomous systems,calibration,Cameras,classification,intelligent robots,Laser radar,LIDAR,marine RADAR,marine robotics,mobile agents unmanned autonomous vehicles,object detection,Object detection,Radar tracking,Robot sensing systems,segmentation,sensor fusion,Sensor fusion,Software algorithms},
  file = {/Users/dplane/Zotero/storage/ELDJNRXK/Clunie et al_2021_Development of a Perception System for an Autonomous Surface Vehicle using.pdf;/Users/dplane/Zotero/storage/DLQXEKQU/9561275.html}
}

@misc{coyleE,
  title = {Efficient {{Grid-Based Clustering}} and {{Concave Hull Extraction}} for {{Unstructured Point Clouds}}},
  author = {Coyle, Eric},
  urldate = {2023-11-09},
  abstract = {This paper presents a situational awareness technique for point clouds, called Grid-Based Clustering and Concave Hull Extraction (GB-CACHE), which is shown to be suitable for real-time implementation. GB-CACHE is able to efficiently segment and extract concave hulls from unstructured point clouds by sub-sampling these point clouds to a structured grid. The technique is specifically design for unmanned systems perceiving objects that are assumed to lie on a flat surface, such as the ground, water, or sea bed. In addition to segmentation and hull extraction, the technique is shown to enable the implementation of optional processes of point filtering, object classification, mapping, and object tracking. Example GB-CACHE results are shown from four separate unmanned case studies covering three different domains (surface, aerial, underwater), using three different sensing modalities (LiDAR, radar, sonar). Dense LiDAR point clouds are then used to analyze the computational efficiency of each main processes of GB-CACHE. Collectively, GB-CACHE is shown to be efficient enough for real-time implementation even with these dense point clouds and low to mid-grade computing solutions.},
  keywords = {Global Positioning System,Laser radar,LiDAR,mapping,object segmentation,Object segmentation,occupancy grid,Path planning,Sea surface,Sensors},
  file = {/Users/dplane/Zotero/storage/P9WUJSX4/Efficient_Grid_Based_Clustering_and_Concave_Hull_Extraction_for_Unsegmented_Point_Clouds.pdf;/Users/dplane/Zotero/storage/VWKZE93P/8656488.html}
}

@article{cui2022,
  title = {Deep {{Learning}} for {{Image}} and {{Point Cloud Fusion}} in {{Autonomous Driving}}: {{A Review}}},
  shorttitle = {Deep {{Learning}} for {{Image}} and {{Point Cloud Fusion}} in {{Autonomous Driving}}},
  author = {Cui, Yaodong and Chen, Ren and Chu, Wenbo and Chen, Long and Tian, Daxin and Li, Ying and Cao, Dongpu},
  year = 2022,
  month = feb,
  journal = {IEEE Transactions on Intelligent Transportation Systems},
  volume = {23},
  number = {2},
  pages = {722--739},
  issn = {1558-0016},
  doi = {10.1109/TITS.2020.3023541},
  urldate = {2025-03-16},
  abstract = {Autonomous vehicles were experiencing rapid development in the past few years. However, achieving full autonomy is not a trivial task, due to the nature of the complex and dynamic driving environment. Therefore, autonomous vehicles are equipped with a suite of different sensors to ensure robust, accurate environmental perception. In particular, the camera-LiDAR fusion is becoming an emerging research theme. However, so far there has been no critical review that focuses on deep-learning-based camera-LiDAR fusion methods. To bridge this gap and motivate future research, this article devotes to review recent deep-learning-based data fusion approaches that leverage both image and point cloud. This review gives a brief overview of deep learning on image and point cloud data processing. Followed by in-depth reviews of camera-LiDAR fusion methods in depth completion, object detection, semantic segmentation, tracking and online cross-sensor calibration, which are organized based on their respective fusion levels. Furthermore, we compare these methods on publicly available datasets. Finally, we identified gaps and over-looked challenges between current academic researches and real-world applications. Based on these observations, we provide our insights and point out promising research directions.},
  keywords = {Camera-LiDAR fusion,Convolution,deep learning,Deep learning,depth completion,Feature extraction,Geometry,Laser radar,object detection,semantic segmentation,Semantics,sensor fusion,Three-dimensional displays,tracking},
  file = {/Users/dplane/Zotero/storage/YR2LPNE4/Cui et al_2022_Deep Learning for Image and Point Cloud Fusion in Autonomous Driving.pdf;/Users/dplane/Zotero/storage/ZBD64QLB/9380166.html}
}

@inproceedings{das2022,
  title = {Sensor Fusion in Autonomous Vehicle Using {{LiDAR}} and Camera {{Sensor}}},
  booktitle = {2022 {{IEEE}} 10th {{Region}} 10 {{Humanitarian Technology Conference}} ({{R10-HTC}})},
  author = {Das, Diptadip and Adhikary, Nabanita and Chaudhury, Saurabh},
  year = 2022,
  month = sep,
  pages = {336--341},
  issn = {2572-7621},
  doi = {10.1109/R10-HTC54060.2022.9929588},
  urldate = {2025-02-15},
  abstract = {This paper presents a sensor fusion methodology for autonomous vehicles (AVs) using Light Detection and Ranging (LiDAR) and camera. Mostly sensors like camera or LiDAR is used only as the sensor for visual perception in AVs. But the hindrance comes during bad weather conditions, dim light or night time. To alleviate this problem, a method which combines both LiDAR and camera sensor using odometry is explored in this paper. The study also attempts to employ Extended Kalman Filter (EKF) to reduce error in position estimate of the vehicle.},
  keywords = {Autonomous vehicles,Camera,Cameras,Distance measurement,GPS,Kalman Filter,Kalman filters,Laser radar,LiDAR,Odometry,Point cloud compression,Real-time systems,Sensor fusion},
  file = {/Users/dplane/Zotero/storage/U7P8B57B/Das et al_2022_Sensor fusion in autonomous vehicle using LiDAR and camera Sensor.pdf;/Users/dplane/Zotero/storage/8EY6A4VX/9929588.html}
}

@misc{eckstein2024,
  title = {{{US Navy}}'s Four Unmanned Ships Return from {{Pacific}} Deployment},
  author = {Eckstein, Megan},
  year = 2024,
  month = jan,
  journal = {Defense News},
  urldate = {2025-02-14},
  abstract = {Four medium USV prototypes spent five months in the region working with the Navy-Marine team and allies to push the limits of concepts of operations.},
  chapter = {name},
  howpublished = {https://www.defensenews.com/naval/2024/01/16/us-navys-four-unmanned-ships-return-from-pacific-deployment/},
  langid = {english},
  keywords = {News Article},
  file = {/Users/dplane/Zotero/storage/VKARHDCY/us-navys-four-unmanned-ships-return-from-pacific-deployment.html}
}

@inproceedings{farahnakian2018,
  title = {Object {{Detection Based}} on {{Multi-sensor Proposal Fusion}} in {{Maritime Environment}}},
  booktitle = {2018 17th {{IEEE International Conference}} on {{Machine Learning}} and {{Applications}} ({{ICMLA}})},
  author = {Farahnakian, Fahimeh and Haghbayan, Mohammad-Hashem and Poikonen, Jonne and Laurinen, Markus and Nevalainen, Paavo and Heikkonen, Jukka},
  year = 2018,
  month = dec,
  pages = {971--976},
  publisher = {IEEE},
  address = {Orlando, FL},
  doi = {10.1109/ICMLA.2018.00158},
  urldate = {2025-02-15},
  abstract = {In this paper, we propose an effective object detection framework based on proposal fusion of multiple sensors such as infrared camera, RGB cameras, radar and LiDAR. Our framework first applies the Selective Search (SS) method on RGB image data to extract possible candidate proposals which likely contain the objects of interest. Then it uses the information from other sensors in order to reduce the number of generated proposals by SS and find more dense proposals. Finally, the class of objects within the final proposals are identified by Convolutional Neural Network (CNN). Experimental results on real dataset demonstrate that our framework can precisely detect meaningful object regions using a smaller number of proposals than other object proposals methods. Further, our framework can achieve reliable object detection and classification results in maritime environments.},
  isbn = {978-1-5386-6805-4},
  langid = {english},
  keywords = {multisensor},
  file = {/Users/dplane/Zotero/storage/T6N9CJ4I/Farahnakian et al. - 2018 - Object Detection Based on Multi-sensor Proposal Fu.pdf;/Users/dplane/Zotero/storage/3VHX9UEK/8614183.html}
}

@article{farahnakian2020,
  title = {Deep {{Learning Based Multi-Modal Fusion Architectures}} for {{Maritime Vessel Detection}}},
  author = {Farahnakian, Fahimeh and Heikkonen, Jukka},
  year = 2020,
  month = jan,
  journal = {Remote Sensing},
  volume = {12},
  number = {16},
  pages = {2509},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2072-4292},
  doi = {10.3390/rs12162509},
  urldate = {2025-02-15},
  abstract = {Object detection is a fundamental computer vision task for many real-world applications. In the maritime environment, this task is challenging due to varying light, view distances, weather conditions, and sea waves. In addition, light reflection, camera motion and illumination changes may cause to false detections. To address this challenge, we present three fusion architectures to fuse two imaging modalities: visible and infrared. These architectures can provide complementary information from two modalities in different levels: pixel-level, feature-level, and decision-level. They employed deep learning for performing fusion and detection. We investigate the performance of the proposed architectures conducting a real marine image dataset, which is captured by color and infrared cameras on-board a vessel in the Finnish archipelago. The cameras are employed for developing autonomous ships, and collect data in a range of operation and climatic conditions. Experiments show that feature-level fusion architecture outperforms the state-of-the-art other fusion level architectures.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {autonomous vehicles,convolutional neural networks,deep learning,favorite,marine environment,multi-sensor fusion,object detection},
  file = {/Users/dplane/Zotero/storage/ZT9I8YBH/Farahnakian_Heikkonen_2020_Deep Learning Based Multi-Modal Fusion Architectures for Maritime Vessel.pdf;/Users/dplane/Zotero/storage/V3MP3HIL/2509.html}
}

@article{feng2021,
  title = {Deep {{Multi-Modal Object Detection}} and {{Semantic Segmentation}} for {{Autonomous Driving}}: {{Datasets}}, {{Methods}}, and {{Challenges}}},
  shorttitle = {Deep {{Multi-Modal Object Detection}} and {{Semantic Segmentation}} for {{Autonomous Driving}}},
  author = {Feng, Di and {Haase-Sch{\"u}tz}, Christian and Rosenbaum, Lars and Hertlein, Heinz and Gl{\"a}ser, Claudius and Timm, Fabian and Wiesbeck, Werner and Dietmayer, Klaus},
  year = 2021,
  month = mar,
  journal = {IEEE Transactions on Intelligent Transportation Systems},
  volume = {22},
  number = {3},
  pages = {1341--1360},
  issn = {1558-0016},
  doi = {10.1109/TITS.2020.2972974},
  urldate = {2025-03-16},
  abstract = {Recent advancements in perception for autonomous driving are driven by deep learning. In order to achieve robust and accurate scene understanding, autonomous vehicles are usually equipped with different sensors (e.g. cameras, LiDARs, Radars), and multiple sensing modalities can be fused to exploit their complementary properties. In this context, many methods have been proposed for deep multi-modal perception problems. However, there is no general guideline for network architecture design, and questions of ``what to fuse'', ``when to fuse'', and ``how to fuse'' remain open. This review paper attempts to systematically summarize methodologies and discuss challenges for deep multi-modal object detection and semantic segmentation in autonomous driving. To this end, we first provide an overview of on-board sensors on test vehicles, open datasets, and background information for object detection and semantic segmentation in autonomous driving research. We then summarize the fusion methodologies and discuss challenges and open questions. In the appendix, we provide tables that summarize topics and methods. We also provide an interactive online platform to navigate each reference: https://boschresearch.github.io/multimodalperception/.},
  keywords = {autonomous driving,Autonomous vehicles,Cameras,deep learning,Fuses,Laser radar,Multi-modality,object detection,Object detection,semantic segmentation,Sensors},
  file = {/Users/dplane/Zotero/storage/UHGPBCSA/Feng et al_2021_Deep Multi-Modal Object Detection and Semantic Segmentation for Autonomous.pdf;/Users/dplane/Zotero/storage/MS47RZGG/9000872.html}
}

@article{ferreira2022,
  title = {Editorial: {{Navigation}} and {{Perception}} for {{Autonomous Surface Vessels}}},
  shorttitle = {Editorial},
  author = {Ferreira, Fausto and Quattrini Li, Alberto and R{\o}dseth, {\O}rnulf J.},
  year = 2022,
  month = may,
  journal = {Frontiers in Robotics and AI},
  volume = {9},
  pages = {918464},
  issn = {2296-9144},
  doi = {10.3389/frobt.2022.918464},
  urldate = {2025-11-01},
  pmcid = {PMC9163955},
  pmid = {35669289},
  file = {/Users/dplane/Zotero/storage/W6JPGS6K/Ferreira et al. - 2022 - Editorial Navigation and Perception for Autonomous Surface Vessels.pdf}
}

@article{fischer2022,
  title = {A {{RoboStack Tutorial}}: {{Using}} the {{Robot Operating System Alongside}} the {{Conda}} and {{Jupyter Data Science Ecosystems}}},
  shorttitle = {A {{RoboStack Tutorial}}},
  author = {Fischer, Tobias and Vollprecht, Wolf and Traversaro, Silvio and Yen, Sean and Herrero, Carlos and Milford, Michael},
  year = 2022,
  month = jun,
  journal = {IEEE Robotics \& Automation Magazine},
  volume = {29},
  number = {2},
  pages = {65--74},
  issn = {1558-223X},
  doi = {10.1109/MRA.2021.3128367},
  urldate = {2025-05-22},
  abstract = {The Robot Operating System (ROS) has become the de facto standard middleware in the robotics community [1]. ROS bundles everything, from low-level drivers to tools that transform among coordinate systems, to state-of-the-art perception and control algorithms. One of ROS's key merits is the rich ecosystem of standardized tools to build and distribute ROS-based software.},
  keywords = {Control systems,Ecosystems,Middleware,Operating systems,Robot kinematics,Software engineering,Tutorials},
  file = {/Users/dplane/Zotero/storage/J3J5RRGW/Fischer et al_2022_A RoboStack Tutorial.pdf}
}

@inproceedings{furgale2013,
  title = {Unified Temporal and Spatial Calibration for Multi-Sensor Systems},
  booktitle = {2013 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}}},
  author = {Furgale, Paul and Rehder, Joern and Siegwart, Roland},
  year = 2013,
  month = nov,
  pages = {1280--1286},
  issn = {2153-0866},
  doi = {10.1109/IROS.2013.6696514},
  urldate = {2025-10-22},
  abstract = {In order to increase accuracy and robustness in state estimation for robotics, a growing number of applications rely on data from multiple complementary sensors. For the best performance in sensor fusion, these different sensors must be spatially and temporally registered with respect to each other. To this end, a number of approaches have been developed to estimate these system parameters in a two stage process, first estimating the time offset and subsequently solving for the spatial transformation between sensors. In this work, we present on a novel framework for jointly estimating the temporal offset between measurements of different sensors and their spatial displacements with respect to each other. The approach is enabled by continuous-time batch estimation and extends previous work by seamlessly incorporating time offsets within the rigorous theoretical framework of maximum likelihood estimation. Experimental results for a camera to inertial measurement unit (IMU) calibration prove the ability of this framework to accurately estimate time offsets up to a fraction of the smallest measurement period.},
  keywords = {Calibration,Cameras,Estimation,Measurement uncertainty,Sensors,Splines (mathematics),Time measurement},
  file = {/Users/dplane/Zotero/storage/IIVABFGT/Furgale et al. - 2013 - Unified temporal and spatial calibration for multi-sensor systems.pdf}
}

@inproceedings{garcia-garcia2016,
  title = {{{PointNet}}: {{A 3D Convolutional Neural Network}} for Real-Time Object Class Recognition},
  shorttitle = {{{PointNet}}},
  booktitle = {2016 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  author = {{Garcia-Garcia}, A. and {Gomez-Donoso}, F. and {Garcia-Rodriguez}, J. and {Orts-Escolano}, S. and Cazorla, M. and {Azorin-Lopez}, J.},
  year = 2016,
  month = jul,
  pages = {1578--1584},
  issn = {2161-4407},
  doi = {10.1109/IJCNN.2016.7727386},
  urldate = {2025-02-22},
  abstract = {During the last few years, Convolutional Neural Networks are slowly but surely becoming the default method solve many computer vision related problems. This is mainly due to the continuous success that they have achieved when applied to certain tasks such as image, speech, or object recognition. Despite all the efforts, object class recognition methods based on deep learning techniques still have room for improvement. Most of the current approaches do not fully exploit 3D information, which has been proven to effectively improve the performance of other traditional object recognition methods. In this work, we propose PointNet, a new approach inspired by VoxNet and 3D ShapeNets, as an improvement over the existing methods by using density occupancy grids representations for the input data, and integrating them into a supervised Convolutional Neural Network architecture. An extensive experimentation was carried out, using ModelNet - a large-scale 3D CAD models dataset - to train and test the system, to prove that our approach is on par with state-of-the-art methods in terms of accuracy while being able to perform recognition under real-time constraints.},
  keywords = {Computer architecture,Machine learning,Neural networks,Object recognition,PointNet,Solid modeling,Three-dimensional displays,Two dimensional displays},
  file = {/Users/dplane/Zotero/storage/B5E8SE2I/Garcia-Garcia et al_2016_PointNet.pdf;/Users/dplane/Zotero/storage/QY3UAIDX/7727386.html}
}

@inproceedings{geiger2012,
  title = {Are We Ready for Autonomous Driving? {{The KITTI}} Vision Benchmark Suite},
  shorttitle = {Are We Ready for Autonomous Driving?},
  booktitle = {2012 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Geiger, Andreas and Lenz, Philip and Urtasun, Raquel},
  year = 2012,
  month = jun,
  pages = {3354--3361},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2012.6248074},
  urldate = {2025-03-16},
  abstract = {Today, visual recognition systems are still rarely employed in robotics applications. Perhaps one of the main reasons for this is the lack of demanding benchmarks that mimic such scenarios. In this paper, we take advantage of our autonomous driving platform to develop novel challenging benchmarks for the tasks of stereo, optical flow, visual odometry/SLAM and 3D object detection. Our recording platform is equipped with four high resolution video cameras, a Velodyne laser scanner and a state-of-the-art localization system. Our benchmarks comprise 389 stereo and optical flow image pairs, stereo visual odometry sequences of 39.2 km length, and more than 200k 3D object annotations captured in cluttered scenarios (up to 15 cars and 30 pedestrians are visible per image). Results from state-of-the-art algorithms reveal that methods ranking high on established datasets such as Middlebury perform below average when being moved outside the laboratory to the real world. Our goal is to reduce this bias by providing challenging benchmarks with novel difficulties to the computer vision community. Our benchmarks are available online at: www.cvlibs.net/datasets/kitti},
  keywords = {Benchmark testing,Cameras,Measurement,Optical imaging,Optical sensors,Visualization},
  file = {/Users/dplane/Zotero/storage/Y666P9UW/Geiger et al_2012_Are we ready for autonomous driving.pdf;/Users/dplane/Zotero/storage/7DKIWKUK/6248074.html}
}

@inproceedings{girshick2014,
  title = {Rich {{Feature Hierarchies}} for {{Accurate Object Detection}} and {{Semantic Segmentation}}},
  booktitle = {2014 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
  year = 2014,
  month = jun,
  pages = {580--587},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2014.81},
  urldate = {2025-02-14},
  abstract = {Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30\% relative to the previous best result on VOC 2012 -- achieving a mAP of 53.3\%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also present experiments that provide insight into what the network learns, revealing a rich hierarchy of image features. Source code for the complete system is available at http://www.cs.berkeley.edu/ rbg/rcnn.},
  keywords = {Feature extraction,nn_core,object deteciton,Object detection,Proposals,Support vector machines,Training,Vectors,Visualization},
  file = {/Users/dplane/Zotero/storage/CJRLEN2A/Girshick et al_2014_Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation.pdf;/Users/dplane/Zotero/storage/N9STWHI6/6909475.html}
}

@article{gu2024,
  title = {{{CLFT}}: {{Camera-LiDAR Fusion Transformer}} for {{Semantic Segmentation}} in {{Autonomous Driving}}},
  shorttitle = {{{CLFT}}},
  author = {Gu, Junyi and Bellone, Mauro and Pivo{\v n}ka, Tom{\'a}{\v s} and Sell, Raivo},
  year = 2024,
  journal = {IEEE Transactions on Intelligent Vehicles},
  eprint = {2404.17793},
  primaryclass = {cs},
  pages = {1--12},
  issn = {2379-8904, 2379-8858},
  doi = {10.1109/TIV.2024.3454971},
  urldate = {2025-10-22},
  abstract = {Critical research about camera-and-LiDAR-based semantic object segmentation for autonomous driving significantly benefited from the recent development of deep learning. Specifically, the vision transformer is the novel ground-breaker that successfully brought the multi-head-attention mechanism to computer vision applications. Therefore, we propose a vision-transformer-based network to carry out camera-LiDAR fusion for semantic segmentation applied to autonomous driving. Our proposal uses the novel progressive-assemble strategy of vision transformers on a double-direction network and then integrates the results in a cross-fusion strategy over the transformer decoder layers. Unlike other works in the literature, our camera-LiDAR fusion transformers have been evaluated in challenging conditions like rain and low illumination, showing robust performance. The paper reports the segmentation results over the vehicle and human classes in different modalities: camera-only, LiDAR-only, and camera-LiDAR fusion. We perform coherent controlled benchmark experiments of CLFT against other networks that are also designed for semantic segmentation. The experiments aim to evaluate the performance of CLFT independently from two perspectives: multimodal sensor fusion and backbone architectures. The quantitative assessments show our CLFT networks yield an improvement of up to 10\% for challenging dark-wet conditions when comparing with Fully-Convolutional-Neural-Network-based (FCN) camera-LiDAR fusion neural network. Contrasting to the network with transformer backbone but using single modality input, the all-around improvement is 5-10\%.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  file = {/Users/dplane/Zotero/storage/45TLVDER/Gu et al. - 2024 - CLFT Camera-LiDAR Fusion Transformer for Semantic Segmentation in Autonomous Driving.pdf;/Users/dplane/Zotero/storage/C2DE3T7Y/2404.html}
}

@inproceedings{guo2023,
  title = {Investigating the~{{Transferability}} of~{{YOLOv5-Based Water Surface Object Detection Model}} in~{{Maritime Applications}}},
  booktitle = {International {{Conference}} on {{Neural Computing}} for {{Advanced Applications}}},
  author = {Guo, Yu and Chen, Zhuo and Wang, Qi and Bao, Tao and Zhou, Zexing},
  editor = {Zhang, Haijun and Ke, Yinggen and Wu, Zhou and Hao, Tianyong and Zhang, Zhao and Meng, Weizhi and Mu, Yuanyuan},
  year = 2023,
  pages = {103--115},
  publisher = {Springer Nature},
  address = {Singapore},
  doi = {10.1007/978-981-99-5847-4_8},
  abstract = {Object detection on the water surface is crucial for unmanned surface vehicles in maritime environments. Despite the challenges posed by variable lighting and ocean conditions, advancements in this field are necessary. In this paper, we investigate the transferability of YOLOv5-based water surface object detection models in cross-domain scenarios. The evaluation is based on publicly available datasets and two newly proposed datasets, Taihu Trial Dataset(TTD) and Fuxian Trial Dataset(FTD), which contain similar target classes but distinct scene and features. Results from extensive experiments indicate that zero-shot transfer is challenging, but a limited number of samples from the target domain can greatly enhance model performance.},
  isbn = {978-981-99-5847-4},
  langid = {english},
  keywords = {Intelligent Perception,Maritime,Model Transferability,Object detection,Yolo},
  file = {/Users/dplane/Zotero/storage/L6EJCEGE/Guo et al_2023_Investigating the Transferability of YOLOv5-Based Water Surface Object.pdf}
}

@inproceedings{haghbayan2018,
  title = {An {{Efficient Multi-sensor Fusion Approach}} for {{Object Detection}} in {{Maritime Environments}}},
  booktitle = {2018 21st {{International Conference}} on {{Intelligent Transportation Systems}} ({{ITSC}})},
  author = {Haghbayan, Mohammad-Hashem and Farahnakian, Fahimeh and Poikonen, Jonne and Laurinen, Markus and Nevalainen, Paavo and Plosila, Juha and Heikkonen, Jukka},
  year = 2018,
  month = nov,
  pages = {2163--2170},
  issn = {2153-0017},
  doi = {10.1109/ITSC.2018.8569890},
  urldate = {2025-02-14},
  abstract = {Robust real-time object detection and tracking are challenging problems in autonomous transportation systems due to operation of algorithms in inherently uncertain and dynamic environments and rapid movement of objects. Therefore, tracking and detection algorithms must cooperate with each other to achieve smooth tracking of detected objects that later can be used by the navigation system. In this paper, we first present an efficient multi-sensor fusion approach based on the probabilistic data association method in order to achieve accurate object detection and tracking results. The proposed approach fuses the detection results obtained independently from four main sensors: radar, LiDAR, RGB camera and infrared camera. It generates object region proposals based on the fused detection result. Then, a Convolutional Neural Network (CNN) approach is used to identify the object categories within these regions. The CNN is trained on a real dataset from different ferry driving scenarios. The experimental results of tracking and classification on real datasets show that the proposed approach provides reliable object detection and classification results in maritime environments.},
  keywords = {autonomous vessel,Cameras,convolutional neural networks,favorite,Feature extraction,Fusion,Infrared,LiDAR,maritime environment,multi-sensor fusion,multisensor,object detection,Object detection,Proposals,Radar,region proposals,RGB Camera,Sensor fusion},
  file = {/Users/dplane/Zotero/storage/XGTZHTAP/Haghbayan et al_2018_An Efficient Multi-sensor Fusion Approach for Object Detection in Maritime.pdf;/Users/dplane/Zotero/storage/3XDUSZAU/8569890.html}
}

@inproceedings{he2016,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  booktitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = 2016,
  month = jun,
  pages = {770--778},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2016.90},
  urldate = {2025-02-14},
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8{\texttimes} deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  keywords = {Complexity theory,Degradation,DNN,Image recognition,Image segmentation,Neural networks,nn_core,Training,Visualization},
  file = {/Users/dplane/Zotero/storage/5PUFB45X/He et al_2016_Deep Residual Learning for Image Recognition.pdf;/Users/dplane/Zotero/storage/I79LPQ6F/7780459.html}
}

@inproceedings{helgesen2019,
  title = {Sensor {{Combinations}} in {{Heterogeneous Multi-sensor Fusion}} for {{Maritime Target Tracking}}},
  booktitle = {2019 22th {{International Conference}} on {{Information Fusion}} ({{FUSION}})},
  author = {Helgesen, {\O}ystein Kaarstad and Brekke, Edmund F{$\phi$}rland and Helgesen, H{\aa}kon Hagen and Engelhardtsen, {\O}ystein},
  year = 2019,
  month = jul,
  pages = {1--9},
  doi = {10.23919/FUSION43075.2019.9011297},
  urldate = {2025-02-15},
  abstract = {Safe navigation for autonomous surface vehicles requires a robust and reliable tracking system that maintains and estimates position and velocity of other vessels. This paper demonstrates a measurement level sensor fusion system for tracking in a maritime environment using lidar, radar, electrooptical and infrared cameras. The backbone of the system is a multi-sensor version of the Joint Integrated Probabilistic Data Association (JIPDA) with both existence and visibility probabilities. Using reference targets equipped with GPS receivers, the performance of different sensors and sensor combinations are evaluated for autonomous surface vehicles (ASVs), Several interesting observations are made, among them that passive sensors can help resolve merged measurements issues in radar tracking, and that the choice between radar and lidar may boil down to a trade-off between fast track initiation and large numbers of false tracks.},
  keywords = {Cameras,Detectors,Radar tracking,Robot sensing systems,Sensor fusion,target tracking,Target tracking,unmanned surface vehicle},
  file = {/Users/dplane/Zotero/storage/HPC86MI5/Helgesen et al_2019_Sensor Combinations in Heterogeneous Multi-sensor Fusion for Maritime Target.pdf;/Users/dplane/Zotero/storage/UWXSBENQ/9011297.html}
}

@article{holland,
  title = {Design of the {{Minion Research Platform}} for the 2022 {{Maritime RobotX Challenge}}},
  author = {Holland, David and Landaeta, Erasmo and Montagnoli, Charles and Ayars, Taylor and Barnes, Jamie and Barthelemy, Kesmir and Brown, Robert and Delp, Grady and Garnier, Thomas and Halleran, Juan and Helms, Matthew and Hendrickson, James and Kay, James and Kuennen, Kurt and Lachguar, Adam and Perskin, Jennifer and Schoener, Marco and Thomas, Ryan and Thompson, David and Vail, Devon and Coyle, Dr Eric J and Currier, Dr Patrick N and Reinholtz, Dr Charles F},
  abstract = {For the 2022 Maritime RobotX Challenge Embry-Riddle Aeronautical University (ERAU) has made significant improvements to their fully autonomous research platform, Minion. To complete mission tasks, Minion uses sophisticated sensory and perception algorithms fusing data from a suite consisting of multi-beam LiDARs, multi-modal imagery sensors, and a high precision GPS/INS. This data feeds decision-making algorithms that include neural network visual detection, long-range LiDAR-based object detection, and dynamic path planning. These processes are all tied together using a unique tasking system designed to initiate tasks when perception queues are received and select a task order to maximum time usage. The research team has also developed an autonomous drone platform, for completing the tasks that require aerial reconnaissance.},
  langid = {english},
  file = {/Users/dplane/Zotero/storage/DBSZFZ2X/Holland et al. - Design of the Minion Research Platform for the 202.pdf;/Users/dplane/Zotero/storage/X9IY8QUP/Minion_Journal_Paper_appendices.pdf}
}

@article{huang,
  title = {Autonomy {{Levels}} for {{Unmanned Systems}} ({{ALFUS}}) {{Framework Volume I}}: {{Terminology Version}} 1.},
  author = {Huang, Hui-Min},
  journal = {NIST Special Publication},
  langid = {english},
  file = {/Users/dplane/Zotero/storage/JWKGNNEA/Huang - Autonomy Levels for Unmanned Systems (ALFUS) Framework Volume I Terminology Version 1..pdf}
}

@misc{huang2024,
  title = {Multi-Modal {{Sensor Fusion}} for {{Auto Driving Perception}}: {{A Survey}}},
  shorttitle = {Multi-Modal {{Sensor Fusion}} for {{Auto Driving Perception}}},
  author = {Huang, Keli and Shi, Botian and Li, Xiang and Li, Xin and Huang, Siyuan and Li, Yikang},
  year = 2024,
  month = dec,
  number = {arXiv:2202.02703},
  eprint = {2202.02703},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2202.02703},
  urldate = {2025-03-19},
  abstract = {Multi-modal fusion is a fundamental task for the perception of an autonomous driving system, which has recently intrigued many researchers. However, achieving a rather good performance is not an easy task due to the noisy raw data, underutilized information, and the misalignment of multi-modal sensors. In this paper, we provide a literature review of the existing multi-modal-based methods for perception tasks in autonomous driving. Generally, we make a detailed analysis including over 50 papers leveraging perception sensors including LiDAR and camera trying to solve object detection and semantic segmentation tasks. Different from traditional fusion methodology for categorizing fusion models, we propose an innovative way that divides them into two major classes, four minor classes by a more reasonable taxonomy in the view of the fusion stage. Moreover, we dive deep into the current fusion methods, focusing on the remaining problems and open-up discussions on the potential research opportunities. In conclusion, what we expect to do in this paper is to present a new taxonomy of multi-modal fusion methods for the autonomous driving perception tasks and provoke thoughts of the fusion-based techniques in the future.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/dplane/Zotero/storage/3ADB7SG6/Huang et al_2024_Multi-modal Sensor Fusion for Auto Driving Perception.pdf;/Users/dplane/Zotero/storage/3D5U7J8Q/2202.html}
}

@misc{huang2024a,
  title = {Multi-Modal {{Sensor Fusion}} for {{Auto Driving Perception}}: {{A Survey}}},
  shorttitle = {Multi-Modal {{Sensor Fusion}} for {{Auto Driving Perception}}},
  author = {Huang, Keli and Shi, Botian and Li, Xiang and Li, Xin and Huang, Siyuan and Li, Yikang},
  year = 2024,
  month = dec,
  number = {arXiv:2202.02703},
  eprint = {2202.02703},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2202.02703},
  urldate = {2025-10-22},
  abstract = {Multi-modal fusion is a fundamental task for the perception of an autonomous driving system, which has recently intrigued many researchers. However, achieving a rather good performance is not an easy task due to the noisy raw data, underutilized information, and the misalignment of multi-modal sensors. In this paper, we provide a literature review of the existing multi-modal-based methods for perception tasks in autonomous driving. Generally, we make a detailed analysis including over 50 papers leveraging perception sensors including LiDAR and camera trying to solve object detection and semantic segmentation tasks. Different from traditional fusion methodology for categorizing fusion models, we propose an innovative way that divides them into two major classes, four minor classes by a more reasonable taxonomy in the view of the fusion stage. Moreover, we dive deep into the current fusion methods, focusing on the remaining problems and open-up discussions on the potential research opportunities. In conclusion, what we expect to do in this paper is to present a new taxonomy of multi-modal fusion methods for the autonomous driving perception tasks and provoke thoughts of the fusion-based techniques in the future.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/dplane/Zotero/storage/QWDNIHVE/Huang et al. - 2024 - Multi-modal Sensor Fusion for Auto Driving Perception A Survey.pdf;/Users/dplane/Zotero/storage/LZUBI5SR/2202.html}
}

@article{huang2025,
  title = {Surface {{Vessels Detection}} and {{Tracking Method}} and {{Datasets}} with {{Multi-Source Data Fusion}} in {{Real-World Complex Scenarios}}},
  author = {Huang, Wenbin and Feng, Hui and Xu, Haixiang and Liu, Xu and He, Jianhua and Gan, Langxiong and Wang, Xiaoqian and Wang, Shanshan},
  year = 2025,
  month = mar,
  journal = {Sensors (Basel, Switzerland)},
  volume = {25},
  number = {7},
  pages = {2179},
  issn = {1424-8220},
  doi = {10.3390/s25072179},
  urldate = {2025-11-01},
  abstract = {Environment sensing plays an important role for the safe autonomous navigation of intelligent ships. However, the inherent limitations of sensors, such as the low frequency of the automatic identification system (AIS), blind zone of the marine radar, and lack of depth information in visible images, make it difficult to achieve accurate sensing with a single modality of sensor data. To overcome this limitation, we propose a new multi-source data fusion framework and technologies that integrate AIS, radar, and visible data. This framework leverages the complementary strengths of these different types of sensors to enhance sensing performance, especially in real complex scenarios where single-modality data are significantly affected by blind zone and adverse weather conditions. We first design a multi-stage detection and tracking method (named MSTrack). By feeding the historical fusion results back to earlier tracking stages, the proposed method identifies and refines potential missing detections from the layered detection and tracking processes of radar and visible images. Then, a cascade association matching method is proposed to realize the association between multi-source trajectories. It first performs pairwise association in a high-accuracy aligned coordinate system, followed by association in a low-accuracy coordinate system and integrated matching between multi-source data. Through these association operations, the method can effectively reduce the association errors caused by measurement noise and projection system errors. Furthermore, we develop the first multi-source fusion dataset for intelligent vessel (WHUT-MSFVessel), and validate our methods. The experimental results show that our multi-source data fusion methods significantly improve the sensing accuracy and identity consistency of tracking, achieving average MOTA scores of 0.872 and 0.938 on the radar and visible images, respectively, and IDF1 scores of 0.811 and 0.929. Additionally, the fusion accuracy reaches up to 0.9, which can provide vessels with a comprehensive perception of the navigation environment for safer navigation.},
  pmcid = {PMC11990929},
  pmid = {40218690},
  file = {/Users/dplane/Zotero/storage/WMQJM424/Huang et al. - 2025 - Surface Vessels Detection and Tracking Method and Datasets with Multi-Source Data Fusion in Real-Wor.pdf}
}

@inproceedings{iyer2018,
  title = {{{CalibNet}}: {{Geometrically Supervised Extrinsic Calibration}} Using {{3D Spatial Transformer Networks}}},
  shorttitle = {{{CalibNet}}},
  booktitle = {2018 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Iyer, Ganesh and Ram, R. Karnik and Murthy, J. Krishna and Krishna, K. Madhava},
  year = 2018,
  month = oct,
  eprint = {1803.08181},
  primaryclass = {cs},
  pages = {1110--1117},
  doi = {10.1109/IROS.2018.8593693},
  urldate = {2025-10-22},
  abstract = {3D LiDARs and 2D cameras are increasingly being used alongside each other in sensor rigs for perception tasks. Before these sensors can be used to gather meaningful data, however, their extrinsics (and intrinsics) need to be accurately calibrated, as the performance of the sensor rig is extremely sensitive to these calibration parameters. A vast majority of existing calibration techniques require significant amounts of data and/or calibration targets and human effort, severely impacting their applicability in large-scale production systems. We address this gap with CalibNet: a self-supervised deep network capable of automatically estimating the 6-DoF rigid body transformation between a 3D LiDAR and a 2D camera in real-time. CalibNet alleviates the need for calibration targets, thereby resulting in significant savings in calibration efforts. During training, the network only takes as input a LiDAR point cloud, the corresponding monocular image, and the camera calibration matrix K. At train time, we do not impose direct supervision (i.e., we do not directly regress to the calibration parameters, for example). Instead, we train the network to predict calibration parameters that maximize the geometric and photometric consistency of the input images and point clouds. CalibNet learns to iteratively solve the underlying geometric problem and accurately predicts extrinsic calibration parameters for a wide range of mis-calibrations, without requiring retraining or domain adaptation. The project page is hosted at https://epiception.github.io/CalibNet},
  archiveprefix = {arXiv},
  keywords = {Calibration,Cameras,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics,Laser radar,Robot sensing systems,Three-dimensional displays,Training,Two dimensional displays},
  file = {/Users/dplane/Zotero/storage/37GLMX8V/Iyer et al. - 2018 - CalibNet Geometrically Supervised Extrinsic Calibration using 3D Spatial Transformer Networks.pdf;/Users/dplane/Zotero/storage/UPFWJRJF/Iyer et al. - 2018 - CalibNet Geometrically Supervised Extrinsic Calibration using 3D Spatial Transformer Networks.pdf;/Users/dplane/Zotero/storage/4NXNDJ37/1803.html}
}

@article{kim2022,
  title = {Object {{Detection}} and {{Classification Based}} on {{YOLO-V5}} with {{Improved Maritime Dataset}}},
  author = {Kim, Jun-Hwa and Kim, Namho and Park, Yong Woon and Won, Chee Sun},
  year = 2022,
  month = mar,
  journal = {Journal of Marine Science and Engineering},
  volume = {10},
  number = {3},
  pages = {377},
  issn = {2077-1312},
  doi = {10.3390/jmse10030377},
  urldate = {2025-02-14},
  abstract = {SMD (Singapore Maritime Dataset) is a public dataset with annotated videos, and it is almost unique in the training of deep neural networks (DNN) for the recognition of maritime objects. However, there are noisy labels and imprecisely located bounding boxes in the ground truth of the SMD. In this paper, for the benchmark of DNN algorithms, we correct the annotations of the SMD dataset and present an improved version, which we coined SMD-Plus. We also propose augmentation techniques designed especially for the SMD-Plus. More specifically, an online transformation of training images via Copy \& Paste is applied to solve the class-imbalance problem in the training dataset. Furthermore, the mix-up technique is adopted in addition to the basic augmentation techniques for YOLO-V5. Experimental results show that the detection and classification performance of the modified YOLO-V5 with the SMD-Plus has improved in comparison to the original YOLO-V5. The ground truth of the SMD-Plus and our experimental results are available for download.},
  copyright = {https://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  keywords = {Algorithms,Annotations,Artificial neural networks,Augmentation,Boxes,Classification,data relabel,Datasets,deep learning,Deep learning,Detection,Environmental,maritime dataset,Neural networks,object detection,Object detection,Object recognition,Training},
  file = {/Users/dplane/Zotero/storage/QEHUDF5F/Jun-Hwa et al_2022_Object Detection and Classification Based on YOLO-V5 with Improved Maritime.pdf}
}

@article{krizhevsky2017,
  title = {{{ImageNet}} Classification with Deep Convolutional Neural Networks},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
  year = 2017,
  month = may,
  journal = {Communications of the ACM},
  volume = {60},
  number = {6},
  pages = {84--90},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/3065386},
  urldate = {2025-02-14},
  abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called ``dropout'' that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
  langid = {english},
  keywords = {nn_core alexnet},
  file = {/Users/dplane/Zotero/storage/H7KD8E5V/Krizhevsky et al. - 2017 - ImageNet classification with deep convolutional ne.pdf}
}

@article{kumar2020,
  title = {{{LiDAR}} and {{Camera Fusion Approach}} for {{Object Distance Estimation}} in {{Self-Driving Vehicles}}},
  author = {Kumar, G. Ajay and Lee, Jin Hee and Hwang, Jongrak and Park, Jaehyeong and Youn, Sung Hoon and Kwon, Soon},
  year = 2020,
  month = feb,
  journal = {Symmetry},
  volume = {12},
  number = {2},
  pages = {324},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2073-8994},
  doi = {10.3390/sym12020324},
  urldate = {2025-02-21},
  abstract = {The fusion of light detection and ranging (LiDAR) and camera data in real-time is known to be a crucial process in many applications, such as in autonomous driving, industrial automation, and robotics. Especially in the case of autonomous vehicles, the efficient fusion of data from these two types of sensors is important to enabling the depth of objects as well as the detection of objects at short and long distances. As both the sensors are capable of capturing the different attributes of the environment simultaneously, the integration of those attributes with an efficient fusion approach greatly benefits the reliable and consistent perception of the environment. This paper presents a method to estimate the distance (depth) between a self-driving car and other vehicles, objects, and signboards on its path using the accurate fusion approach. Based on the geometrical transformation and projection, low-level sensor fusion was performed between a camera and LiDAR using a 3D marker. Further, the fusion information is utilized to estimate the distance of objects detected by the RefineDet detector. Finally, the accuracy and performance of the sensor fusion and distance estimation approach were evaluated in terms of quantitative and qualitative analysis by considering real road and simulation environment scenarios. Thus the proposed low-level sensor fusion, based on the computational geometric transformation and projection for object distance estimation proves to be a promising solution for enabling reliable and consistent environment perception ability for autonomous vehicles.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {autonomous vehicle,computational geometry transformation,depth sensing,image source,point cloud to image mapping,projection,self-driving vehicle,sensor calibration,sensor fusion},
  file = {/Users/dplane/Zotero/storage/MYAW7BJS/Kumar et al_2020_LiDAR and Camera Fusion Approach for Object Distance Estimation in Self-Driving.pdf}
}

@article{kunz2005,
  title = {Detection of Small Targets in a Marine Environment Using Laser Radar},
  author = {Kunz, Gerard J. and Bekman, Herman H. P. T. and Benoist, Koen W. and Coen, Leo H. and Van Den Heuvel, Johan C. and Van Putten, Frank J. M.},
  editor = {Frouin, Robert J. and Babin, Marcel and Sathyendranath, Shubha},
  year = 2005,
  month = aug,
  journal = {Society of Photo-Optical Instrumentation Engineers},
  volume = {5885},
  pages = {58850F},
  doi = {10.1117/12.614914},
  urldate = {2025-02-15},
  abstract = {Small maritime targets, e.g., periscope tubes, jet skies, swimmers and small boats, are potential threats for naval ships under many conditions, but are difficult to detect with current radar systems due to their limited radar cross section and the presence of sea clutter. On the other hand, applications of lidar systems have shown that the reflections from small targets are significantly stronger than reflections from the sea surface. As a result, dedicated lidar systems are potential tools for the detection of small maritime targets. A geometric approach is used to compare the diffuse reflection properties of cylinders and spheres with flat surfaces, which is used to estimate the maximum detectable range of such objects for a given lidar system. Experimental results using lasers operating at 1.06 {\textmu}m and 1.57 {\textmu}m confirm this theory and are discussed. Small buoys near Scheveningen harbor could be detected under adverse weather over more than 9 km. Extrapolation of these results indicates that small targets can be detected out to ranges of approximately 20 km.},
  langid = {english},
  keywords = {multiseonsor},
  file = {/Users/dplane/Zotero/storage/LM29JWG2/Kunz et al. - 2005 - Detection of small targets in a marine environment.pdf}
}

@article{lachguar,
  title = {Minion: {{Design}} and {{Competition Strategy}} for the 2024 {{Maritime RobotX Challenge Minion}}},
  author = {Lachguar, Adam and Ucles, Giovanna and Sarkar, Sagar and Lane, Dan and Liebergall, Erik and Aggarwal, Sarthak and Proper, Willis and Saravis, Michael and Dcruz, Dean and Kay, Isaac and Abe, Matis and Jagwani, Bharat and Vinnakota, Rohit and Prudencio, Mateus and Yelvington, Marshall and Young, Jacob and Pathi, Neel and Park, Brian and Anand, Aarambh and Lam, Andrew and Jones, Thomas and Coyle, Dr Eric and Currier, Dr Patrick},
  abstract = {Embry-Riddle Aeronautical University's Team Minion is returning to RobotX with significant improvements to its defending champion fully autonomous surface vessel (ASV), Minion. Team Minion's new design strategy and systems engineering approach, called Minion Process, has allowed a balance of academics, research, and team objectives throughout the team. This design strategy combined with a rigorous multistep testing process that values safety and innovation has led to an ever-improving toolset for Minion and its Uncrewed Aerial Vehicle (UAV), Kevin. These include software enhancements of a new patent-pending control scheme and better integration of computer vision throughout the system, as well as hardware improvements of azimuthing motor control, new UAV capabilities, and a new ball launcher.},
  langid = {english},
  file = {/Users/dplane/Zotero/storage/IGRH95RN/Lachguar et al. - Minion Design and Competition Strategy for the 20.pdf;/Users/dplane/Zotero/storage/UJDUIAW2/Lachguar et al. - Minion Design and Competition Strategy for the 20.pdf}
}

@misc{landaeta,
  title = {Assessing {{High Dynamic Range Imagery Performance}} for {{Object Detection}} in {{Maritime Environments}}},
  author = {Landaeta, Erasmo},
  year = 2024,
  month = jan,
  langid = {english},
  keywords = {disertation},
  file = {/Users/dplane/Zotero/storage/E56CK556/Landaeta - Assessing High Dynamic Range Imagery Performance f.pdf}
}

@article{lecun1998,
  title = {Gradient-Based Learning Applied to Document Recognition},
  author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
  year = 1998,
  month = nov,
  journal = {Proceedings of the IEEE},
  volume = {86},
  number = {11},
  pages = {2278--2324},
  issn = {1558-2256},
  doi = {10.1109/5.726791},
  urldate = {2025-02-14},
  abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day.},
  keywords = {Character recognition,Feature extraction,gradient descent,Hidden Markov models,Machine learning,Multi-layer neural network,Neural networks,nn_core,Optical character recognition software,Optical computing,Pattern recognition,Principal component analysis},
  file = {/Users/dplane/Zotero/storage/3UZN4EJL/Lecun et al_1998_Gradient-based learning applied to document recognition.pdf;/Users/dplane/Zotero/storage/YBE8NWC2/726791.html}
}

@article{li2020a,
  title = {Compact {{Antenna}} for {{Picosatellites Using}} a {{Meandered Folded-Shorted Patch Array}}},
  author = {Li, Yuepei and Podilchak, Symon K. and Anagnostou, Dimitris E. and Constantinides, Constantin and Walkinshaw, Tom},
  year = 2020,
  month = mar,
  journal = {IEEE Antennas and Wireless Propagation Letters},
  volume = {19},
  number = {3},
  pages = {477--481},
  issn = {1548-5757},
  doi = {10.1109/LAWP.2020.2966088},
  urldate = {2025-03-16},
  abstract = {The design and operation of a compact antenna array offering circularly polarized (CP) radiation for picosat and other small satellites for communication applications is presented. The proposed array combines the folded-shorted patches (FSPs) and meandering for antenna miniaturization. Realization of CP is achieved by a compact and planar feed circuit consisting of a network of meander-shaped 90{$^\circ$} and 180{$^\circ$} hybrid couplers, which can provide quadrature feeding of the FSP elements and can be integrated onto the backside of the antenna ground plane, which is only 5 {\texttimes} 5 cm2. Agreement in terms of the simulations and measurements is observed with realized gain values of more than 3 dBic at 1.065 GHz and with an antenna size of 0.17{$\lambda$}0 {\texttimes} 0.17{$\lambda$}0.},
  keywords = {Antenna arrays,Antenna feeds,Antenna measurements,Circular polarisation (CP),Delays,folded-shorted patch (FSP) antenna,Gain,meandering,Resonant frequency,satellite communications,sequentially rotated arrays},
  file = {/Users/dplane/Zotero/storage/Q35TZ7P8/Li et al_2020_Compact Antenna for Picosatellites Using a Meandered Folded-Shorted Patch Array.pdf;/Users/dplane/Zotero/storage/GQI48D7Q/8957242.html}
}

@article{li2023,
  title = {A Survey of Maritime Unmanned Search System: {{Theory}}, Applications and Future Directions},
  shorttitle = {A Survey of Maritime Unmanned Search System},
  author = {Li, Jiqiang and Zhang, Guoqing and Jiang, Changyan and Zhang, Weidong},
  year = 2023,
  month = oct,
  journal = {Ocean Engineering},
  volume = {285},
  pages = {115359},
  issn = {0029-8018},
  doi = {10.1016/j.oceaneng.2023.115359},
  urldate = {2025-03-14},
  abstract = {The rising frequency of ocean activities, such as ocean transportation and marine resources development, inevitably leads to a higher incidence of sudden accidents at sea. Maritime search and rescue (MSAR) plays a crucial role in marine transportation. Specifically, maritime search serves as a preparatory mission before executing the actual rescue operations. In 2018, the International Maritime Organization (IMO) made significant legal amendments to the international convention for the Safety of Life at Sea (SOLAS), emphasizing the critical importance of MSAR operations. Nevertheless, the conventional maritime search systems, such as independent surface ships, underwater vehicles, or aerial platforms, fall short of meeting the demands of modern shipping and can impose significant energy and economic burdens. Meanwhile, unmanned systems are gaining prominence in maritime search due to their notable advantages, including high efficiency, cost-effectiveness, and rapid deployment. One observes that the research of unmanned maritime search is still in an early stage. This paper provides a comprehensive survey of the state of the art of maritime unmanned search system development with four dimensions, including the unmanned aerial vehicle (UAV), unmanned surface vessel (USV) and underwater unmanned vehicle (UUV) and its cooperative heterogeneous vehicles. Firstly, the search types, the superiorities and the application scenarios of the unmanned search system are summarized. Then, the theoretical progress, engineering applications and limitations for the unmanned search system are investigated. To further discuss the advantages of the unmanned search system, a novel cooperative platform is established that uses an USV and two UAVs. In the proposed platform, the sensor, communication, guidance and control subsystems are described to illustrate the convenience and the effectiveness of the proposed cooperative system. Finally, the future research directions are examined to expedite the practical implementation of theoretical advancements in the field of maritime unmanned search.},
  keywords = {Autonomous guidance and control,Cooperative unmanned systems,Maritime unmanned search system,Underwater unmanned vehicle,Unmanned aerial vehicle,Unmanned surface vessel},
  file = {/Users/dplane/Zotero/storage/9PQ6WKBM/Li et al_2023_A survey of maritime unmanned search system.pdf;/Users/dplane/Zotero/storage/TDL5W3SE/S0029801823017432.html}
}

@misc{liang2022,
  title = {{{BEVFusion}}: {{A Simple}} and {{Robust LiDAR-Camera Fusion Framework}}},
  shorttitle = {{{BEVFusion}}},
  author = {Liang, Tingting and Xie, Hongwei and Yu, Kaicheng and Xia, Zhongyu and Lin, Zhiwei and Wang, Yongtao and Tang, Tao and Wang, Bing and Tang, Zhi},
  year = 2022,
  month = nov,
  number = {arXiv:2205.13790},
  eprint = {2205.13790},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2205.13790},
  urldate = {2025-10-22},
  abstract = {Fusing the camera and LiDAR information has become a de-facto standard for 3D object detection tasks. Current methods rely on point clouds from the LiDAR sensor as queries to leverage the feature from the image space. However, people discovered that this underlying assumption makes the current fusion framework infeasible to produce any prediction when there is a LiDAR malfunction, regardless of minor or major. This fundamentally limits the deployment capability to realistic autonomous driving scenarios. In contrast, we propose a surprisingly simple yet novel fusion framework, dubbed BEVFusion, whose camera stream does not depend on the input of LiDAR data, thus addressing the downside of previous methods. We empirically show that our framework surpasses the state-of-the-art methods under the normal training settings. Under the robustness training settings that simulate various LiDAR malfunctions, our framework significantly surpasses the state-of-the-art methods by 15.7\% to 28.9\% mAP. To the best of our knowledge, we are the first to handle realistic LiDAR malfunction and can be deployed to realistic scenarios without any post-processing procedure. The code is available at https://github.com/ADLab-AutoDrive/BEVFusion.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/dplane/Zotero/storage/XG6TXB53/Liang et al. - 2022 - BEVFusion A Simple and Robust LiDAR-Camera Fusion Framework.pdf;/Users/dplane/Zotero/storage/H73XKXTX/2205.html}
}

@article{liebergall,
  title = {Comparing {{Standard}} and {{High Dynamic Range Imagery}} for {{Maritime Object Detection}}},
  author = {Liebergall, Erik and Landaeta, Erasmo and Coyle, Dr Eric},
  year = 2024,
  month = jan,
  journal = {Naval Engineers Journal},
  volume = {136},
  number = {3},
  pages = {117--124},
  langid = {english},
  keywords = {disertation},
  file = {/Users/dplane/Zotero/storage/FPCS7BMN/Liebergall et al. - Comparing Standard and High Dynamic Range Imagery .pdf}
}

@inproceedings{liu2021,
  title = {Brief {{Industry Paper}}: {{The Matter}} of {{Time}} --- {{A General}} and {{Efficient System}} for {{Precise Sensor Synchronization}} in {{Robotic Computing}}},
  shorttitle = {Brief {{Industry Paper}}},
  booktitle = {2021 {{IEEE}} 27th {{Real-Time}} and {{Embedded Technology}} and {{Applications Symposium}} ({{RTAS}})},
  author = {Liu, Shaoshan and Yu, Bo and Liu, Yahui and Zhang, Kunai and Qiao, Yisong and Li, Thomas Yuang and Tang, Jie and Zhu, Yuhao},
  year = 2021,
  month = may,
  pages = {413--416},
  issn = {2642-7346},
  doi = {10.1109/RTAS52030.2021.00040},
  urldate = {2025-10-22},
  abstract = {Time synchronization is a critical task in robotic computing such as autonomous driving. In the past few years, as we developed advanced robotic applications, our synchronization system has evolved as well. In this paper, we first introduce the time synchronization problem and explain the challenges of time synchronization, especially in robotic workloads. Summarizing these challenges, we then present a general hardware synchronization system for robotic computing, which delivers high synchronization accuracy while maintaining low energy and resource consumption. The proposed hardware synchronization system is a key building block in our future robotic products.},
  keywords = {Hardware,Industries,Real-time systems,Robot sensing systems,Service robots,Synchronization,Task analysis},
  file = {/Users/dplane/Zotero/storage/9QV3A5KI/Liu et al. - 2021 - Brief Industry Paper The Matter of Time â€” A General and Efficient System for Precise Sensor Synchro.pdf}
}

@article{liu2023a,
  title = {Real Time Object Detection Using {{LiDAR}} and Camera Fusion for Autonomous Driving},
  author = {Liu, Haibin and Wu, Chao and Wang, Huanjie},
  year = 2023,
  month = may,
  journal = {Scientific Reports},
  volume = {13},
  number = {1},
  pages = {8056},
  publisher = {Nature Publishing Group},
  issn = {2045-2322},
  doi = {10.1038/s41598-023-35170-z},
  urldate = {2025-03-16},
  abstract = {Autonomous driving has been widely applied in commercial and industrial applications, along with the upgrade of environmental awareness systems. Tasks such as path planning, trajectory tracking, and obstacle avoidance are strongly dependent on the ability to perform real-time object detection and position regression. Among the most commonly used sensors, camera provides dense semantic information but lacks accurate distance information to the target, while LiDAR provides accurate depth information but with sparse resolution. In this paper, a LiDAR-camera-based fusion algorithm is proposed to improve the above-mentioned trade-off problems by constructing a Siamese network for object detection. Raw point clouds are converted to camera planes to obtain a 2D depth image. By designing a cross feature fusion block to connect the depth and RGB processing branches, the feature-layer fusion strategy is applied to integrate multi-modality data. The proposed fusion algorithm is evaluated on the KITTI dataset. Experimental results demonstrate that our algorithm has superior performance and real-time efficiency. Remarkably, it outperforms other state-of-the-art algorithms at the most important moderate level and achieves excellent performance at the easy and hard levels.},
  copyright = {2023 The Author(s)},
  langid = {english},
  keywords = {Civil engineering,Mechanical engineering},
  file = {/Users/dplane/Zotero/storage/56MBH3R2/Liu et al_2023_Real time object detection using LiDAR and camera fusion for autonomous driving.pdf}
}

@inproceedings{liu2023b,
  title = {{{BEVFusion}}: {{Multi-Task Multi-Sensor Fusion}} with {{Unified Bird}}'s-{{Eye View Representation}}},
  shorttitle = {{{BEVFusion}}},
  booktitle = {2023 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Liu, Zhijian and Tang, Haotian and Amini, Alexander and Yang, Xinyu and Mao, Huizi and Rus, Daniela L. and Han, Song},
  year = 2023,
  month = may,
  pages = {2774--2781},
  doi = {10.1109/ICRA48891.2023.10160968},
  urldate = {2025-10-22},
  abstract = {Multi-sensor fusion is essential for an accurate and reliable autonomous driving system. Recent approaches are based on point-level fusion: augmenting the LiDAR point cloud with camera features. However, the camera-to-LiDAR projection throws away the semantic density of camera features, hindering the effectiveness of such methods, especially for semantic-oriented tasks (such as 3D scene segmentation). In this paper, we propose BEVFusion, an efficient and generic multi-task multi-sensor fusion framework. It unifies multi-modal features in the shared bird's-eye view (BEV) representation space, which nicely preserves both geometric and semantic information. To achieve this, we diagnose and lift the key efficiency bottlenecks in the view transformation with optimized BEV pooling, reducing latency by more than {\textbackslash}mathbf40{\textbackslash}times. BEVFusion is fundamentally task-agnostic and seamlessly supports different 3D perception tasks with almost no architectural changes. It establishes the new state of the art on the nuScenes benchmark, achieving 1.3\% higher mAP and NDS on 3D object detection and 13.6\% higher mIoU on BEV map segmentation, with 1.9{\texttimes} lower computation cost. Code to reproduce our results is available at https://github.com/mit-han-lab/bevfusion.},
  keywords = {Laser radar,Multitasking,Object detection,Point cloud compression,Semantics,Sensor fusion,Three-dimensional displays},
  file = {/Users/dplane/Zotero/storage/9YL4R4M7/Liu et al. - 2023 - BEVFusion Multi-Task Multi-Sensor Fusion with Unified Bird's-Eye View Representation.pdf}
}

@article{ma2024,
  title = {Multi-Modal Information Fusion for {{LiDAR-based 3D}} Object Detection Framework},
  author = {Ma, Ruixin and Yin, Yong and Chen, Jing and Chang, Rihao},
  year = 2024,
  month = jan,
  journal = {Multimedia Tools and Applications},
  volume = {83},
  number = {3},
  pages = {7995--8012},
  issn = {1573-7721},
  doi = {10.1007/s11042-023-15452-4},
  urldate = {2025-03-14},
  abstract = {With the rapid development of water transportation, ship safety supervision is facing more severe pressures and challenges. Precise and efficient detection of ship targets is becoming more and more important, which urgently requires intelligent detection methods to ultimately improves shipping management efficiency. However, the surveillance video of waterway transportation is often influenced by fog and rain, which can affect the performance of object detection and reduce the efficiency of management. The current traditional object approaches are hard to handle these problems. In this paper, we propose a novel multi-modal information fusion method to handle multi-object detection in waterway transportation, which introduces the LiDAR (Light Detection And Ranging) dataset to add spatial information and handle the interference of fog and rain. The target ROI (Region Of Interest) point cloud and image data are initially fused in the pre-fusion stage. This phase can efficiently direct the network's attention to the region with the highest target probability, increasing the target recall rate. The 3D bounding box in the point cloud and 2D bounding boxes in the image retrieved are then fused in the post-fusion stage to improve target precision and enrich target detection information. Finally, using time synchronization and a space transformation matrix, the detection result is transferred to the picture coordinate system to create a ship image target with 3D depth information. This technique overcomes the constraints of single-sensor environment perception, adapts to the detection of ship targets in a variety of situations, and is more precise and robust. The algorithm's superiority is also demonstrated by the experiments.},
  langid = {english},
  keywords = {Computer vision,Data fusion,LiDAR,Multi-modality,Object detection},
  file = {/Users/dplane/Zotero/storage/2CPN2KY2/Ma et al_2024_Multi-modal information fusion for LiDAR-based 3D object detection framework.pdf}
}

@article{norbye,
  title = {Camera-{{Lidar}} Sensor Fusion in Real Time for Autonomous Surface Vehicles},
  author = {Norbye, H{\aa}kon Gjertsen},
  langid = {english},
  keywords = {favorite},
  file = {/Users/dplane/Zotero/storage/CB8XH9TV/Norbye - Camera-Lidar sensor fusion in real time for autono.pdf}
}

@inproceedings{pang2020,
  title = {{{CLOCs}}: {{Camera-LiDAR Object Candidates Fusion}} for {{3D Object Detection}}},
  shorttitle = {{{CLOCs}}},
  booktitle = {2020 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Pang, Su and Morris, Daniel and Radha, Hayder},
  year = 2020,
  month = oct,
  pages = {10386--10393},
  publisher = {IEEE Press},
  address = {Las Vegas, NV, USA},
  doi = {10.1109/IROS45743.2020.9341791},
  urldate = {2025-03-16},
  abstract = {There have been significant advances in neural networks for both 3D object detection using LiDAR and 2D object detection using video. However, it has been surprisingly difficult to train networks to effectively use both modalities in a way that demonstrates gain over single-modality networks. In this paper, we propose a novel Camera-LiDAR Object Candidates (CLOCs) fusion network. CLOCs fusion provides a low-complexity multi-modal fusion framework that significantly improves the performance of single-modality detectors. CLOCs operates on the combined output candidates before Non-Maximum Suppression (NMS) of any 2D and any 3D detector, and is trained to leverage their geometric and semantic consistencies to produce more accurate final 3D and 2D detection results. Our experimental evaluation on the challenging KITTI object detection benchmark, including 3D and bird's eye view metrics, shows significant improvements, especially at long distance, over the state-of-the-art fusion based methods. At time of submission, CLOCs ranks the highest among all the fusion-based methods in the official KITTI leaderboard. We will release our code upon acceptance.},
  file = {/Users/dplane/Zotero/storage/H4P9JDYX/Pang et al_2020_CLOCs.pdf}
}

@misc{prakash2021,
  title = {Multi-{{Modal Fusion Transformer}} for {{End-to-End Autonomous Driving}}},
  author = {Prakash, Aditya and Chitta, Kashyap and Geiger, Andreas},
  year = 2021,
  month = apr,
  number = {arXiv:2104.09224},
  eprint = {2104.09224},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2104.09224},
  urldate = {2025-10-22},
  abstract = {How should representations from complementary sensors be integrated for autonomous driving? Geometry-based sensor fusion has shown great promise for perception tasks such as object detection and motion forecasting. However, for the actual driving task, the global context of the 3D scene is key, e.g. a change in traffic light state can affect the behavior of a vehicle geometrically distant from that traffic light. Geometry alone may therefore be insufficient for effectively fusing representations in end-to-end driving models. In this work, we demonstrate that imitation learning policies based on existing sensor fusion methods under-perform in the presence of a high density of dynamic agents and complex scenarios, which require global contextual reasoning, such as handling traffic oncoming from multiple directions at uncontrolled intersections. Therefore, we propose TransFuser, a novel Multi-Modal Fusion Transformer, to integrate image and LiDAR representations using attention. We experimentally validate the efficacy of our approach in urban settings involving complex scenarios using the CARLA urban driving simulator. Our approach achieves state-of-the-art driving performance while reducing collisions by 76\% compared to geometry-based fusion.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/dplane/Zotero/storage/3BSK2T39/Prakash et al. - 2021 - Multi-Modal Fusion Transformer for End-to-End Autonomous Driving.pdf;/Users/dplane/Zotero/storage/P8X95D2B/2104.html}
}

@article{prasad2017,
  title = {Video {{Processing From Electro-Optical Sensors}} for {{Object Detection}} and {{Tracking}} in a {{Maritime Environment}}: {{A Survey}}},
  shorttitle = {Video {{Processing From Electro-Optical Sensors}} for {{Object Detection}} and {{Tracking}} in a {{Maritime Environment}}},
  author = {Prasad, Dilip K. and Rajan, Deepu and Rachmawati, Lily and Rajabally, Eshan and Quek, Chai},
  year = 2017,
  month = aug,
  journal = {IEEE Transactions on Intelligent Transportation Systems},
  volume = {18},
  number = {8},
  pages = {1993--2016},
  issn = {1558-0016},
  doi = {10.1109/TITS.2016.2634580},
  urldate = {2025-02-15},
  abstract = {We present a survey on maritime object detection and tracking approaches, which are essential for the development of a navigational system for autonomous ships. The electro-optical (EO) sensor considered here is a video camera that operates in the visible or the infrared spectra, which conventionally complements radar and sonar for situational awareness at sea and has demonstrated its effectiveness over the last few years. This paper provides a comprehensive overview of various approaches of video processing for object detection and tracking in the maritime environment. We follow an approach-based taxonomy wherein the advantages and limitations of each approach are compared. The object detection system consists of the following modules: horizon detection, static background subtraction, and foreground segmentation. Each of these has been studied extensively in maritime situations and has been shown to be challenging due to the presence of background motion especially due to waves and wakes. The key processes involved in object tracking include video frame registration, dynamic background subtraction, and the object tracking algorithm itself. The challenges for robust tracking arise due to camera motion, dynamic background, and low contrast of tracked object, possibly due to environmental degradation. The survey also discusses multisensor approaches and commercial maritime systems that use EO sensors. The survey also highlights methods from computer vision research, which hold promise to perform well in maritime EO data processing. Performance of several maritime and computer vision techniques is evaluated on Singapore Maritime Dataset.},
  keywords = {autonomous automobiles,Cameras,computer vision,Image edge detection,Intelligent sensors,Marine vehicles,maritime navigation,Maritime vehicles,Object detection,Radar tracking,video signal processing},
  file = {/Users/dplane/Zotero/storage/2BXQTRNP/Prasad et al_2017_Video Processing From Electro-Optical Sensors for Object Detection and Tracking.pdf;/Users/dplane/Zotero/storage/XGZHX9U5/7812788.html}
}

@misc{rekavandi2022,
  title = {A {{Guide}} to {{Image}} and {{Video}} Based {{Small Object Detection}} Using {{Deep Learning}} : {{Case Study}} of {{Maritime Surveillance}}},
  shorttitle = {A {{Guide}} to {{Image}} and {{Video}} Based {{Small Object Detection}} Using {{Deep Learning}}},
  author = {Rekavandi, Aref Miri and Xu, Lian and Boussaid, Farid and Seghouane, Abd-Krim and Hoefs, Stephen and Bennamoun, Mohammed},
  year = 2022,
  month = jul,
  number = {arXiv:2207.12926},
  eprint = {2207.12926},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2207.12926},
  urldate = {2025-02-15},
  abstract = {Small object detection (SOD) in optical images and videos is a challenging problem that even state-of-the-art generic object detection methods fail to accurately localize and identify such objects. Typically, small objects appear in real-world due to large camera-object distance. Because small objects occupy only a small area in the input image (e.g., less than 10\%), the information extracted from such a small area is not always rich enough to support decision making. Multidisciplinary strategies are being developed by researchers working at the interface of deep learning and computer vision to enhance the performance of SOD deep learning based methods. In this paper, we provide a comprehensive review of over 160 research papers published between 2017 and 2022 in order to survey this growing subject. This paper summarizes the existing literature and provide a taxonomy that illustrates the broad picture of current research. We investigate how to improve the performance of small object detection in maritime environments, where increasing performance is critical. By establishing a connection between generic and maritime SOD research, future directions have been identified. In addition, the popular datasets that have been used for SOD for generic and maritime applications are discussed, and also well-known evaluation metrics for the state-of-the-art methods on some of the datasets are provided.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/dplane/Zotero/storage/WA2PRAAR/Rekavandi et al_2022_A Guide to Image and Video based Small Object Detection using Deep Learning.pdf;/Users/dplane/Zotero/storage/V2C82P7U/2207.html}
}

@article{roriz2022,
  title = {Automotive {{LiDAR Technology}}: {{A Survey}}},
  shorttitle = {Automotive {{LiDAR Technology}}},
  author = {Roriz, Ricardo and Cabral, Jorge and Gomes, Tiago},
  year = 2022,
  month = jul,
  journal = {IEEE Transactions on Intelligent Transportation Systems},
  volume = {23},
  number = {7},
  pages = {6282--6297},
  issn = {1558-0016},
  doi = {10.1109/TITS.2021.3086804},
  urldate = {2025-03-19},
  abstract = {Nowadays, and more than a decade after the first steps towards autonomous driving, we keep heading to achieve fully autonomous vehicles on our roads, with LiDAR sensors being a key instrument for the success of this technology. Such advances trigger the emergence of new players in the automotive industry, and along with car manufacturers, this sector represents a multibillion-dollar market where everyone wants to take a share. To understand recent advances and technologies behind LiDAR, this article presents a survey on LiDAR sensors for the automotive industry. With this work, we show the measurement principles and imaging techniques currently being used, going through a review of commercial systems and development solutions available in the market today. Furthermore, we highlight the current and future challenges, providing insights on how both research and industry can step towards better LiDAR solutions.},
  keywords = {3D imaging,Automobiles,Automotive engineering,Autonomous vehicles,Laser radar,LiDAR,Sensors,Three-dimensional displays,ToF,Wavelength measurement},
  file = {/Users/dplane/Zotero/storage/H76ZAHCL/Roriz et al_2022_Automotive LiDAR Technology.pdf;/Users/dplane/Zotero/storage/TL2LHVVW/9455394.html}
}

@inproceedings{schneider2017,
  title = {{{RegNet}}: {{Multimodal}} Sensor Registration Using Deep Neural Networks},
  shorttitle = {{{RegNet}}},
  booktitle = {2017 {{IEEE Intelligent Vehicles Symposium}} ({{IV}})},
  author = {Schneider, Nick and Piewak, Florian and Stiller, Christoph and Franke, Uwe},
  year = 2017,
  month = jun,
  pages = {1803--1810},
  doi = {10.1109/IVS.2017.7995968},
  urldate = {2025-10-22},
  abstract = {In this paper, we present RegNet, the first deep convolutional neural network (CNN) to infer a 6 degrees of freedom (DOF) extrinsic calibration between multimodal sensors, exemplified using a scanning LiDAR and a monocular camera. Compared to existing approaches, RegNet casts all three conventional calibration steps (feature extraction, feature matching and global regression) into a single real-time capable CNN. Our method does not require any human interaction and bridges the gap between classical offline and target-less online calibration approaches as it provides both a stable initial estimation as well as a continuous online correction of the extrinsic parameters. During training we randomly decalibrate our system in order to train RegNet to infer the correspondence between projected depth measurements and RGB image and finally regress the extrinsic calibration. Additionally, with an iterative execution of multiple CNNs, that are trained on different magnitudes of decalibration, our approach compares favorably to state-of-the-art methods in terms of a mean calibration error of 0.28{$^\circ$} for the rotational and 6 cm for the translation components even for large decalibrations up to 1.5 m and 20{$^\circ$}.},
  keywords = {Calibration,Cameras,Feature extraction,Laser radar,Neural networks,Sensor systems},
  file = {/Users/dplane/Zotero/storage/LPDUXM52/Schneider et al. - 2017 - RegNet Multimodal sensor registration using deep neural networks.pdf}
}

@inproceedings{shan2020,
  title = {{{LIO-SAM}}: {{Tightly-coupled Lidar Inertial Odometry}} via {{Smoothing}} and {{Mapping}}},
  shorttitle = {{{LIO-SAM}}},
  booktitle = {2020 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Shan, Tixiao and Englot, Brendan and Meyers, Drew and Wang, Wei and Ratti, Carlo and Rus, Daniela},
  year = 2020,
  month = oct,
  pages = {5135--5142},
  issn = {2153-0866},
  doi = {10.1109/IROS45743.2020.9341176},
  urldate = {2025-03-16},
  abstract = {We propose a framework for tightly-coupled lidar inertial odometry via smoothing and mapping, LIO-SAM, that achieves highly accurate, real-time mobile robot trajectory estimation and map-building. LIO-SAM formulates lidar-inertial odometry atop a factor graph, allowing a multitude of relative and absolute measurements, including loop closures, to be incorporated from different sources as factors into the system. The estimated motion from inertial measurement unit (IMU) pre-integration de-skews point clouds and produces an initial guess for lidar odometry optimization. The obtained lidar odometry solution is used to estimate the bias of the IMU. To ensure high performance in real-time, we marginalize old lidar scans for pose optimization, rather than matching lidar scans to a global map. Scan-matching at a local scale instead of a global scale significantly improves the real-time performance of the system, as does the selective introduction of keyframes, and an efficient sliding window approach that registers a new keyframe to a fixed-size set of prior "sub-keyframes." The proposed method is extensively evaluated on datasets gathered from three platforms over various scales and environments.},
  keywords = {Laser radar,Optimization,Real-time systems,Registers,Smoothing methods,Three-dimensional displays,Trajectory},
  file = {/Users/dplane/Zotero/storage/AREX9CDQ/Shan et al_2020_LIO-SAM.pdf;/Users/dplane/Zotero/storage/PM83DYPZ/9341176.html}
}

@article{shao2022,
  title = {Multi-{{Scale Object Detection Model}} for {{Autonomous Ship Navigation}} in {{Maritime Environment}}},
  author = {Shao, Zeyuan and Lyu, Hongguang and Yin, Yong and Cheng, Tao and Gao, Xiaowei and Zhang, Wenjun and Jing, Qianfeng and Zhao, Yanjie and Zhang, Lunping},
  year = 2022,
  month = nov,
  journal = {Journal of Marine Science and Engineering},
  volume = {10},
  number = {11},
  pages = {1783--1803},
  issn = {2077-1312},
  doi = {10.3390/jmse10111783},
  urldate = {2025-02-14},
  abstract = {Accurate detection of sea-surface objects is vital for the safe navigation of autonomous ships. With the continuous development of artificial intelligence, electro-optical (EO) sensors such as video cameras are used to supplement marine radar to improve the detection of objects that produce weak radar signals and small sizes. In this study, we propose an enhanced convolutional neural network (CNN) named VarifocalNet * that improves object detection in harsh maritime environments. Specifically, the feature representation and learning ability of the VarifocalNet model are improved by using a deformable convolution module, redesigning the loss function, introducing a soft non-maximum suppression algorithm, and incorporating multi-scale prediction methods. These strategies improve the accuracy and reliability of our CNN-based detection results under complex sea conditions, such as in turbulent waves, sea fog, and water reflection. Experimental results under different maritime conditions show that our method significantly outperforms similar methods (such as SSD, YOLOv3, RetinaNet, Faster R-CNN, Cascade R-CNN) in terms of the detection accuracy and robustness for small objects. The maritime obstacle detection results were obtained under harsh imaging conditions to demonstrate the performance of our network model.},
  copyright = {https://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {/Users/dplane/Zotero/storage/45Y2JPIU/Shao et al. - 2022 - Multi-Scale Object Detection Model for Autonomous .pdf}
}

@inproceedings{shi2019,
  title = {{{PointRCNN}}: {{3D Object Proposal Generation}} and {{Detection From Point Cloud}}},
  shorttitle = {{{PointRCNN}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Shi, Shaoshuai and Wang, Xiaogang and Li, Hongsheng},
  year = 2019,
  pages = {770--779},
  urldate = {2025-03-16},
  file = {/Users/dplane/Zotero/storage/TXCK35WL/Shi et al_2019_PointRCNN.pdf}
}

@inproceedings{shi2020,
  title = {{{CalibRCNN}}: {{Calibrating Camera}} and {{LiDAR}} by {{Recurrent Convolutional Neural Network}} and {{Geometric Constraints}}},
  shorttitle = {{{CalibRCNN}}},
  booktitle = {2020 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Shi, Jieying and Zhu, Ziheng and Zhang, Jianhua and Liu, Ruyu and Wang, Zhenhua and Chen, Shengyong and Liu, Honghai},
  year = 2020,
  month = oct,
  pages = {10197--10202},
  issn = {2153-0866},
  doi = {10.1109/IROS45743.2020.9341147},
  urldate = {2025-10-22},
  abstract = {In this paper, we present Calibration Recurrent Convolutional Neural Network (CalibRCNN) to infer a 6 degrees of freedom (DOF) rigid body transformation between 3D LiDAR and 2D camera. Different from the existing methods, our 3D-2D CalibRCNN not only uses the LSTM network to extract the temporal features between 3D point clouds and RGB images of consecutive frames, but also uses the geometric loss and photometric loss obtained by the interframe constraint to refine the calibration accuracy of the predicted transformation parameters. The CalibRCNN aims at inferring the correspondence between projected depth image and RGB image to learn the underlying geometry of 2D-3D calibration. Thus, the proposed calibration model achieves a good generalization ability to adapt to unknown initial calibration error ranges, and other 3D LiDAR and 2D camera pairs with different intrinsic parameters from the training dataset. Extensive experiments have demonstrated that our CalibRCNN can achieve state-of-the-art accuracy by comparison with other CNN based methods.},
  keywords = {Adaptation models,Calibration,Cameras,Feature extraction,Laser radar,Three-dimensional displays,Two dimensional displays},
  file = {/Users/dplane/Zotero/storage/F362DZIZ/Shi et al. - 2020 - CalibRCNN Calibrating Camera and LiDAR by Recurrent Convolutional Neural Network and Geometric Cons.pdf}
}

@misc{shi2021,
  title = {{{PV-RCNN}}: {{Point-Voxel Feature Set Abstraction}} for {{3D Object Detection}}},
  shorttitle = {{{PV-RCNN}}},
  author = {Shi, Shaoshuai and Guo, Chaoxu and Jiang, Li and Wang, Zhe and Shi, Jianping and Wang, Xiaogang and Li, Hongsheng},
  year = 2021,
  month = apr,
  number = {arXiv:1912.13192},
  eprint = {1912.13192},
  primaryclass = {cs, eess},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1912.13192},
  urldate = {2024-05-19},
  abstract = {We present a novel and high-performance 3D object detection framework, named PointVoxel-RCNN (PV-RCNN), for accurate 3D object detection from point clouds. Our proposed method deeply integrates both 3D voxel Convolutional Neural Network (CNN) and PointNet-based set abstraction to learn more discriminative point cloud features. It takes advantages of efficient learning and high-quality proposals of the 3D voxel CNN and the flexible receptive fields of the PointNet-based networks. Specifically, the proposed framework summarizes the 3D scene with a 3D voxel CNN into a small set of keypoints via a novel voxel set abstraction module to save follow-up computations and also to encode representative scene features. Given the high-quality 3D proposals generated by the voxel CNN, the RoI-grid pooling is proposed to abstract proposal-specific features from the keypoints to the RoI-grid points via keypoint set abstraction with multiple receptive fields. Compared with conventional pooling operations, the RoI-grid feature points encode much richer context information for accurately estimating object confidences and locations. Extensive experiments on both the KITTI dataset and the Waymo Open dataset show that our proposed PV-RCNN surpasses state-of-the-art 3D detection methods with remarkable margins by using only point clouds. Code is available at https://github.com/open-mmlab/OpenPCDet.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {/Users/dplane/Zotero/storage/MU9GRBXN/Shi et al. - 2021 - PV-RCNN Point-Voxel Feature Set Abstraction for 3.pdf;/Users/dplane/Zotero/storage/YVMUCYY7/1912.html}
}

@article{su2023,
  title = {A Survey of Maritime Vision Datasets},
  author = {Su, Li and Chen, Yusheng and Song, Hao and Li, Wanyi},
  year = 2023,
  month = aug,
  journal = {Multimedia Tools and Applications},
  volume = {82},
  number = {19},
  pages = {28873--28893},
  issn = {1573-7721},
  doi = {10.1007/s11042-023-14756-9},
  urldate = {2025-02-15},
  abstract = {The field of computer vision has been applied in many topics and scenes, especially in the shipping business which occupies a large position in the world trade. With the development of ship intellectualization, the task of detection, tracking, segmentation and classification of interested targets become more and more important. Publicly available dataset is the foundation to promote research in shipping. Based on this intention, we systematically present a review of maritime datasets on maritime perception. In this paper, comparison is made in terms of data type, environment, ground authenticity, and applicable research directions. The aim of writing this paper is to help researchers quickly identify the most suitable dataset for their work.},
  langid = {english},
  keywords = {Computer vision,Maritime perception,Maritime ship dataset,Ship intellectualization},
  file = {/Users/dplane/Zotero/storage/WEQJLWZZ/Su et al_2023_A survey of maritime vision datasets.pdf}
}

@inproceedings{subedi2020,
  title = {Camera-{{LiDAR Data Fusion}} for {{Autonomous Mooring Operation}}},
  booktitle = {2020 15th {{IEEE Conference}} on {{Industrial Electronics}} and {{Applications}} ({{ICIEA}})},
  author = {Subedi, Dipendra and Jha, Ajit and Tyapin, Ilya and Hovland, Geir},
  year = 2020,
  month = nov,
  pages = {1176--1181},
  issn = {2158-2297},
  doi = {10.1109/ICIEA48937.2020.9248089},
  urldate = {2025-02-15},
  abstract = {The use of camera and LiDAR sensors to sense the environment has gained increasing popularity in robotics. Individual sensors, such as cameras and LiDARs, fail to meet the growing challenges in complex autonomous systems. One such scenario is autonomous mooring, where the ship has to be tied to a fixed rigid structure (bollard) to keep it stationary safely. The detection and pose estimation of the bollard based on data fusion from the camera and LiDAR are presented here. Firstly, a single shot extrinsic calibration of LiDAR with the camera is presented. Secondly, the camera-LiDAR data fusion method using camera intrinsic parameters and camera to LiDAR extrinsic parameters is proposed. Finally, the use of an image-based segmentation method to segment the corresponding point cloud from the fused camera-LiDAR data is developed and tailored for its application in autonomous mooring operation.},
  keywords = {autonomous mooring,camera calibration,Cameras,Data integration,favorite,Image segmentation,Laser radar,LiDAR calibration,Robot vision systems,sensor fusion,Sensors,Three-dimensional displays},
  file = {/Users/dplane/Zotero/storage/Z6HBKJUK/Subedi et al_2020_Camera-LiDAR Data Fusion for Autonomous Mooring Operation.pdf;/Users/dplane/Zotero/storage/PU6PNWZ6/9248089.html}
}

@article{sun,
  title = {Sparse {{Voxels Rasterization}}: {{Real-time High-fidelity Radiance Field Rendering}}},
  author = {Sun, Cheng and Choe, Jaesung and Loop, Charles and Ma, Wei-Chiu and Wang, Yu-Chiang Frank},
  langid = {english},
  file = {/Users/dplane/Zotero/storage/MBQ97JZD/Sun et al. - Sparse Voxels Rasterization Real-time High-ï¬delit.pdf}
}

@misc{tan2020,
  title = {{{EfficientDet}}: {{Scalable}} and {{Efficient Object Detection}}},
  shorttitle = {{{EfficientDet}}},
  author = {Tan, Mingxing and Pang, Ruoming and Le, Quoc V.},
  year = 2020,
  month = jul,
  number = {arXiv:1911.09070},
  eprint = {1911.09070},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1911.09070},
  urldate = {2025-04-24},
  abstract = {Model efficiency has become increasingly important in computer vision. In this paper, we systematically study neural network architecture design choices for object detection and propose several key optimizations to improve efficiency. First, we propose a weighted bi-directional feature pyramid network (BiFPN), which allows easy and fast multiscale feature fusion; Second, we propose a compound scaling method that uniformly scales the resolution, depth, and width for all backbone, feature network, and box/class prediction networks at the same time. Based on these optimizations and better backbones, we have developed a new family of object detectors, called EfficientDet, which consistently achieve much better efficiency than prior art across a wide spectrum of resource constraints. In particular, with single model and single-scale, our EfficientDet-D7 achieves state-of-the-art 55.1 AP on COCO test-dev with 77M parameters and 410B FLOPs, being 4x - 9x smaller and using 13x - 42x fewer FLOPs than previous detectors. Code is available at https://github.com/google/automl/tree/master/efficientdet.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {/Users/dplane/Zotero/storage/XG2PXB4U/Tan et al_2020_EfficientDet.pdf;/Users/dplane/Zotero/storage/Y6Y7A6FU/1911.html}
}

@misc{thompson2017,
  title = {Maritime {{Object Detection}}, {{Tracking}}, and {{Classification Using Lidar}} and {{Vision-Based Sensor Fusion}}},
  author = {Thompson, David},
  year = 2017,
  month = nov,
  keywords = {Masters Thesis},
  file = {/Users/dplane/Zotero/storage/GHDNDT5U/Thompson_2017_Maritime Object Detection, Tracking, and Classification Using Lidar and.pdf;/Users/dplane/Zotero/storage/WRPX37BP/377.html}
}

@article{thompson2019,
  title = {Efficient {{LiDAR-Based Object Segmentation}} and {{Mapping}} for {{Maritime Environments}}},
  author = {Thompson, David and Coyle, Eric and Brown, Jeremy},
  year = 2019,
  month = apr,
  journal = {IEEE Journal of Oceanic Engineering},
  volume = {44},
  number = {2},
  pages = {352--362},
  issn = {1558-1691},
  doi = {10.1109/JOE.2019.2898762},
  urldate = {2025-02-22},
  abstract = {This paper proposes a method that utilizes a 3-D occupancy grid to efficiently map a large area while retaining simple representations of objects for path planning and provide spatial characteristics of objects, which may be used for object classification. To enable large-scale mapping of objects, a region around the unmanned surface vehicle (USV) is defined where a high density of LiDAR returns is expected, termed the visibility horizon. The polygon intersection between the visibility horizon and the newly detected objects is computed, as well as the polygon subtraction of the visibility horizon from the mapped list of polygons. The two polygon lists are then combined using a polygon union operation, with the objects retaining class designations. The result is a 2-D map that contains polygon representations of objects, where the object is described with a tunable number of vertices and may have an associated object class. Thus, providing necessary information for path planning and tasking. The resultant polygons are shown here to be accurate to 20 cm using a 10-cm occupancy grid and 16-ft-long unmanned surface vehicles with four multibeam LiDAR sensors.},
  keywords = {Global Positioning System,Laser radar,LiDAR,mapping,object segmentation,Object segmentation,occupancy grid,Path planning,Sea surface,Sensors},
  file = {/Users/dplane/Zotero/storage/TVJDZG8K/Thompson et al_2019_Efficient LiDAR-Based Object Segmentation and Mapping for Maritime Environments.pdf;/Users/dplane/Zotero/storage/RIEJ7BRH/8656488.html}
}

@article{thompson2023,
  title = {Neural {{Network Fusion}} of {{Multi-Modal Sensor Data For Autonomous Surface Vessels}}},
  author = {Thompson, David},
  year = 2023,
  month = apr,
  journal = {Doctoral Dissertations and Master's Theses},
  keywords = {Phd Dissertation},
  file = {/Users/dplane/Zotero/storage/VB7LSWXG/Thompson - Neural Network Fusion of Multi-Modal Sensor Data F.pdf;/Users/dplane/Zotero/storage/AULNLBPS/741.html}
}

@article{tufekci2023,
  title = {Review of Lidar-Image Sensor Fusion Methods in {{3D}} Object Detection and the Experiment of Camera-Lidar Fusion in Autonomous Driving},
  author = {Tufekci, Zeynep},
  year = 2023,
  urldate = {2025-02-15},
  abstract = {In Autonomous driving, the LiDAR sensor has great importance to catch 3D shapes and visualize the environment around cars. These cars have also mounted camera systems calibrated with LiDAR sensors. These camera systems have multiple cameras with overlapping fields of view which totally cover 360{$^\circ$}. In this research, we focus on 3D Object Detection and Semantic Segmentation frameworks adopting sensor fusion technologies specifically camera-LiDAR and LiDAR-only frameworks. Also, how the camera view helps LiDAR point cloud is discussed. Early fusion approach by fusing point cloud and pseudo-LiDAR in the early step before the object detection network is experimented within the implementation. The depth map is generated from a single camera view. Depth map from front-view camera image which has forward direction information is adopted in this experiment while merging multi-camera images and merging depth maps are discussed at the same time. Pseudo-LiDAR is generated from a depth map while encountering distortion problems due to an unknown depth shift. To overcome this problem and find exact metric reconstruction, some possible solutions are offered. One possible solution is that pseudo-LiDAR uses a 3D point cloud to learn this unknown depth shift. To align two 3D point clouds, one original and one virtual, into one common base, ground extracted from both point clouds. Pandaset [1] which has 360{$^\circ$} and forward-facing LiDAR point cloud and multi-view images is exploited as an autonomous driving dataset.--Author's abstract},
  keywords = {automotive,fusion},
  file = {/Users/dplane/Zotero/storage/F9GASTS4/Tufekci_2023_Review of lidar-image sensor fusion methods in 3D object detection and the.pdf}
}

@inproceedings{tzeng2017,
  title = {Adversarial {{Discriminative Domain Adaptation}}},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Tzeng, Eric and Hoffman, Judy and Saenko, Kate and Darrell, Trevor},
  year = 2017,
  pages = {7167--7176},
  urldate = {2025-03-16},
  file = {/Users/dplane/Zotero/storage/5PKKI2P9/Tzeng et al_2017_Adversarial Discriminative Domain Adaptation.pdf}
}

@misc{ultralytics,
  title = {Yolo - {{Ultralytics Home Page}}},
  author = {Ultralytics},
  urldate = {2025-02-14},
  abstract = {Discover Ultralytics YOLO - the latest in real-time object detection and image segmentation. Learn its features and maximize its potential in your projects.},
  howpublished = {https://docs.ultralytics.com/},
  langid = {english},
  keywords = {Documentation,Yolo},
  file = {/Users/dplane/Zotero/storage/YQZY7FTG/docs.ultralytics.com.html}
}

@inproceedings{vaswani2017,
  title = {Attention Is {{All}} You {{Need}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and ukasz Kaiser, {\L} and Polosukhin, Illia},
  year = 2017,
  volume = {30},
  publisher = {Curran Associates, Inc.},
  urldate = {2025-03-16},
  abstract = {The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.},
  keywords = {nn_core attention transformers},
  file = {/Users/dplane/Zotero/storage/4RWCHYZN/Vaswani et al_2017_Attention is All you Need.pdf}
}

@article{wang2019a,
  title = {Traffic {{Light Recognition With High Dynamic Range Imaging}} and {{Deep Learning}}},
  author = {Wang, Jian-Gang and Zhou, Lu-Bing},
  year = 2019,
  month = apr,
  journal = {IEEE Transactions on Intelligent Transportation Systems},
  volume = {20},
  number = {4},
  pages = {1341--1352},
  issn = {1558-0016},
  doi = {10.1109/TITS.2018.2849505},
  urldate = {2025-02-14},
  abstract = {Traffic light recognition (TLR) detects the traffic light from an image and then estimates the state of the light signal. TLR is important for autonomous vehicles because running against a red light could cause a deadly car accident. For a practical TLR system, computation time, varying illumination conditions, and false positives are three key challenges. In this paper, a novel real-time method is proposed to recognize a traffic light with high dynamic imaging and deep learning. In our approach, traffic light candidates are robustly detected from low exposure/dark frames and accurately classified using a deep neural network in consecutive high exposure/bright frames. This dual-channel mechanism can make full use of undistorted color and shape information in dark frames as well as the rich context in bright frames. In the dark channel, a non-parametric multi-color saliency model is proposed to simultaneously extract lights with different colors. A multiclass classifier with convolutional neural network (CNN) model is then adopted to reduce the number of false positives in the bright channel. The performance is further boosted by incorporating temporal trajectory tracking. In order to speed up the algorithm, a prior detection mask is generated to limit the potential search regions. Intensive experiments on a large dual-channel dataset show that the proposed approach outperforms the state-of-the-art real-time deep learning object detector, which could cause more false positives because it uses bright images only. The algorithm has been integrated into our autonomous vehicle and can work robustly on real roads.},
  keywords = {automotive,autonomous vehicle,Cameras,deep learning,high dynamic range imaging,Histograms,Image color analysis,Image recognition,Lighting,Machine learning,Robustness,Traffic light recognition},
  file = {/Users/dplane/Zotero/storage/GE3JC7PF/Wang_Zhou_2019_Traffic Light Recognition With High Dynamic Range Imaging and Deep Learning.pdf;/Users/dplane/Zotero/storage/3M6X3JUK/8419782.html}
}

@article{wang2020a,
  title = {Multi-{{Sensor Fusion}} in {{Automated Driving}}: {{A Survey}}},
  shorttitle = {Multi-{{Sensor Fusion}} in {{Automated Driving}}},
  author = {Wang, Zhangjing and Wu, Yu and Niu, Qingqing},
  year = 2020,
  journal = {IEEE Access},
  volume = {8},
  pages = {2847--2868},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2019.2962554},
  urldate = {2025-05-21},
  abstract = {With the significant development of practicability in deep learning and the ultra-high-speed information transmission rate of 5G communication technology will overcome the barrier of data transmission on the Internet of Vehicles, automated driving is becoming a pivotal technology affecting the future industry. Sensors are the key to the perception of the outside world in the automated driving system and whose cooperation performance directly determines the safety of automated driving vehicles. In this survey, we mainly discuss the different strategies of multi-sensor fusion in automated driving in recent years. The performance of conventional sensors and the necessity of multi-sensor fusion are analyzed, including radar, LiDAR, camera, ultrasonic, GPS, IMU, and V2X. According to the differences in the latest studies, we divide the fusion strategies into four categories and point out some shortcomings. Sensor fusion is mainly applied for multi-target tracking and environment reconstruction. We discuss the method of establishing a motion model and data association in multi-target tracking. At the end of the paper, we analyzed the deficiencies in the current studies and put forward some suggestions for further improvement in the future. Through this investigation, we hope to analyze the current situation of multi-sensor fusion in the automated driving process and provide more efficient and reliable fusion strategies.},
  keywords = {Automated driving,Cameras,data association,deep learning,environmental reconstruction,intent analysis,Laser radar,multi-sensor fusion strategy,multi-target tracking,Sensor fusion,Sensor phenomena and characterization,Sensor systems},
  file = {/Users/dplane/Zotero/storage/FLRQEEBL/Wang et al_2020_Multi-Sensor Fusion in Automated Driving.pdf}
}

@article{wu2021,
  title = {This {{Is}} the {{Way}}: {{Sensors Auto-Calibration Approach Based}} on {{Deep Learning}} for {{Self-Driving Cars}}},
  shorttitle = {This {{Is}} the {{Way}}},
  author = {Wu, Shan and Hadachi, Amnir and Vivet, Damien and Prabhakar, Yadu},
  year = 2021,
  month = dec,
  journal = {IEEE Sensors Journal},
  volume = {21},
  number = {24},
  pages = {27779--27788},
  issn = {1558-1748},
  doi = {10.1109/JSEN.2021.3124788},
  urldate = {2025-10-22},
  abstract = {The technological advancement of sensors and computational power has opened a new chapter in machine learning for robotics applications, especially in image classification, segmentation, object detection, and self-driving cars. One of the challenges among these applications is improving the systems perception reliability and accuracy through sensors fusion. Hence, the focus on using Stereo-cameras and LiDARs as a complement to its accurate distance measurement. However, the calibration process of the sensors is mandatory before deployment. Some may use the conventional methods, including checkerboards, specific pattern labels, or even human labeling, which is labor-intensive and repetitive as it involves doing the same calibration process every time before using. In this work, we have proposed NetCalib -- an auto-calibration methodology based on a deep neural network. This research aims to utilize the power of machine learning to find the geometric transformation between stereo cameras and LiDAR automatically. From the experiments, our method manages to find the transformations from randomly sampled artificial errors and outperforms the linear optimization-based ICP algorithm. Furthermore, this research work is open-sourced to the community to fully use the advances of the methodology and initiate collaboration and innovation in this field.},
  keywords = {auto-calibration,Calibration,Cameras,Computational modeling,Deep learning,Laser radar,LiDAR,linear optimization,self-driving cars,Sensors,sensors fusion,stereo-camera,Three-dimensional displays},
  file = {/Users/dplane/Zotero/storage/EA5PZ5KU/Wu et al. - 2021 - This Is the Way Sensors Auto-Calibration Approach Based on Deep Learning for Self-Driving Cars.pdf}
}

@misc{xiang2023,
  title = {{{FusionViT}}: {{Hierarchical 3D Object Detection}} via {{LiDAR-Camera Vision Transformer Fusion}}},
  shorttitle = {{{FusionViT}}},
  author = {Xiang, Xinhao and Zhang, Jiawei},
  year = 2023,
  month = nov,
  number = {arXiv:2311.03620},
  eprint = {2311.03620},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2311.03620},
  urldate = {2025-10-22},
  abstract = {For 3D object detection, both camera and lidar have been demonstrated to be useful sensory devices for providing complementary information about the same scenery with data representations in different modalities, e.g., 2D RGB image vs 3D point cloud. An effective representation learning and fusion of such multi-modal sensor data is necessary and critical for better 3D object detection performance. To solve the problem, in this paper, we will introduce a novel vision transformer-based 3D object detection model, namely FusionViT. Different from the existing 3D object detection approaches, FusionViT is a pure-ViT based framework, which adopts a hierarchical architecture by extending the transformer model to embed both images and point clouds for effective representation learning. Such multi-modal data embedding representations will be further fused together via a fusion vision transformer model prior to feeding the learned features to the object detection head for both detection and localization of the 3D objects in the input scenery. To demonstrate the effectiveness of FusionViT, extensive experiments have been done on real-world traffic object detection benchmark datasets KITTI and Waymo Open. Notably, our FusionViT model can achieve state-of-the-art performance and outperforms not only the existing baseline methods that merely rely on camera images or lidar point clouds, but also the latest multi-modal image-point cloud deep fusion approaches.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/dplane/Zotero/storage/8GBLEW9A/Xiang and Zhang - 2023 - FusionViT Hierarchical 3D Object Detection via LiDAR-Camera Vision Transformer Fusion.pdf;/Users/dplane/Zotero/storage/CAT4W235/2311.html}
}

@misc{xiao2024,
  title = {{{CalibFormer}}: {{A Transformer-based Automatic LiDAR-Camera Calibration Network}}},
  shorttitle = {{{CalibFormer}}},
  author = {Xiao, Yuxuan and Li, Yao and Meng, Chengzhen and Li, Xingchen and Ji, Jianmin and Zhang, Yanyong},
  year = 2024,
  month = mar,
  number = {arXiv:2311.15241},
  eprint = {2311.15241},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2311.15241},
  urldate = {2025-10-22},
  abstract = {The fusion of LiDARs and cameras has been increasingly adopted in autonomous driving for perception tasks. The performance of such fusion-based algorithms largely depends on the accuracy of sensor calibration, which is challenging due to the difficulty of identifying common features across different data modalities. Previously, many calibration methods involved specific targets and/or manual intervention, which has proven to be cumbersome and costly. Learning-based online calibration methods have been proposed, but their performance is barely satisfactory in most cases. These methods usually suffer from issues such as sparse feature maps, unreliable cross-modality association, inaccurate calibration parameter regression, etc. In this paper, to address these issues, we propose CalibFormer, an end-to-end network for automatic LiDAR-camera calibration. We aggregate multiple layers of camera and LiDAR image features to achieve high-resolution representations. A multi-head correlation module is utilized to identify correlations between features more accurately. Lastly, we employ transformer architectures to estimate accurate calibration parameters from the correlation information. Our method achieved a mean translation error of \$0.8751 {\textbackslash}mathrm\{cm\}\$ and a mean rotation error of \$0.0562 {\textasciicircum}\{{\textbackslash}circ\}\$ on the KITTI dataset, surpassing existing state-of-the-art methods and demonstrating strong robustness, accuracy, and generalization capabilities.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  file = {/Users/dplane/Zotero/storage/8G9MAX95/Xiao et al. - 2024 - CalibFormer A Transformer-based Automatic LiDAR-Camera Calibration Network.pdf;/Users/dplane/Zotero/storage/KWYT9JCV/2311.html}
}

@article{xie2024,
  title = {Reliable {{LiDAR-based}} Ship Detection and Tracking for {{Autonomous Surface Vehicles}} in Busy Maritime Environments},
  author = {Xie, Yongchang and Nanlal, Cassandra and Liu, Yuanchang},
  year = 2024,
  month = nov,
  journal = {Ocean Engineering},
  volume = {312},
  pages = {119288},
  issn = {0029-8018},
  doi = {10.1016/j.oceaneng.2024.119288},
  urldate = {2025-03-15},
  abstract = {Environmental perception is a crucial requirement of Autonomous Surface Vehicles (ASVs) if required to perform tasks safely in a dynamically complex operational environment. Most existing methods for ship detection rely on camera-based methods, which are sensitive to environmental conditions and cannot directly provide spatial location information related to detected targets. To overcome this limitation, we propose a LiDAR-based ship detection and tracking framework that can be applied to busy maritime environments. The proposed framework consists of two functional modules: a ship detection and multi-object tracking. For ship detection, a modularised network structure was adapted, allowing for ease of switching between different types of detection network to prioritise either detection accuracy, detection speed or a compromise of both, depending on the task requirements. A Kalman Filter-based multi-object tracking method is also implemented to compensate for any detections that may have been missed as a result of ship motions or occlusions, relying solely on the detection results. We also collected the first-ever real-world LiDAR dataset for maritime applications across the River Thames and marinas, including a range of ship types, with lengths ranging from 5 m up to 40 m, and different hull types. The datasets are organised in a similar manner to the KITTI datasets, which can be easily applied to the well-developed point cloud detection networks. Remarkably, our methods achieve an overall detection accuracy of 74.1\% in the collected datasets. The proposed framework and dataset make LiDAR-based environmental perception feasible for implementation in ASVs and support development in the autonomous maritime navigation field.},
  keywords = {Autonomous Surface Vehicle,Deep learning,LiDAR-based perception,Object tracking,Ship detection},
  file = {/Users/dplane/Zotero/storage/L2YDM7K3/Xie et al_2024_Reliable LiDAR-based ship detection and tracking for Autonomous Surface.pdf;/Users/dplane/Zotero/storage/CU2A8UMG/S002980182402626X.html}
}

@article{xu2023,
  title = {{{FusionRCNN}}: {{LiDAR-Camera Fusion}} for {{Two-Stage 3D Object Detection}}},
  shorttitle = {{{FusionRCNN}}},
  author = {Xu, Xinli and Dong, Shaocong and Xu, Tingfa and Ding, Lihe and Wang, Jie and Jiang, Peng and Song, Liqiang and Li, Jianan},
  year = 2023,
  month = jan,
  journal = {Remote Sensing},
  volume = {15},
  number = {7},
  pages = {1839},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2072-4292},
  doi = {10.3390/rs15071839},
  urldate = {2025-03-16},
  abstract = {Accurate and reliable perception systems are essential for autonomous driving and robotics. To achieve this, 3D object detection with multi-sensors is necessary. Existing 3D detectors have significantly improved accuracy by adopting a two-stage paradigm that relies solely on LiDAR point clouds for 3D proposal refinement. However, the sparsity of point clouds, particularly for faraway points, makes it difficult for the LiDAR-only refinement module to recognize and locate objects accurately. To address this issue, we propose a novel multi-modality two-stage approach called FusionRCNN. This approach effectively and efficiently fuses point clouds and camera images in the Regions of Interest (RoI). The FusionRCNN adaptively integrates both sparse geometry information from LiDAR and dense texture information from the camera in a unified attention mechanism. Specifically, FusionRCNN first utilizes RoIPooling to obtain an image set with a unified size and gets the point set by sampling raw points within proposals in the RoI extraction step. Then, it leverages an intra-modality self-attention to enhance the domain-specific features, followed by a well-designed cross-attention to fuse the information from two modalities. FusionRCNN is fundamentally plug-and-play and supports different one-stage methods with almost no architectural changes. Extensive experiments on KITTI and Waymo benchmarks demonstrate that our method significantly boosts the performances of popular detectors. Remarkably, FusionRCNN improves the strong SECOND baseline by 6.14\% mAP on Waymo and outperforms competing two-stage approaches.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {3D object detection,Fusion,LiDAR-camera fusion,RCNN,two-stage},
  file = {/Users/dplane/Zotero/storage/YMESBATK/Xu et al_2023_FusionRCNN.pdf}
}

@article{xue2025,
  title = {A Bibliometric Analysis of the Development of Autonomous Ships in Inland Waterway Transport},
  author = {Xue, Jie and Li, Qianbing and Song, Yuanming and Yang, Peijie and Feng, Yuanjun and Hu, Hao},
  year = 2025,
  month = jul,
  journal = {Frontiers in Marine Science},
  volume = {12},
  publisher = {Frontiers},
  issn = {2296-7745},
  doi = {10.3389/fmars.2025.1624596},
  urldate = {2025-11-01},
  abstract = {Inland waterway transport (IWT) plays a critical role in global logistics, offering large-capacity, long-distance transport at a lower cost. Recently, the advent of autonomous ships has promised to revolutionize efficiency and sustainability within the shipping industry. While existing research predominantly targets maritime settings, the distinct challenges of inland waterways such as fluctuating water depths, varying river currents, and confined channels demand tailored technological solutions. This study provides a thorough bibliometric analysis of autonomous ships in inland waterway transport, based on 163 publications from the Web of Science (WoS) core collection. This study identifies key technological milestones in this field and highlights the research gaps of adapting maritime autonomous ship technologies to inland waterways. The pressing need for customized solutions is also discussed. By reviewing the current landscape, this study contributes to the field as a beneficial reference for researchers, industry professionals, and policymakers, promoting the development of autonomous ship technology in inland waterways.},
  langid = {english},
  keywords = {autonomous ship,bibliometric analysis,evolutionary trends,inland waterway transport,VOSviewer},
  file = {/Users/dplane/Zotero/storage/B83VP59J/Xue et al. - 2025 - A bibliometric analysis of the development of autonomous ships in inland waterway transport.pdf}
}

@article{yang2024,
  title = {A Review of Intelligent Ship Marine Object Detection Based on {{RGB}} Camera},
  author = {Yang, Defu and Solihin, Mahmud Iwan and Zhao, Yawen and Yao, Benchun and Chen, Chaoran and Cai, Bingyu and Machmudah, Affiani},
  year = 2024,
  journal = {IET Image Processing},
  volume = {18},
  number = {2},
  pages = {281--297},
  issn = {1751-9667},
  doi = {10.1049/ipr2.12959},
  urldate = {2025-03-16},
  abstract = {The article presents a comprehensive summary of Intelligent Ship Marine Object Detection (ISMOD) based on the RGB Camera. Marine object detection plays a pivotal role in enabling intelligent ships to acquire crucial data and security assurances for autonomous navigation. Among the various detection sensors, the RGB Camera is an informative and cost-effective tool with a wide range of civil applications. In the beginning, the ISMOD metrics based on the RGB camera is analyzed from three significant aspects, namely accuracy, speed, and robustness. Subsequently, the latest research status and comparative overview are presented, encompassing three mainstream detection methods: traditional detection, deep learning detection, and sensor fusion detection. Finally, the existing challenges of ISMOD are discussed and future development trends are recommended. The results demonstrate that forthcoming development will predominantly concentrate on deep learning approaches, complemented by other techniques. It is imperative to advance detection performance in domains such as deep fusion, multi-feature extraction, multi-fusion technology, and lightweight detection architecture.},
  copyright = {{\copyright} 2023 The Authors. IET Image Processing published by John Wiley \& Sons Ltd on behalf of The Institution of Engineering and Technology.},
  langid = {english},
  keywords = {intelligent transportation systems,learning (artificial intelligence),marine navigation,object detection},
  file = {/Users/dplane/Zotero/storage/A3QJZBQZ/Yang et al_2024_A review of intelligent ship marine object detection based on RGB camera.pdf;/Users/dplane/Zotero/storage/59GKT6H7/ipr2.html}
}

@article{yang2024a,
  title = {A Review of Intelligent Ship Marine Object Detection Based on {{RGB}} Camera},
  author = {Yang, Defu and Solihin, Mahmud Iwan and Zhao, Yawen and Yao, Benchun and Chen, Chaoran and Cai, Bingyu and Machmudah, Affiani},
  year = 2024,
  journal = {IET Image Processing},
  volume = {18},
  number = {2},
  pages = {281--297},
  issn = {1751-9667},
  doi = {10.1049/ipr2.12959},
  urldate = {2025-10-22},
  abstract = {The article presents a comprehensive summary of Intelligent Ship Marine Object Detection (ISMOD) based on the RGB Camera. Marine object detection plays a pivotal role in enabling intelligent ships to acquire crucial data and security assurances for autonomous navigation. Among the various detection sensors, the RGB Camera is an informative and cost-effective tool with a wide range of civil applications. In the beginning, the ISMOD metrics based on the RGB camera is analyzed from three significant aspects, namely accuracy, speed, and robustness. Subsequently, the latest research status and comparative overview are presented, encompassing three mainstream detection methods: traditional detection, deep learning detection, and sensor fusion detection. Finally, the existing challenges of ISMOD are discussed and future development trends are recommended. The results demonstrate that forthcoming development will predominantly concentrate on deep learning approaches, complemented by other techniques. It is imperative to advance detection performance in domains such as deep fusion, multi-feature extraction, multi-fusion technology, and lightweight detection architecture.},
  copyright = {{\copyright} 2023 The Authors. IET Image Processing published by John Wiley \& Sons Ltd on behalf of The Institution of Engineering and Technology.},
  langid = {english},
  keywords = {intelligent transportation systems,learning (artificial intelligence),marine navigation,object detection},
  file = {/Users/dplane/Zotero/storage/Q9A8SX2E/Yang et al. - 2024 - A review of intelligent ship marine object detection based on RGB camera.pdf;/Users/dplane/Zotero/storage/I3VSFXUQ/ipr2.html}
}

@article{yeong2021,
  title = {Sensor and {{Sensor Fusion Technology}} in {{Autonomous Vehicles}}: {{A Review}}},
  shorttitle = {Sensor and {{Sensor Fusion Technology}} in {{Autonomous Vehicles}}},
  author = {Yeong, De Jong and {Velasco-Hernandez}, Gustavo and Barry, John and Walsh, Joseph},
  year = 2021,
  month = jan,
  journal = {Sensors},
  volume = {21},
  number = {6},
  pages = {2140},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {1424-8220},
  doi = {10.3390/s21062140},
  urldate = {2025-03-25},
  abstract = {With the significant advancement of sensor and communication technology and the reliable application of obstacle detection techniques and algorithms, automated driving is becoming a pivotal technology that can revolutionize the future of transportation and mobility. Sensors are fundamental to the perception of vehicle surroundings in an automated driving system, and the use and performance of multiple integrated sensors can directly determine the safety and feasibility of automated driving vehicles. Sensor calibration is the foundation block of any autonomous system and its constituent sensors and must be performed correctly before sensor fusion and obstacle detection processes may be implemented. This paper evaluates the capabilities and the technical performance of sensors which are commonly employed in autonomous vehicles, primarily focusing on a large selection of vision cameras, LiDAR sensors, and radar sensors and the various conditions in which such sensors may operate in practice. We present an overview of the three primary categories of sensor calibration and review existing open-source calibration packages for multi-sensor calibration and their compatibility with numerous commercial sensors. We also summarize the three main approaches to sensor fusion and review current state-of-the-art multi-sensor fusion techniques and algorithms for object detection in autonomous driving applications. The current paper, therefore, provides an end-to-end review of the hardware and software methods required for sensor fusion object detection. We conclude by highlighting some of the challenges in the sensor fusion field and propose possible future research directions for automated driving systems.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {autonomous vehicles,calibration,camera,lidar,obstacle detection,perception,radar,self-driving cars,sensor fusion},
  file = {/Users/dplane/Zotero/storage/ZMH3DFVN/Yeong et al. - 2021 - Sensor and Sensor Fusion Technology in Autonomous .pdf}
}

@article{yeong2021a,
  title = {Sensor and {{Sensor Fusion Technology}} in {{Autonomous Vehicles}}: {{A Review}}},
  shorttitle = {Sensor and {{Sensor Fusion Technology}} in {{Autonomous Vehicles}}},
  author = {Yeong, De Jong and {Velasco-Hernandez}, Gustavo and Barry, John and Walsh, Joseph},
  year = 2021,
  month = jan,
  journal = {Sensors},
  volume = {21},
  number = {6},
  pages = {2140},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {1424-8220},
  doi = {10.3390/s21062140},
  urldate = {2025-10-22},
  abstract = {With the significant advancement of sensor and communication technology and the reliable application of obstacle detection techniques and algorithms, automated driving is becoming a pivotal technology that can revolutionize the future of transportation and mobility. Sensors are fundamental to the perception of vehicle surroundings in an automated driving system, and the use and performance of multiple integrated sensors can directly determine the safety and feasibility of automated driving vehicles. Sensor calibration is the foundation block of any autonomous system and its constituent sensors and must be performed correctly before sensor fusion and obstacle detection processes may be implemented. This paper evaluates the capabilities and the technical performance of sensors which are commonly employed in autonomous vehicles, primarily focusing on a large selection of vision cameras, LiDAR sensors, and radar sensors and the various conditions in which such sensors may operate in practice. We present an overview of the three primary categories of sensor calibration and review existing open-source calibration packages for multi-sensor calibration and their compatibility with numerous commercial sensors. We also summarize the three main approaches to sensor fusion and review current state-of-the-art multi-sensor fusion techniques and algorithms for object detection in autonomous driving applications. The current paper, therefore, provides an end-to-end review of the hardware and software methods required for sensor fusion object detection. We conclude by highlighting some of the challenges in the sensor fusion field and propose possible future research directions for automated driving systems.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {autonomous vehicles,calibration,camera,lidar,obstacle detection,perception,radar,self-driving cars,sensor fusion},
  file = {/Users/dplane/Zotero/storage/DDVG6N5N/Yeong et al. - 2021 - Sensor and Sensor Fusion Technology in Autonomous Vehicles A Review.pdf}
}

@article{yuan2020,
  title = {{{RGGNet}}: {{Tolerance Aware LiDAR-Camera Online Calibration With Geometric Deep Learning}} and {{Generative Model}}},
  shorttitle = {{{RGGNet}}},
  author = {Yuan, Kaiwen and Guo, Zhenyu and Wang, Z. Jane},
  year = 2020,
  month = oct,
  journal = {IEEE Robotics and Automation Letters},
  volume = {5},
  number = {4},
  pages = {6956--6963},
  issn = {2377-3766},
  doi = {10.1109/LRA.2020.3026958},
  urldate = {2025-10-22},
  abstract = {Accurate LiDAR-camera online calibration is critical for modern autonomous vehicles and robot platforms. Dominant methods heavily rely on hand-crafted features, which are not scalable in practice. With the increasing popularity of deep learning (DL), a few recent efforts have demonstrated the advantages of DL for feature extraction on this task. However, their reported performances are not sufficiently satisfying yet. We believe one improvement can be the problem formulation with proper consideration of the underneath geometry. Besides, existing online calibration methods focus on optimizing the calibration error while overlooking the tolerance within the error bounds. To address the research gap, a DL-based LiDAR-camera calibration method, named as the RGGNet, is proposed by considering the Riemannian geometry and utilizing a deep generative model to learn an implicit tolerance model. When evaluated on KITTI benchmark datasets, the proposed method demonstrates superior performances to the state-of-the-art DL-based methods. The code will be publicly available at https://github.com/KleinYuan/RGGNet.},
  keywords = {Calibration,Cameras,deep learning,Deep learning,Extrinsic calibration,Feature extraction,generative model,Geometry,Laser radar,LiDAR,point cloud,riemannian geometry,Three-dimensional displays},
  file = {/Users/dplane/Zotero/storage/AMUBA8NL/Yuan et al. - 2020 - RGGNet Tolerance Aware LiDAR-Camera Online Calibration With Geometric Deep Learning and Generative.pdf}
}

@article{zhang2021,
  title = {Survey on {{Deep Learning-Based Marine Object Detection}}},
  author = {Zhang, Ruolan and Li, Shaoxi and Ji, Guanfeng and Zhao, Xiuping and Li, Jing and Pan, Mingyang},
  year = 2021,
  journal = {Journal of Advanced Transportation},
  volume = {2021},
  number = {1},
  pages = {5808206},
  issn = {2042-3195},
  doi = {10.1155/2021/5808206},
  urldate = {2025-02-15},
  abstract = {We present a survey on marine object detection based on deep neural network approaches, which are state-of-the-art approaches for the development of autonomous ship navigation, maritime surveillance, shipping management, and other intelligent transportation system applications in the future. The fundamental task of maritime transportation surveillance and autonomous ship navigation is to construct a reachable visual perception system that requires high efficiency and high accuracy of marine object detection. Therefore, high-performance deep learning-based algorithms and high-quality marine-related datasets need to be summarized. This survey focuses on summarizing the methods and application scenarios of maritime object detection, analyzes the characteristics of different marine-related datasets, highlights the marine detection application of the YOLO series model, and also discusses the current limitations of object detection based on deep learning and possible breakthrough directions. The large-scale, multiscenario industrialized neural network training is an indispensable link to solve the practical application of marine object detection. A widely accepted and standardized large-scale marine object verification dataset should be proposed.},
  copyright = {Copyright {\copyright} 2021 Ruolan Zhang et al.},
  langid = {english},
  file = {/Users/dplane/Zotero/storage/H9VJR5BM/Zhang et al_2021_Survey on Deep Learning-Based Marine Object Detection.pdf;/Users/dplane/Zotero/storage/Y2B7SNZ7/5808206.html}
}

@article{zhang2022a,
  title = {Research of {{Maritime Object Detection Method}} in {{Foggy Environment Based}} on {{Improved Model SRC-YOLO}}},
  author = {Zhang, Yihong and Ge, Hang and Lin, Qin and Zhang, Ming and Sun, Qiantao},
  year = 2022,
  journal = {Sensors},
  volume = {22},
  number = {20},
  pages = {7786},
  publisher = {MDPI AG},
  address = {Basel, Switzerland},
  doi = {10.3390/s22207786},
  urldate = {2025-02-17},
  abstract = {An improved maritime object detection algorithm, SRC-YOLO, based on the YOLOv4-tiny, is proposed in the foggy environment to address the issues of false detection, missed detection, and low detection accuracy in complicated situations. To confirm the model's validity, an ocean dataset containing various concentrations of haze, target angles, and sizes was produced for the research. Firstly, the Single Scale Retinex (SSR) algorithm was applied to preprocess the dataset to reduce the interference of the complex scenes on the ocean. Secondly, in order to increase the model's receptive field, we employed a modified Receptive Field Block (RFB) module in place of the standard convolution in the Neck part of the model. Finally, the Convolutional Block Attention Module (CBAM), which integrates channel and spatial information, was introduced to raise detection performance by expanding the network model's attention to the context information in the feature map and the object location points. The experimental results demonstrate that the improved SRC-YOLO model effectively detects marine targets in foggy scenes by increasing the mean Average Precision (mAP) of detection results from 79.56\% to 86.15\%.},
  copyright = {{\copyright} 2022 by the authors.  Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (https://creativecommons.org/licenses/by/4.0/).  Notwithstanding the ProQuest Terms and Conditions, you may use this content in accordance with the terms of the License.},
  langid = {english},
  keywords = {Accuracy,Algorithms,CNN,convolutional block attention module,Datasets,Deep learning,Detection,Evacuations & rescues,Feature maps,Haze,inclement weather,object detection,Object detection,receptive field block,Spatial data,YOLOv4-tiny},
  file = {/Users/dplane/Zotero/storage/8IGZLRX6/Zhang et al. - 2022 - Research of Maritime Object Detection Method in Fo.pdf}
}

@inproceedings{zhou2018,
  title = {{{VoxelNet}}: {{End-to-End Learning}} for {{Point Cloud Based 3D Object Detection}}},
  shorttitle = {{{VoxelNet}}},
  booktitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Zhou, Yin and Tuzel, Oncel},
  year = 2018,
  month = jun,
  pages = {4490--4499},
  publisher = {IEEE},
  address = {Salt Lake City, UT, USA},
  doi = {10.1109/CVPR.2018.00472},
  urldate = {2025-10-22},
  abstract = {Accurate detection of objects in 3D point clouds is a central problem in many applications, such as autonomous navigation, housekeeping robots, and augmented/virtual reality. To interface a highly sparse LiDAR point cloud with a region proposal network (RPN), most existing efforts have focused on hand-crafted feature representations, for example, a bird's eye view projection. In this work, we remove the need of manual feature engineering for 3D point clouds and propose VoxelNet, a generic 3D detection network that unifies feature extraction and bounding box prediction into a single stage, end-to-end trainable deep network. Specifically, VoxelNet divides a point cloud into equally spaced 3D voxels and transforms a group of points within each voxel into a unified feature representation through the newly introduced voxel feature encoding (VFE) layer. In this way, the point cloud is encoded as a descriptive volumetric representation, which is then connected to a RPN to generate detections. Experiments on the KITTI car detection benchmark show that VoxelNet outperforms the state-of-the-art LiDAR based 3D detection methods by a large margin. Furthermore, our network learns an effective discriminative representation of objects with various geometries, leading to encouraging results in 3D detection of pedestrians and cyclists, based on only LiDAR.},
  isbn = {978-1-5386-6420-9},
  langid = {english},
  file = {/Users/dplane/Zotero/storage/RPVEU2JM/Zhou and Tuzel - 2018 - VoxelNet End-to-End Learning for Point Cloud Based 3D Object Detection.pdf}
}

@article{zhu2024,
  title = {Camera, {{LiDAR}}, and {{IMU Based Multi-Sensor Fusion SLAM}}: {{A Survey}}},
  shorttitle = {Camera, {{LiDAR}}, and {{IMU Based Multi-Sensor Fusion SLAM}}},
  author = {Zhu, Jun and Li, Hongyi and Zhang, Tao},
  year = 2024,
  month = apr,
  journal = {Tsinghua Science and Technology},
  volume = {29},
  number = {2},
  pages = {415--429},
  issn = {1007-0214},
  doi = {10.26599/TST.2023.9010010},
  urldate = {2025-02-15},
  abstract = {In recent years, Simultaneous Localization And Mapping (SLAM) technology has prevailed in a wide range of applications, such as autonomous driving, intelligent robots, Augmented Reality (AR), and Virtual Reality (VR). Multi-sensor fusion using the most popular three types of sensors (e.g., visual sensor, LiDAR sensor, and IMU) is becoming ubiquitous in SLAM, in part because of the complementary sensing capabilities and the inevitable shortages (e.g., low precision and long-term drift) of the stand-alone sensor in challenging environments. In this article, we survey thoroughly the research efforts taken in this field and strive to provide a concise but complete review of the related work. Firstly, a brief introduction of the state estimator formation in SLAM is presented. Secondly, the state-of-the-art algorithms of different multi-sensor fusion algorithms are given. Then we analyze the deficiencies associated with the reviewed approaches and formulate some future research considerations. This paper can be considered as a brief guide to newcomers and a comprehensive reference for experienced researchers and engineers to explore new interesting orientations.},
  keywords = {Data integration,fusion,Laser radar,localization,Location awareness,multi-sensor fusion,navigation,Robot vision systems,Simultaneous localization and mapping,Simultaneous Localization And Mapping (SLAM),survey,Surveys,Visualization},
  file = {/Users/dplane/Zotero/storage/ZFDKWZ9Z/Zhu et al_2024_Camera, LiDAR, and IMU Based Multi-Sensor Fusion SLAM.pdf;/Users/dplane/Zotero/storage/5F6LAGFY/10258154.html}
}

@article{zorlu2025,
  title = {A {{Comprehensive Bibliometric Review}} of {{Autonomous Vehicle Research}}: {{Trends}}, {{Disciplines}}, and {{Future Directions}}},
  shorttitle = {A {{Comprehensive Bibliometric Review}} of {{Autonomous Vehicle Research}}},
  author = {Zorlu, Eray Can and {\c C}ift{\c c}i, Muhammed Ey{\"u}p and Zorlu, Eray Can and {\c C}ift{\c c}i, Muhammed Ey{\"u}p and Aydin, Metin Mutlu},
  year = 2025,
  month = may,
  doi = {10.56578/mits040205},
  urldate = {2025-11-01},
  file = {/Users/dplane/Zotero/storage/Y92ZX3F2/Zorlu et al. - 2025 - A Comprehensive Bibliometric Review of Autonomous Vehicle Research Trends, Disciplines, and Future.pdf}
}

@misc{zotero-1645,
  title = {Autonomous {{Vessels}} Are {{Becoming}} a {{Commercial Reality}}},
  journal = {The Maritime Executive},
  urldate = {2025-02-14},
  abstract = {TheAutonomous Ship Technology Symposium 2021conference brought together the largest public and priva...},
  howpublished = {https://maritime-executive.com/editorials/autonomous-vessels-are-becoming-a-commercial-reality},
  langid = {english},
  keywords = {News Article},
  file = {/Users/dplane/Zotero/storage/3Y3NY458/autonomous-vessels-are-becoming-a-commercial-reality.html}
}

@misc{zotero-1652,
  title = {More {{Data}} with {{Less Effort}} and {{Risk}}},
  urldate = {2025-02-14},
  abstract = {DEA Marine Services, a division of David Evans and Associates, Inc. (DEA), of Vancouver, Wash., has invested in and utilized Sea Machines Robotics\&rsq...},
  howpublished = {https://www.hydro-international.com/case-study/sea-machines-autonomy-enables-dea-marine-services-to-collect-more-data-with-less-effort-and-risk},
  langid = {english},
  keywords = {News Article},
  file = {/Users/dplane/Zotero/storage/AVQXU927/sea-machines-autonomy-enables-dea-marine-services-to-collect-more-data-with-less-effort-and-ris.html}
}

@misc{zotero-1745,
  title = {Camera {{Calibration}}},
  urldate = {2025-02-17},
  howpublished = {https://www.mathworks.com/help/vision/camera-calibration.html},
  keywords = {matlab},
  file = {/Users/dplane/Zotero/storage/XKH2REGE/camera-calibration.html}
}

@misc{zotero-1860,
  title = {Torc {{Pinpoint User Manual}}},
  file = {/Users/dplane/Zotero/storage/CEUP7K2V/_.pdf;/Users/dplane/Zotero/storage/JRXV9LTG/PinPoint User Manual v1.5 DRAFT.pdf}
}

@misc{zotero-1864,
  title = {Livox {{Horizon LiDAR User Manual}}},
  file = {/Users/dplane/Zotero/storage/MLFKZ8YW/Livox Horizon user manual v1.0.pdf}
}

@misc{zotero-item-1877,
  title = {[2104.09224] {{Multi-Modal Fusion Transformer}} for {{End-to-End Autonomous Driving}}},
  urldate = {2025-10-22},
  howpublished = {https://arxiv.org/abs/2104.09224},
  file = {/Users/dplane/Zotero/storage/TW94WNS7/2104.html}
}

@misc{zotero-item-1911,
  title = {Taxonomy and {{Definitions}} for {{Terms Related}} to {{Driving Automation Systems}} for {{On-Road Motor Vehicles J3016}}\_202104},
  urldate = {2025-10-22},
  howpublished = {https://www.sae.org/standards/j3016\_202104-taxonomy-definitions-terms-related-driving-automation-systems-road-motor-vehicles},
  keywords = {SAE J3016_202104},
  file = {/Users/dplane/Zotero/storage/TIFBWQ6B/j3016_202104-taxonomy-definitions-terms-related-driving-automation-systems-road-motor-vehicles.html}
}
