\documentclass[../main.tex]{subfiles}
\graphicspath{{\subfix{../images/}}}
\begin{document}

Real-time perception for autonomous surface vessels has evolved substantially over the past decade, primarily driven by advances in deep learning architectures and sensor technologies. %initially developed for terrestrial autonomous vehicles.
Vision-based detection methods, particularly single-stage architectures such as YOLO~\cite{redmon2016}, demonstrate strong performance on automotive benchmarks~\cite{geiger2013} but experience degradation under extreme illumination variations.
\ac{LiDAR}-based approaches offer complementary capabilities through geometric precision independent of illumination~\cite{zhou2018}.
Recent surveys of maritime autonomy~\cite{bae2023, zhang2021, xue2025, ferreira2022, bai2022} reinforce perception as a critical enabling technology, yet systematic performance characterization of these methods in maritime operational conditions remains limited.

The maritime environment presents distinct challenges compared to terrestrial scenarios.
Studies of visual detection in coastal and harbor settings report substantial performance degradation due to water reflections, high dynamic range, and low-texture backgrounds~\cite{prasad2017}.
\ac{LiDAR} systems face complementary difficulties with sparse water-surface returns and atmospheric scattering~\cite{kunz2005}.
While sensor fusion has been proposed to address individual modality limitations~\cite{huang2024a, liang2022}, most fusion research focuses on automotive applications using terrestrial datasets such as KITTI~\cite{geiger2013} or Waymo \cite{sun2020}, and its direct applicability to maritime platforms operating under power and computational constraints has not been thoroughly validated~\cite{wang2020a}.

This review synthesizes research across vision and \ac{LiDAR}-based detection methods independently before addressing multimodal fusion architectures and calibration requirements, and concludes by identifying specific research gaps motivating this dissertation's contributions.

\section{Modern Vision-Based Object Detection for Autonomy}

\subsection{Deep Learning Foundations and Single-Stage Detectors}

The application of convolutional neural networks to visual recognition gained widespread adoption following Krizhevsky et al.'s demonstration that deep architectures trained on large-scale datasets could achieve superior performance on ImageNet~\cite{krizhevsky2017}.
Subsequent architectural innovations, including residual learning frameworks~\cite{he2016}, demonstrated that network depth correlates with representational capacity for complex visual recognition tasks.
Early object detection methods adapted these classification networks into two-stage pipelines.
Girshick et al.~\cite{girshick2014} introduced R-CNN, which applies convolutional neural networks to region proposals; however, the computational overhead limited throughput.
Ren et al.~\cite{ren2016} addressed these limitations with Faster R-CNN, introducing region proposal networks that enabled end-to-end training and substantially improved inference speed.

Single-stage detectors emerged to address latency constraints by reformulating detection as direct regression from image features to object classes and bounding box coordinates.
The YOLO family exemplifies this paradigm, enabling real-time operation through unified spatial prediction~\cite{redmon2016}.
Subsequent YOLO iterations introduced architectural refinements, including multi-scale detection heads and anchor-free prediction mechanisms \cite{terven2023}.

YOLOv8, released in 2023 as a commercial implementation by Ultralytics \cite{ultralytics}, introduced architectural innovations addressing limitations of earlier single-stage detectors.
The anchor-free detection strategy employs dual detection heads that independently identify object presence and predict bounding box coordinates, simplifying transfer learning and enhancing adaptability to varying aspect ratios.
% The backbone incorporates Cross-Stage Partial Fusion (C2f) modules that enhance feature reuse while reducing computational overhead compared to earlier designs.
Modern loss functions, including Complete Intersection-over-Union (CIoU) and Distribution Focal Loss, improve localization precision and boundary regression stability~\cite{redmon2016}.
Max pooling and image down-sampling also contribute to CNN speed, but 
impose a fundamental constraint on small object detection (SOD): objects smaller than approximately 16-32 pixels produce low-confidence or unstable detections~\cite{rekavandi2022}.
% However, the multi-scale detection architecture with feature-map strides of 8, 16, and 32 pixels imposes fundamental constraints: objects smaller than approximately 16-32 pixels produce low-confidence or unstable detections due to progressive downsampling~\cite{rekavandi2022}.
For maritime applications, down-sampling imagery to 640×640 pixel inputs corresponds to smaller or distant targets near the horizon occupying less than 0.5-1\% of the image frame.
These architectural properties make YOLOv8 suitable for real-time use; however, the model's constraints on SOD, along with other environmental factors, motivate the investigation of complementary sensing modalities.


% Transfer learning from large-scale datasets like ImageNet~\cite{krizhevsky2017} and COCO~\cite{lin2015} has become standard practice for vision-based detectors, and adapting these datasets for the maritime domain still presents challenges.
% Guo et al.~\cite{guo2023} found that zero-shot transfer of YOLOv5 across maritime datasets yields poor performance. However, limited fine-tuning from the target domain substantially improves accuracy, suggesting learned features encode distributional assumptions about texture, spatial layouts, and illumination that fail to generalize across domains~\cite{tzeng2017}.

% \subsection{Maritime-Specific Detection Challenges}

Visual detection in maritime environments presents challenges that are fundamentally distinct from those in terrestrial scenarios.
Prasad et al.~\cite{prasad2017} identify background motion from waves and wakes, low contrast objects, and environmental degradation as primary failure modes.
High dynamic range presents persistent challenges for maritime detection.
While HDR imaging offers theoretical advantages for high-contrast maritime environments, empirical studies report counterintuitive results~\cite{landaeta, liebergall, wang2019a}.
Networks pre-trained on standard dynamic range (SDR) datasets achieve 10\% higher recall than HDR-based approaches~\cite{landaeta, liebergall}.
This performance gap stems from pre-training bias: networks learn feature extraction strategies optimized for SDR characteristics, and the scarcity of HDR training data prevents effective adaptation through transfer learning.
Atmospheric conditions impose additional constraints: Zhang et al.~\cite{zhang2022a} addressed foggy maritime detection through preprocessing and attention mechanisms. In contrast, Rekavandi et al.~\cite{rekavandi2022} note that objects occupying less than 10\% of image area yield insufficient information for reliable classification.
Maritime scenarios exacerbate this difficulty, as vessels and navigation markers often appear near the horizon, occupying a minimal pixel area.
Shao et al.~\cite{shao2022} addressed this through multi-scale detection strategies and deformable convolution modules.
Maritime detection research remains fragmented, lacking standardized large-scale datasets analogous to KITTI or COCO~\cite{zhang2021, yang2024, su2023}.

% \subsection{Synthesis: Gaps in Vision-Based Maritime Detection}

% The reviewed literature reveals substantial gaps across three critical dimensions relevant to real-time embedded maritime perception.

% First, image quality and resolution trade-offs remain uncharacterized for maritime applications.
% Model performance on small maritime objects degrades substantially at reduced input resolutions
% % yet native HDR resolution processing exceeds embedded GPU throughput constraints
% ~\cite{shao2022, rekavandi2022}.
% The optimal resolution-accuracy-latency trade-off for embedded maritime platforms remains unquantified.
% Furthermore, systematic evaluation of HDR preprocessing for maritime YOLO variants is absent, with available studies reporting contradictory findings~\cite{landaeta, liebergall}, leaving uncertainty about whether HDR provides practical advantages sufficient to justify computational overhead.

% Second, embedded platform characterization gaps prevent informed deployment decisions.
% While qualitative descriptions of YOLO model variants exist~\cite{kim2022, guo2023}, quantitative characterization of accuracy-latency-power consumption trade-offs on representative embedded hardware for maritime-specific object classes is missing.
% Power consumption directly affects mission endurance for battery-operated vessels, yet few studies report energy efficiency metrics alongside detection accuracy, making it impossible to predict operational range impacts of algorithm selection.

% Third, training and domain adaptation requirements for maritime deployment lack systematic investigation.
% Limited evidence exists regarding fine-tuning data requirements for adapting terrestrial pre-trained models to maritime environments~\cite{guo2023}.
% The relationship between training set size, environmental diversity, and generalization performance remains unestablished, creating uncertainty about dataset collection efforts needed for reliable maritime detection.

% These interconnected gaps—spanning image preprocessing, hardware deployment, and training requirements—motivate this dissertation's systematic evaluation of YOLO variants under maritime-specific conditions with emphasis on embedded platform feasibility.

% While vision-based detection faces illumination-dependent challenges, geometric sensing modalities offer complementary capabilities through illumination-independent three-dimensional measurements.

\section{LiDAR-Based Object Detection for Autonomous Systems}

\subsection{Deep Learning Approaches for Point Cloud Processing}

\acl{LiDAR} sensors provide three-dimensional geometric measurements independent of illumination by measuring the time-of-flight of laser pulses.
% Kunz et al.~\cite{kunz2005} demonstrated that \ac{LiDAR} reflections from small maritime targets produce significantly stronger returns than sea surface returns, with small buoys detectable at ranges exceeding 9 kilometers under adverse weather.
Processing point cloud data presents unique computational challenges: unlike images with a regular grid structure, point clouds present non-uniform three-dimensional sampling of the surroundng area~\cite {zhou2018, shi2019}.
This sparse sampling presents opportunities for a variety of methods of detection.

Deep learning has emerged as the dominant paradigm for point cloud object detection, with neural networks designed to learn hierarchical feature representations from three-dimensional data.
These learned approaches leverage large labeled datasets to extract spatial patterns and semantic information directly from point coordinates.
The following architectural families represent the evolution of neural network-based methods, progressing from point-based processing to voxelization strategies and hybrid architectures.

Point-based networks (PointNet~\cite{qi2017}) process raw coordinates directly; however, they scale unfavorably with point cloud size~\cite{shi2019}.
Voxelization methods address scalability by discretizing point clouds into regular grids.
Zhou and Tuzel~\cite{zhou2018} developed VoxelNet, which transforms sparse point clouds into dense volumetric representations.
Yan et al.~\cite{yan2018} extended this with SECOND, introducing sparse convolutional operations that improve efficiency by processing only occupied voxels.
Lang et al.~\cite{lang2019} proposed PointPillars, which encodes point clouds into vertical columns, enabling efficient 2D convolutional processing.
The fundamental trade-off in voxelization is quantization error: small objects or fine geometric details may be lost during the discretization process~\cite {zhou2018}.
Hybrid architectures (PV-RCNN~\cite{shi2019, shi2020}) combine the strengths of point-based and voxel-based processing, achieving state-of-the-art automotive accuracy but assuming dense ground plane returns that do not hold for maritime applications where water surfaces produce sparse returns~\cite{kunz2005}.

Recent innovations demonstrate continued advancement, but with substantial computational requirements.
BEVFusion~\cite{liang2022, liu2023b} achieves state-of-the-art accuracy but requires desktop-class GPUs with substantial power consumption and achieves only 15-20 frames per second~\cite{liu2023b}—this power budget alone exceeds total available power for many small autonomous vessels.
Transformer-based architectures that apply self-attention to point clouds offer theoretical advantages but require multi-GPU configurations with substantial power consumption and have not been validated under maritime-specific conditions~\cite{xie2024}.

Beyond computational constraints, these neural network-based approaches face limitations for safety-critical maritime applications.
Enforcing strict rules in neural networks, such as ensuring object separation distances, is difficult due to their non-deterministic nature.
This limitation is particularly problematic when using spatial data for navigational purposes and obstacle avoidance~\cite{coyleE}.
Given this persistent challenge, there is a continued need to investigate deterministic approaches that can provide stricter guarantees for safety-critical applications. 

\subsection{Deterministic Geometric Alternatives}

In contrast to neural network-based approaches, deterministic geometric algorithms offer advantages for resource-constrained embedded platforms, where runtime predictability and minimal training data requirements are highly valued.
Coyle~\cite{coyleE} developed Grid-Based Clustering and Concave Hull Extraction (GB-CACHE), a deterministic approach specifically designed for unmanned systems perceiving objects on flat surfaces, including water.
GB-CACHE efficiently segments point clouds through grid-based spatial partitioning, then extracts concave hull boundaries that accurately represent objects with irregular geometries.
Computational efficiency analysis demonstrates real-time feasibility on low to mid-grade computing platforms~\cite{coyleE}.

The deterministic nature of GB-CACHE provides specific operational advantages.
Processing latency remains bounded and predictable, eliminating variable inference times characteristic of neural networks, where batch size and scene complexity affect throughput.
Rule enforcement for spatial constraints—critical for obstacle avoidance and navigation—can be rigorously guaranteed, whereas neural approaches provide probabilistic outputs without strict guarantees~\cite{coyleE}.
Training data requirements are minimal; the algorithm requires only geometric parameters rather than extensive labeled datasets—a practical advantage for maritime deployment where training data scarcity persists.

\subsection{Maritime-Specific LiDAR Challenges}

Maritime \ac{LiDAR} performance differs fundamentally from automotive scenarios.
Automotive systems achieve near 100\% return rates on asphalt, providing dense point clouds with abundant ground plane context~\cite{roriz2022}.
Water surfaces produce specular reflection, yielding only 10\% return rates versus near-100\% on asphalt~\cite{kunz2005, roriz2022}, creating sparse point clouds lacking dense ground plane context exploited by automotive algorithms~\cite{zhou2018, shi2019}.
Kunz et al.~\cite{kunz2005} measured return rates over calm water as low as 10\% of the emitted pulses, noting that water surface characteristics produce significantly weaker returns than small floating targets, creating a favorable contrast for target detection despite a sparse environmental context.

The sparsity of water returns poses challenges to detection algorithms designed for dense point clouds in automotive applications.
Maritime applications lack dense reference, necessitating alternative spatial reasoning strategies when objects appear isolated without contextual grounding.
Atmospheric effects reduce effective range by 50-70\% in heavy haze~\cite{roriz2022}, while wave-induced motion requires IMU-based motion compensation, introducing processing latency~\cite{xie2024, ahmed2024}.
Platform motion caused by continuous wave-induced pitch, roll, and heave motions vary dynamically with sea state.
Ahmed et al.~\cite{ahmed2024} developed specialized Kalman filtering approaches tailored for maritime platform motion, reporting 25-30\% improvement in detection accuracy when compensating for tilt-induced point cloud distortions.

Recent maritime \ac{LiDAR} detection work demonstrates growing interest yet reveals substantial validation gaps.
Xie et al.~\cite{xie2024} achieved an overall detection accuracy of 74.1\% by integrating state-of-the-art networks for ship detection, although performance degraded for vessels under 5 meters and at ranges exceeding 40 meters.
Detection methods designed for dense ground plane returns require fundamental adaptation for sparse maritime point clouds where objects lack grounding context~\cite{kunz2005}.
A systematic evaluation of detection performance versus water surface characteristics, motion compensation quality, and embedded platform profiling (including latency, power, and throughput) is absent from the reviewed literature~\cite{xie2024, ahmed2024}.

The complementary limitations of camera and \ac{LiDAR} modalities—vision failing under extreme lighting while LiDAR struggles with sparse returns—suggest potential benefits from sensor fusion.

\section{Sensor Fusion Paradigms}

Sensor fusion combines information from multiple modalities to overcome the limitations of individual sensors.
Recent surveys~\cite{feng2021, tufekci2023, huang2024a} identify fusion level—early, mid, or late—as the primary architectural distinction determining computational requirements, calibration sensitivity, and failure mode propagation relevant to maritime deployment.
Each fusion paradigm offers distinct trade-offs between peak accuracy potential, robustness to sensor degradation, and real-time implementation feasibility~\cite{feng2021}.

\subsection{Early Fusion: Data-Level Integration}

Early fusion combines raw sensor data before feature extraction, projecting measurements from different modalities into a common representation~\cite{cui2022}.
Cui et al.~\cite{cui2022} note that early fusion enables networks to learn joint features during training.
PointPainting exemplifies this paradigm by projecting \ac{LiDAR} points into camera image coordinates, assigning semantic labels from segmentation networks, then processing the semantically enriched point cloud through object detectors~\cite{cui2022}.

However, early fusion imposes stringent calibration and synchronization requirements, creating practical deployment challenges.
Projection errors of even a few degrees of rotation or centimeters of translation misalign features, corrupting the joint representation~\cite{cui2022}.
Temporal synchronization must be precise; misaligned timestamps create spatial inconsistencies, degrading fusion quality.
Sensor-specific noise and failures propagate directly: corrupted camera images from lens occlusion or extreme lighting produce degraded semantic labels affecting all projected \ac{LiDAR} points~\cite{cui2022}.
For maritime platforms experiencing continuous wave-induced motion and intermittent sun glare, calibration sensitivity and failure propagation of early fusion represent significant operational risks.
Maritime platform dynamics introduce additional calibration complexity beyond automotive scenarios.
While automotive research established general calibration tolerances~\cite{cui2022}, wave-induced motion can create time-varying calibration errors even when initial alignment is precise.
These dynamic calibration challenges affect all fusion paradigms but with varying severity depending on the required precision of spatial correspondence.

\subsection{Mid Fusion: Feature-Level Integration}

Mid fusion extracts features independently from each modality before combining them in intermediate network layers~\cite{huang2024a}.
Huang et al.~\cite{huang2024a} systematically compare fusion paradigms, finding that mid fusion balances accuracy and robustness: independent feature extraction prevents raw sensor corruption from propagating between modalities, yet learned fusion layers capture cross-modal correlations unavailable to late fusion.

BEVFusion~\cite{liang2022, liu2023b} transforms camera features into bird's-eye-view representations compatible with \ac{LiDAR} BEV features, combining them through efficient convolution operations.
Liu et al.~\cite{liu2023b} note that BEVFusion achieves state-of-the-art performance but requires desktop-class GPU hardware, exceeding embedded platform capabilities by an order of magnitude.
For battery-powered maritime platforms with constrained power budgets, mid-fusion's computational demands preclude direct deployment without substantial architectural optimization.

\subsection{Late Fusion: Decision-Level Integration}

Late fusion operates on independent object detections from each modality, combining outputs through spatial association~\cite{wang2020a, pang2020}.
This approach maintains modular detection pipelines, fusing results at the decision level after individual sensor processing is complete.
While sacrificing joint feature learning enabled by earlier fusion, late fusion offers architectural properties advantageous for resource-constrained maritime deployments.

Wang et al.~\cite{wang2020a} survey multi-sensor fusion in automated driving, emphasizing that late fusion enables graceful degradation: sensor failures affect only individual detection streams without corrupting fusion outputs.
Pang et al.~\cite{pang2020} developed CLOCs, which fuse detection candidates before non-maximum suppression, training networks to predict fused detection quality based on geometric and semantic consistency between modalities.

Late fusion's modular architecture provides several practical advantages for maritime deployment.
Computational requirements scale approximately linearly with sensor count rather than exponentially as with joint processing~\cite{wang2020a}.
Processing latency remains predictable because each modality follows fixed pipelines without runtime cross-modal dependencies during feature extraction.
Adding or removing sensors requires minimal system redesign—new detection streams integrate through updated fusion modules without modifying existing pipelines~\cite{feng2021}.
Most critically for maritime applications, individual sensor failures degrade performance gracefully rather than causing catastrophic fusion failure~\cite{huang2024a}.
Temporal synchronization requirements are relaxed compared to early fusion: late fusion spatial association operates on object-level regions rather than pixel-level correspondences, tolerating larger timing offsets—typically 50 to 100 milliseconds versus 10 milliseconds for early fusion~\cite{huang2024a}.
For maritime platforms where precise hardware synchronization may be impractical and wave-induced motion creates apparent object movement, relaxed timing tolerances represent operational advantages.

\subsection{Comparative Analysis and Maritime Fusion Gaps}

Huang et al.~\cite{huang2024a} systematically evaluate fusion paradigms under simulated sensor degradation and synchronization error.
Late fusion maintains superior robustness when individual sensors fail or produce degraded outputs through graceful degradation.
However, under ideal conditions with clean data and perfect calibration, mid fusion achieves higher peak accuracy by learning cross-modal correlations unavailable to late fusion~\cite{huang2024a}.
Wang et al.~\cite{wang2020a} note that mid-fusion architectures approximately double processing requirements compared to single-modality detection.
For battery-powered maritime platforms, doubled power consumption directly reduces mission endurance.

These findings from automotive research suggest that operational requirements and constraints should inform the selection of fusion strategy.
Early and mid fusion optimize for peak accuracy under ideal conditions but require precise calibration, tight synchronization, and substantial computational resources~\cite{feng2021, cui2022}.
Late fusion prioritizes robustness and computational efficiency under degraded conditions, accepting reduced peak accuracy compared to deeper fusion strategies~\cite{huang2024a, wang2020a}.

Automotive fusion methods~\cite{feng2021, cui2022, huang2024a} evaluate performance on terrestrial datasets where environmental characteristics differ fundamentally from maritime scenarios—water reflections, extreme dynamic range, sparse \ac{LiDAR} returns, and platform motion require empirical validation.
Computational requirements, power consumption, and latency characteristics on representative embedded hardware are rarely reported~\cite{liu2023a}.
Systematic evaluation sweeping timing offsets to establish acceptable synchronization tolerances for maritime platforms is absent~\cite{huang2024a}, and wave-induced motion creates time-varying calibration errors differing from static errors typical of ground vehicles~\cite{cui2022}.
Most fusion studies report performance without rigorous comparison against optimized single-modality baselines~\cite{farahnakian2020, haghbayan2018, Clunie2021}.

% \section{Temporal and Spatial Alignment in Fusion Systems}

% Accurate fusion depends on establishing spatial and temporal correspondence between sensors.
% Spatial calibration defines rigid-body transformation relating sensor coordinate frames, while temporal synchronization ensures observations represent the exact moment in time.
% Precision requirements vary with the fusion paradigm—early fusion demands tighter tolerances than late fusion due to the need for pixel-level data alignment.

% Extrinsic calibration establishes a geometric relationship between sensors, typically represented as a rotation matrix and a translation vector.
% Traditional calibration employs fiducial targets, such as checkerboards or AprilTags, visible to both sensors, solving for the transformation through least-squares optimization.
% Manual calibration remains standard for many research systems, as it provides the precision necessary for reliable geometric correspondence.
% Automated calibration methods have emerged as alternatives. Iyer et al.~\cite{iyer2018} developed CalibNet, which uses deep learning to predict calibration parameters directly from sensor data.
% Xiao et al.~\cite{xiao2024} proposed CalibFormer, a transformer-based automatic LiDAR-camera calibration network~\cite{schneider2017, wu2021}.
% However, these automated approaches require sufficient scene structure and may fail in textureless maritime environments, which are often dominated by sky and water.

% Calibration accuracy directly affects fusion quality.
% Rotation errors exceeding one degree or translation errors exceeding 10 millimeters significantly degrade early and mid-fusion performance.
% These tolerances become more stringent as sensor separation increases.

% Temporal synchronization ensures observations represent the same world state~\cite{westenberger2011}.
% Timing offsets create spatial misalignment: a vessel moving at 5 meters per second experiences 0.5-meter displacement in 100 milliseconds.
% IEEE 1588 Precision Time Protocol achieves sub-microsecond accuracy~\cite{ptp2008}, while Network Time Protocol provides millisecond-level accuracy~\cite{furgale2013, liu2021}.
% Early fusion requires synchronization within 10 milliseconds; late fusion tolerates 50-100 milliseconds.
% Maritime wave-induced motion produces apparent object movement requiring motion compensation beyond timestamp alignment.

\section{Maritime Perception Datasets and Benchmarks}

Dataset availability fundamentally shapes algorithm development and validation.
Automotive perception benefits from large-scale benchmarks (KITTI~\cite{geiger2013}, Waymo, nuScenes~\cite{caesar2020}) providing millions of labeled examples with synchronized camera-LiDAR data.
These benchmarks enable quantitative comparison across algorithms and have accelerated progress in terrestrial autonomous vehicle perception.

Maritime perception lacks comparable resources; most datasets provide only monocular RGB imagery without depth information~\cite{su2023}.
Those including \ac{LiDAR} often lack precise temporal synchronization or provide only sparse annotations covering limited object categories~\cite{su2023}.
Weather diversity remains underrepresented—most datasets capture clear conditions with few examples of rain, fog, or heavy seas.
Su et al.~\cite{su2023} provide a comprehensive survey of maritime vision datasets, documenting available resources and identifying critical gaps.
Kim et al.~\cite{kim2022} improved the Singapore Maritime Dataset, providing RGB, infrared, \ac{LiDAR}, and radar modalities, though sparse ground truth limits the scope for training deep networks.
Recent maritime datasets~\cite{thompson2023} provide multimodal observations but remain limited in scale, weather diversity, and public availability.
Small datasets restrict model complexity without overfitting; limited weather coverage prevents robust evaluation; sparse annotations leave uncertainty about detection reliability~\cite{su2023}.


\begin{table}[htbp]
\begin{tabular}{lcc}
\hline
Dataset                          & Image & PointCloud \\
\hline
\hline
\multicolumn{3}{l}{AGV}                                \\
\hline
nuScenes \cite{caesar2020}          & x     & x          \\
Cityscapes \cite{cordts2016}        & x     &            \\
BDD100K\cite{yu2020}                & x     &            \\
Waymo \cite{sun2020}                & x     & x          \\
KITTI \cite{geiger2013}             & x     & x          \\
SYNTHIA \cite{ros2016}              & x     &            \\
Apolloscape \cite{huang2020}        & x     & x          \\
CADC \cite{pitropov2021}            & x     & x          \\
\hline
\multicolumn{3}{l}{USV}                            \\
\hline
MaSTr1325 \cite{bovcon2019}         & x     &            \\
MaSTr1478 \cite{zust2022}           & x     &            \\
MODS \cite{bovcon2022}              & x     &            \\
SMD \cite{prasad2017}               & x     &            \\
Maritime Perception \cite{lin2022}  &       & x         \\
\hline
\end{tabular}
\caption{A comparison of recent and popular datasets for ground-based and surface vessels}
\label{tbl:agv_usv_datasets}
\end{table}
% This research contributes synchronized \ac{LiDAR}-camera dataset collected aboard \acl{WAMV} research vessel during extended operational deployments, addressing key gaps through HDR camera imagery capturing extreme dynamic range conditions, synchronized Livox Horizon \ac{LiDAR} data, diverse maritime object annotations (vessels, buoys, navigation markers), varied weather and lighting conditions spanning multiple sea states, and platform motion data enabling motion compensation validation.



\section{Research Gaps and Dissertation Motivation}

Despite substantial progress in multimodal perception for autonomous systems, several critical gaps remain for maritime applications.
These gaps span algorithmic approaches, computational implementation, and empirical validation resources.

Most published fusion research emphasizes early or mid-level integration, with limited evaluation of decision-level approaches under maritime conditions.
Comparative studies in automotive contexts demonstrate that late fusion offers superior robustness under sensor dropout or environmental degradation—conditions frequently encountered during maritime operations~\cite{huang2024a, wang2020a}.
However, these findings primarily derive from terrestrial datasets where environmental characteristics and sensor failure modes differ substantially from those in maritime scenarios.
Whether late fusion maintains similar robustness advantages under maritime-specific challenges, including water reflections, HDR lighting extremes, and platform motion, remains an open empirical question in the reviewed literature.
This dissertation addresses this gap through systematic evaluation of late fusion performance under maritime conditions, as described in Chapter~\ref{realtime_object_detection}.

Real-time fusion implementation on embedded maritime hardware remains underexplored.
Early and mid-fusion methods often exceed the computational budgets of embedded platforms, requiring desktop-class GPUs with typical power consumption of 200-400 watts for real-time operation—specifications incompatible with small, battery-powered autonomous vessels.
Late fusion's reduced computational requirements suggest potential feasibility for embedded deployment; however, systematic evaluation of accuracy-latency-power trade-offs on desktop-class maritime hardware is lacking in prior maritime fusion research.
This dissertation evaluates YOLO model variants (nano, small, medium) on Minion B workstation hardware (NVIDIA GeForce GTX 1080 GPU), establishing baseline performance and selecting the small variant for its equivalent accuracy to medium with improved inference speed, as detailed in Chapter~\ref{realtime_object_detection}.
Power consumption is particularly critical for battery-operated platforms where computing loads directly affect mission endurance.

Few labeled multimodal maritime datasets exist with synchronized \ac{LiDAR} and camera observations under diverse conditions.
Existing datasets are either single-modality, which limits fusion research, lack precise synchronization, preventing temporal analysis, or provide limited weather and lighting diversity, thereby restricting generalization assessment.
This data gap directly impedes algorithm development and validation.
For this research, a synchronized \ac{LiDAR}-camera dataset was collected aboard the \acl{WAMV} platform during operational deployments, providing 778 annotated images with 1934 labeled maritime objects across diverse weather and lighting conditions for experimental validation, as described in Section~\ref{sec:sensor_data_dataset}.

This dissertation develops and validates a late-fusion framework for real-time \ac{LiDAR}-camera perception on embedded maritime platforms.
The approach is motivated by the advantages of late fusion for maritime deployment, including a modular architecture that isolates sensor failures, computational efficiency compatible with embedded hardware constraints, and robustness to calibration and synchronization errors compared to tightly coupled fusion approaches.
Research methodology quantitatively evaluates accuracy, timing, and computational efficiency through controlled experiments and real-world validation aboard the \ac{WAMV} research platform.

The following chapter describes experimental methodology employed to address these research questions, including detailed hardware configuration, sensor calibration procedures, data collection protocols, and late fusion algorithm design.

\end{document}
