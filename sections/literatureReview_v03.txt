\documentclass[../main.tex]{subfiles}
\graphicspath{{\subfix{../images/}}}
\begin{document}

\section{Modern Vision-Based Object Detection for Autonomy}

Deep learning has fundamentally transformed visual object detection, establishing convolutional neural networks as the dominant paradigm for autonomous perception. Modern architectures extract hierarchical features through successive convolutional layers, learning representations that capture both low-level edges and high-level semantic concepts. These models have demonstrated remarkable performance on large-scale benchmarks, achieving human-level accuracy on many object categories \textcolor{red}{*}.

Two-stage detectors such as Faster R-CNN pioneered the region-based approach, first generating candidate object proposals and then refining classifications through dedicated networks \textcolor{red}{*}. While accurate, these architectures impose computational overhead that challenges real-time operation on embedded hardware. Single-stage detectors emerged as an alternative, sacrificing some accuracy for substantial speed improvements. The YOLO family exemplifies this approach, treating detection as a direct regression problem from image pixels to bounding box coordinates and class probabilities \textcolor{red}{*}.

Recent iterations—YOLOv5 through YOLOv8—have progressively refined this architecture through innovations in backbone design, feature pyramid networks, and anchor-free detection heads \textcolor{red}{*}. These models achieve real-time frame rates on GPU-accelerated platforms while maintaining competitive accuracy. However, achieving real-time performance on embedded platforms such as the NVIDIA Jetson AGX Xavier or Orin typically requires model quantization, reduced input resolution, or architectural modifications. The computational constraints become particularly acute when processing high-resolution imagery or when scenes contain many small objects requiring fine-grained feature extraction.

EfficientDet introduced compound scaling that jointly optimizes network depth, width, and input resolution to maximize accuracy within computational budgets \textcolor{red}{*}. This approach demonstrates strong performance-efficiency trade-offs on terrestrial datasets, though the reliance on dense texture features limits effectiveness in environments dominated by low-texture regions. Transformer-based architectures such as DETR have recently emerged, reformulating detection as a set prediction problem through attention mechanisms \textcolor{red}{*}. While these models show promise for capturing long-range dependencies and handling occlusion, their computational requirements substantially exceed those of convolutional approaches, limiting their deployment on resource-constrained platforms.

The success of these architectures depends critically on transfer learning from large-scale datasets. ImageNet provides millions of labeled images spanning thousands of object categories, enabling networks to learn generalizable visual representations \textcolor{red}{*}. The COCO dataset extends this foundation with instance segmentation annotations and diverse scene compositions, forming the primary benchmark for detection research \textcolor{red}{*}. Pre-trained models initialized with weights learned from these datasets can adapt to new domains with significantly less training data than training from random initialization would require.

However, this transfer learning paradigm introduces domain adaptation challenges when applied beyond terrestrial environments. The learned features encode assumptions about object appearance, texture diversity, spatial layouts, and illumination conditions characteristic of the training distribution. When these assumptions fail—as they do in many specialized environments—performance degrades substantially. Domain-specific fine-tuning or specialized training datasets become necessary to recover acceptable detection accuracy.

\subsection{Challenges in Specialized Environments}

The maritime environment presents conditions fundamentally different from the terrestrial scenes that dominate training datasets. Sky and water surfaces comprise the majority of image area, providing minimal texture gradients for feature extraction. Horizon lines create strong edges that can interfere with object localization algorithms designed for scenes with distributed visual complexity. The high dynamic range of maritime scenes—where bright sky coexists with shadowed vessel hulls—exceeds the capture capability of standard cameras, forcing trade-offs between overexposed highlights and underexposed shadows.

Sun glare produces saturated image regions where all feature information is lost, creating detection blind spots. Specular reflections from water surfaces generate mirror-image artifacts that detection networks may misclassify as objects. The relatively uniform appearance of water and sky provides little contextual information to disambiguate true objects from reflections or sensor artifacts. These characteristics collectively degrade the performance of models trained primarily on terrestrial imagery, where texture diversity and consistent lighting are implicit assumptions.

Beyond maritime applications, other specialized domains face analogous challenges. Aerial imagery from unmanned aircraft exhibits similar horizon-dominated compositions and variable lighting. Underground mining environments present extreme low-light conditions with limited texture. Medical imaging modalities such as ultrasound or X-ray lack the RGB color information that many detection architectures assume. Each domain requires adaptation strategies—whether through specialized preprocessing, domain-specific architectures, or curated training datasets—to achieve reliable performance \textcolor{red}{*}.

\section{Modern LiDAR-Based Object Detection for Autonomy}

Light detection and ranging sensors provide three-dimensional geometry independent of illumination conditions, making them complementary to camera-based perception. LiDAR systems emit laser pulses and measure time-of-flight to determine range, generating point clouds that represent surface geometry with millimeter-scale precision. Unlike cameras, LiDAR performance remains consistent across lighting conditions—from direct sunlight to complete darkness—eliminating a major failure mode of vision-based systems.

Processing point cloud data for object detection presents unique challenges distinct from image-based approaches. Point clouds are unordered sets lacking the regular grid structure that enables efficient convolution operations in image processing. Points are distributed non-uniformly across space, with density varying by range and surface orientation. Occlusion creates irregular gaps in object representations. These properties motivated the development of specialized architectures for point cloud processing.

Point-based networks such as PointNet operate directly on raw point coordinates, learning permutation-invariant features through symmetric aggregation functions \textcolor{red}{*}. PointNet++ extends this approach with hierarchical feature extraction, capturing multi-scale geometric patterns through successive neighborhood grouping \textcolor{red}{*}. While elegant in their handling of unordered data, these networks face computational scalability challenges as point cloud size increases—a common scenario in long-range outdoor sensing.

Voxel-based methods discretize point clouds into regular three-dimensional grids, enabling efficient processing through 3D convolutions. VoxelNet pioneered this approach, using voxel feature encoding to transform sparse point clouds into dense volumetric representations suitable for convolutional processing \textcolor{red}{*}. The trade-off is quantization error—small objects or fine geometric details may be lost during voxelization, and the choice of voxel resolution creates a fundamental accuracy-efficiency trade-off.

Hybrid architectures combine voxel and point-based processing to leverage their complementary strengths. PV-RCNN generates region proposals from voxel features, then refines predictions using point-based feature extraction within candidate regions \textcolor{red}{*}. This two-stage approach achieves high accuracy while maintaining computational efficiency through focused point processing only in regions likely to contain objects.

The automotive industry has driven substantial advances in LiDAR-based detection, with large-scale datasets such as KITTI, Waymo Open Dataset, and nuScenes providing the foundation for algorithm development \textcolor{red}{*}. These benchmarks enable quantitative comparison across architectures and have accelerated progress in 3D object detection for road vehicles. However, the environmental assumptions embedded in these datasets and their trained models limit direct transferability to other domains.

Recent developments in bird's-eye-view representations and transformer architectures offer promising directions. BEVFusion unifies camera and LiDAR features in a shared bird's-eye-view space, enabling efficient spatial alignment without explicit geometric projection \textcolor{red}{*}. TransFusion employs transformer attention mechanisms to aggregate multi-scale features across modalities \textcolor{red}{*}. Point Transformer applies self-attention directly to point clouds, capturing long-range dependencies that may help distinguish objects from complex backgrounds \textcolor{red}{*}. However, these architectures typically require multi-GPU configurations or high-end accelerators such as the NVIDIA RTX 4090 or A100, making them unsuitable for small autonomous platforms without substantial computing infrastructure or offboard processing capabilities. The power consumption alone—often exceeding 200-400 watts during inference—surpasses the total power budget of many small autonomous vehicles. These transformer-based architectures have not been validated for real-time operation on embedded platforms typical of autonomous surface vessels.

\subsection{Domain-Specific LiDAR Challenges}

While LiDAR eliminates illumination-dependent failures of cameras, it introduces modality-specific challenges that vary substantially across operating environments. Automotive LiDAR systems achieve near 100\% return rates on asphalt and concrete surfaces, providing dense geometric representations ideal for detection algorithms trained on road scenes. However, these high return rates depend on diffuse reflection from rough surfaces—a property not universal across environments.

Water surfaces produce sparse, inconsistent returns due to specular reflection. Calm water may return less than 10\% of emitted pulses, with most laser energy reflecting away from the sensor rather than back toward it \textcolor{red}{*}. Choppy water improves return rates slightly by creating varied surface orientations, but introduces noise and measurement uncertainty. This sparsity challenges detection algorithms designed for the dense point clouds typical of terrestrial scenes, where objects are surrounded by abundant ground-plane context.

Atmospheric conditions affect LiDAR performance through beam attenuation and scattering. Fog, rain, and spray scatter laser pulses, reducing effective range and introducing false returns from suspended water droplets. Visibility degradation from 20 kilometers in clear conditions to 2 kilometers in heavy haze corresponds to 50-70\% reduction in effective LiDAR range \textcolor{red}{*}. While cameras face similar degradation, the failure modes differ—cameras lose contrast and semantic information while LiDAR maintains geometric accuracy for returns that do penetrate the atmosphere.

Platform motion introduces additional complexity for mobile LiDAR systems. Automotive platforms operate on roads with predictable motion patterns and relatively stable sensor orientations. Maritime platforms experience continuous wave-induced pitch, roll, and heave motion that varies dynamically with sea state. This motion appears as point cloud distortion unless compensated through IMU integration and motion correction algorithms. The computational cost of motion compensation and the latency it introduces must be considered when evaluating real-time performance feasibility.

Object reflectance properties vary substantially across target categories. Metallic surfaces produce strong returns across most LiDAR wavelengths. Painted surfaces exhibit wavelength-dependent reflectivity. Dark, non-reflective materials such as rubber may produce weak or absent returns despite being geometrically visible. These reflectance variations affect detection reliability differently than camera-based systems, where color and texture dominate appearance.

\section{Applicability to Maritime Autonomous Surface Vessels}

The transition from automotive and general unmanned vehicle perception to maritime applications reveals substantial performance gaps. Both vision and LiDAR systems face environment-specific failure modes that degrade the effectiveness of algorithms developed for terrestrial operation. Understanding these maritime-specific challenges is essential for developing robust perception systems for autonomous surface vessels.

Maritime scenes violate many implicit assumptions of terrestrial perception systems. The horizon divides most images into two dominant regions—sky above and water below—with objects typically appearing near this boundary. This composition differs fundamentally from terrestrial scenes where objects appear distributed across the image with varied backgrounds providing contextual cues. Detection networks trained on terrestrial datasets learn spatial priors about where objects typically appear; these priors become misleading in maritime environments where most objects occupy narrow horizon regions.

The extreme dynamic range of maritime scenes challenges standard imaging systems. Bright sky often exceeds sensor saturation limits while shadowed portions of vessels remain underexposed. Standard cameras with 8-12 bits per pixel cannot simultaneously capture detail in both regions, forcing exposure compromises that degrade detection performance. HDR imaging techniques—whether through multi-exposure fusion or specialized sensors—can extend capture range but introduce computational overhead and may affect real-time processing feasibility.

Sun glare creates intermittent detection blind spots as platform and target orientations vary relative to the sun. Morning and evening operation faces low-angle illumination that produces extended glare periods. Specular reflections from water surfaces generate mirror images that may be misclassified as objects, increasing false positive rates. Detection networks must learn to distinguish true objects from reflections—a distinction that requires understanding physical reflection geometry not typically encountered in terrestrial training data.

LiDAR systems face complementary maritime challenges. Water surface returns are sparse and inconsistent compared to the dense ground-plane returns typical of road environments. Detection algorithms designed for automotive applications rely on abundant ground context to establish spatial reference frames and disambiguate objects from background. Maritime applications lack this dense ground-plane reference, requiring alternative spatial reasoning strategies. Objects appear isolated in space rather than grounded on dense surfaces.

Atmospheric interference affects both modalities but through different mechanisms. Fog and spray reduce camera contrast and blur object boundaries, degrading semantic feature extraction. These same conditions scatter LiDAR beams, reducing range and introducing false returns from suspended water droplets. The correlation between visibility degradation and detection performance differs across modalities—cameras may fail completely under dense fog while LiDAR maintains reduced-range functionality, or vice versa under extreme glare conditions.

Wave-induced platform motion introduces temporal alignment challenges absent from stable terrestrial platforms. A vessel moving at 5 meters per second experiences 0.5 meter displacement in 100 milliseconds—sufficient to create detection mismatches if sensors are not precisely synchronized. Pitch and roll motion apparent object movement even for stationary targets, complicating tracking and motion prediction. Motion compensation through IMU integration is necessary but introduces additional processing latency and computational cost.

These maritime-specific challenges have measurable performance impacts. Studies report 2-3 times higher false positive rates for camera-based detection during midday operation compared to overcast conditions \textcolor{red}{*}. LiDAR return rates over water surfaces drop to 10-30\% of the rates achieved on terrestrial surfaces \textcolor{red}{*}. Detection confidence scores decrease substantially under haze conditions even when objects remain geometrically visible. These quantitative degradations motivate the development of maritime-specific perception approaches rather than direct application of terrestrial methods.

The complementary failure modes of camera and LiDAR systems suggest potential benefits from sensor fusion. Cameras maintain detection capability for low-reflectance objects that produce weak LiDAR returns. LiDAR provides geometric precision under extreme lighting conditions where camera-based detection fails. This complementarity drives interest in multimodal fusion architectures that leverage each sensor's strengths while compensating for individual weaknesses.

\section{Constraints of Real-Time Maritime Deployment}

Autonomous surface vessels operate under constraints fundamentally different from ground vehicles or stationary installations. Power availability, thermal management, processing latency, and physical platform limitations collectively constrain perception system design in ways that favor particular architectural approaches over others.

Small autonomous vessels operate on battery power with mission durations measured in hours rather than days. Every watt consumed by perception systems reduces available endurance or payload capacity. Desktop-class GPUs consuming 200-400 watts during inference operation are incompatible with small platform power budgets. Embedded computing platforms such as the NVIDIA Jetson AGX Xavier provide GPU-accelerated inference at 30-watt power consumption—an order of magnitude reduction enabling extended autonomous operation. However, this efficiency comes at the cost of reduced computational throughput, limiting the complexity of algorithms that can execute in real-time.

Thermal management on maritime platforms presents unique challenges. Sealed enclosures protect electronics from salt spray and humidity but limit cooling airflow. High ambient temperatures during tropical operation reduce thermal headroom for computing systems. Passive cooling solutions are preferred to avoid mechanical failure modes of fans and pumps, but passive cooling capacity limits sustainable power dissipation. These thermal constraints favor algorithms with lower computational intensity and more consistent power consumption rather than approaches with high peak demands.

Processing latency directly affects navigation safety. Obstacle avoidance requires detection, classification, and path planning within the time available before potential collision. A vessel traveling at 5 meters per second requires detection and response within 10 seconds to avoid an obstacle 50 meters ahead—accounting for vehicle dynamics and maneuvering time. Perception latency consumes part of this budget, leaving less time for decision-making and control. Deterministic latency bounds become important; unpredictable processing delays can cause late detection that compromises safety margins.

Additionally, it is difficult to enforce strict rules in neural network-based systems, such as ensuring object separation distances, due to their non-deterministic nature. The lack of rule enforcement is particularly problematic with spatial data, which is often used for navigational purposes and obstacle avoidance. As such, the investigation of efficient deterministic approaches is still of high importance to situational awareness of uncrewed vehicles \textcolor{red}{*}.

Wave-induced platform motion introduces additional temporal alignment requirements. Sensors must be synchronized within tolerances that account for platform velocity and motion prediction accuracy. Poor temporal alignment creates spatial misalignment between modalities even when extrinsic calibration is perfect. The processing latency introduced by complex fusion algorithms can exceed acceptable synchronization windows, necessitating careful algorithm design to maintain alignment under motion.

Physical platform constraints affect sensor placement and field of view. Small vessels offer limited mounting locations with unobstructed views. Sensor positions high above the water reduce wave interference but create blind zones near the vessel. Multiple sensors improve coverage but increase power consumption, data bandwidth, and processing requirements. These trade-offs require careful system-level optimization balancing coverage, performance, and resource constraints.

These operational constraints favor particular architectural approaches for maritime perception. Algorithms must operate within embedded computing power budgets—typically 30-50 watts for the entire perception system including all sensors and processing. Latency must remain bounded and predictable to support safety-critical obstacle avoidance. Power consumption should be consistent rather than highly variable to simplify thermal management. Modularity is valuable—sensor failures should degrade performance gracefully rather than causing complete system failure.

Late fusion architectures align well with these constraints. Independent processing pipelines for each sensor modality enable parallel computation and load balancing. Computational requirements scale approximately linearly with the number of sensors rather than exponentially as with joint processing approaches. Individual sensor failures affect only specific detection streams without corrupting fusion outputs. Processing latency remains more predictable because each modality follows a fixed pipeline without complex cross-modal dependencies during feature extraction. These architectural properties make late fusion particularly attractive for resource-constrained maritime platforms despite potential accuracy trade-offs compared to deeper fusion strategies.

\section{Sensor Fusion Paradigms}

Because maritime datasets and calibrated multimodal sensor suites are limited, most existing fusion works adopt strategies originally proposed for road vehicles or general unmanned platforms. The following survey presents fusion paradigms as developed in those mature domains, with explicit identification of maritime-specific applications where they exist. The literature on multimodal fusion for maritime autonomous surface vessels is comparatively sparse. Unless stated otherwise, the fusion approaches discussed originate from automotive or general unmanned system research; maritime-specific work is explicitly identified where available.

Sensor fusion combines multiple sensing modalities to overcome individual limitations and improve perception robustness. Fusion strategies are categorized by the stage at which information from different sensors is combined, with each level offering distinct trade-offs between accuracy, robustness, and computational efficiency.

\subsection{Early Fusion: Data-Level Integration}

Early fusion combines raw sensor data before feature extraction, enabling joint learning from multimodal inputs. This approach projects data from different modalities into a common representation space where unified processing can exploit correlations between sensory channels. PointPainting exemplifies this strategy, projecting LiDAR points into camera image coordinates and assigning each 3D point the semantic label predicted by a 2D image segmentation network \textcolor{red}{*}. The semantically-labeled point cloud then serves as input to a 3D object detector, enabling the detector to leverage both geometric and semantic information simultaneously.

MV3D takes an alternative approach, converting LiDAR point clouds into multiple 2D representations—bird's-eye view and front view—then fusing these with camera images through convolutional feature extraction \textcolor{red}{*}. The different representations capture complementary geometric perspectives, with bird's-eye view preserving spatial relationships useful for localization and front view capturing object appearance details. These approaches demonstrate that early fusion can effectively combine geometric precision from LiDAR with semantic richness from cameras, validated primarily on automotive datasets including KITTI and KITTI-360 \textcolor{red}{*}.

Early fusion offers the potential for joint feature learning where networks can discover cross-modal correlations during training. The unified representation enables gradient flow across modalities during backpropagation, potentially learning features that exploit subtle relationships between geometric and photometric observations. This joint learning can improve performance when sufficient training data captures the correlation structure between modalities.

However, early fusion imposes stringent requirements on spatial and temporal calibration. Projection errors of even a few degrees in rotation or centimeters in translation misalign features between modalities, corrupting the joint representation. Temporal synchronization must be precise—misaligned timestamps between sensors create spatial inconsistencies that degrade fusion quality. These calibration requirements become particularly challenging on platforms subject to vibration or deformation, where maintaining calibration accuracy over time requires periodic recalibration procedures.

Sensor-specific noise and failures propagate directly into the fused representation in early fusion architectures. Corrupted camera images due to lens occlusion or extreme lighting produce degraded semantic labels that affect all projected LiDAR points. Sparse LiDAR returns or range measurement errors introduce geometric noise into the fused feature space. The tight coupling between modalities means that failure or degradation of one sensor can significantly compromise the entire perception system.

\subsection{Mid Fusion: Feature-Level Integration}

Mid fusion extracts features independently from each modality before combining them in intermediate network layers. This approach preserves the benefits of joint learning while providing some isolation between sensor-specific processing stages. AVOD generates region proposals from both LiDAR bird's-eye-view features and camera image features, then fuses these proposals through concatenation in a shared feature space for joint classification and localization refinement \textcolor{red}{*}. This architecture, evaluated on the KITTI dataset, demonstrates that independent feature extraction followed by fusion can achieve competitive accuracy while providing some robustness to individual sensor noise \textcolor{red}{*}.

DeepFusion employs cross-attention mechanisms to exchange information between image and LiDAR feature pyramids at multiple scales \textcolor{red}{*}. Rather than simple concatenation, attention modules learn to weight features from each modality based on their relevance and reliability for each spatial region. This learned weighting can adapt to sensor degradation, down-weighting unreliable features while emphasizing high-quality inputs. The approach has been validated on automotive benchmarks but requires substantial computational resources for the multi-scale attention operations \textcolor{red}{*}.

BEVFusion transforms camera features into a bird's-eye-view representation compatible with LiDAR BEV features, then combines them through efficient convolution operations in this unified spatial representation \textcolor{red}{*}. By establishing correspondence in BEV space rather than perspective image space, this method simplifies the geometric alignment problem and enables efficient fusion through standard convolutional operations. The approach achieves state-of-the-art performance on nuScenes and Waymo datasets, though computational requirements exceed embedded platform capabilities without optimization \textcolor{red}{*}.

Mid fusion balances joint learning with modular robustness. Independent feature extractors isolate sensor-specific noise, preventing corrupted raw data from one sensor from directly affecting the other modality's features. Fusion layers enable cross-modal context learning where networks can discover correlations between image features and LiDAR geometry. However, this architecture increases computational cost substantially—dual-pathway feature extractors and fusion layers approximately double the processing requirements compared to single-modality detection.

The feature alignment problem remains challenging in mid fusion. Correspondences between image features and 3D geometric features must be established either through explicit geometric projection or learned implicitly through network training. Geometric projection requires accurate calibration similar to early fusion. Learned correspondence requires sufficient training data spanning the range of spatial relationships between modalities, which may not be available for specialized domains like maritime environments.

Computational overhead limits mid fusion deployment on embedded platforms. Running dual feature extraction backbones simultaneously—one for images, one for point clouds—requires memory bandwidth and processing throughput that may exceed embedded GPU capabilities when real-time constraints are imposed. Power consumption scales with computational complexity, potentially exceeding the budgets of small autonomous platforms. These resource constraints motivate investigation of more computationally efficient fusion strategies.

\subsection{Late Fusion: Decision-Level Integration}

Late fusion operates on independent object detections from each modality, combining detection outputs through spatial association, probabilistic weighting, or learned scoring functions. This approach maintains separate detection pipelines for each sensor, then fuses their results at the decision level rather than during feature extraction or raw data processing. Detection outputs from camera-based and LiDAR-based networks are associated spatially, with overlapping or nearby detections combined through various fusion rules.

Simple late fusion approaches employ non-maximum suppression to remove duplicate detections, keeping the highest-confidence prediction when multiple detections overlap in space. Probabilistic methods model detection confidence as uncertainty distributions, combining detections through Bayesian inference or Dempster-Shafer evidence theory to compute fused confidence scores \textcolor{red}{*}. Learned scoring functions train classifiers to predict fused detection quality based on features of individual detections, such as confidence scores, bounding box overlap, and geometric consistency metrics \textcolor{red}{*}.

Late fusion's modular architecture provides several practical advantages for maritime deployment. Sensor failures affect only individual detection streams without corrupting the fusion process—if camera-based detection fails due to sun glare, LiDAR-based detection continues operating independently, and the fusion module simply receives detections from one modality. This graceful degradation maintains partial perception capability under adverse conditions. Computational load is distributed across independent pipelines, enabling parallel processing and load balancing across available hardware resources. Adding or removing sensor modalities requires minimal system redesign—new detection streams can be integrated by updating the fusion module without modifying existing sensor pipelines.

Processing latency remains more predictable in late fusion architectures because each modality follows a fixed pipeline without runtime dependencies on other sensors during feature extraction. Temporal synchronization requirements are relaxed compared to early fusion—spatial association can tolerate larger timing offsets because association operates on object-level regions rather than pixel-level correspondences. These properties make late fusion particularly suitable for platforms with limited computational resources and variable environmental conditions.

However, late fusion sacrifices the joint context learning enabled by early and mid fusion. Correlations between image features and LiDAR geometry must be inferred indirectly through spatial alignment rather than learned directly during feature extraction. The fusion module cannot leverage low-level multimodal patterns that might improve detection of ambiguous objects. Detection quality at the decision level determines fusion performance—poor individual detectors produce poor fusion results regardless of fusion algorithm sophistication.

Recent work has begun exploring hybrid approaches that blur distinctions between fusion levels. TransFusion employs transformer attention mechanisms that can be interpreted as either mid or late fusion depending on their position in the processing pipeline \textcolor{red}{*}. Object-centric attention modules attend to detection proposals rather than raw features, effectively performing late fusion within a learned framework. These hybrid methods, primarily evaluated on automotive datasets, demonstrate that fusion paradigms exist on a continuum rather than as discrete categories.

\subsection{Comparative Insights Across Fusion Paradigms}

Comparative studies provide insight into fusion paradigm trade-offs under various operational conditions, though most are evaluated on automotive rather than maritime datasets. Studies examining fusion under simulated sensor dropout found that late fusion maintained superior robustness when individual sensors failed or produced degraded outputs \textcolor{red}{*}. When synchronization error was systematically varied, late fusion tolerated temporal misalignment better than early fusion due to its spatial rather than pixel-level association \textcolor{red}{*}. However, under ideal conditions with clean data and perfect calibration, mid fusion achieved higher peak accuracy by learning cross-modal correlations unavailable to late fusion \textcolor{red}{*}.

These findings suggest that fusion strategy selection should be informed by operational requirements and constraints. Early and mid fusion optimize for accuracy under ideal conditions but require precise calibration, tight synchronization, and substantial computational resources. Late fusion prioritizes robustness and computational efficiency under degraded conditions at the cost of reduced peak accuracy. For maritime applications on embedded hardware where sensor degradation and computational constraints coexist, late fusion represents a pragmatic choice—preserving robustness and real-time performance while accepting potential accuracy trade-offs compared to deeper fusion strategies.

The scarcity of maritime-specific fusion research leaves open questions about whether these trade-offs hold in maritime environments. The relative importance of geometric versus semantic information may differ from automotive scenarios. The correlation structure between camera and LiDAR observations over water surfaces may not match terrestrial environments. Platform motion introduces temporal dynamics distinct from road vehicle operation. These uncertainties motivate empirical evaluation of fusion approaches specifically in maritime contexts rather than assuming direct transferability from automotive results.

\section{Temporal and Spatial Alignment in Fusion Systems}

Accurate fusion depends on establishing both spatial and temporal correspondence between sensors. Spatial calibration defines the rigid-body transformation relating sensor coordinate frames, while temporal synchronization ensures that observations represent the same moment in time. The precision requirements for calibration and synchronization vary with fusion paradigm—early fusion demands tighter tolerances than late fusion due to its pixel-level or point-level data alignment.

Extrinsic calibration establishes the geometric relationship between sensors, typically represented as a rotation matrix and translation vector that map coordinates from one sensor frame to another. Traditional calibration employs fiducial targets such as checkerboards or AprilTags visible to both sensors, solving for the transformation through least-squares optimization of corresponding point pairs \textcolor{red}{*}. Manual calibration remains the standard for many research systems despite being labor-intensive, as it provides the precision necessary for reliable geometric correspondence.

Automated calibration methods have emerged as alternatives. Approaches using deep learning predict calibration parameters directly from sensor data, eliminating the need for special targets \textcolor{red}{*}. Methods employing geometric consistency losses refine predictions by enforcing physical constraints on the transformation \textcolor{red}{*}. However, these automated approaches require sufficient scene structure and may fail in textureless maritime environments dominated by sky and water. The lack of distinct geometric features over water surfaces challenges feature-based calibration algorithms that rely on identifying corresponding points across modalities.

Calibration accuracy directly affects fusion quality. Rotation errors exceeding one degree or translation errors beyond 10 millimeters produce misalignments that degrade early and mid fusion performance significantly. These tolerances become more stringent as sensor separation increases—cameras and LiDAR units mounted meters apart exhibit larger projection errors for distant objects than closely-mounted sensors. Maritime platforms with limited mounting locations may require larger sensor baselines, amplifying the impact of calibration errors.

It is important to distinguish LiDAR-camera extrinsic calibration from the broader problem of IMU and GPS integration for state estimation. While LiDAR-camera calibration establishes geometric relationships between perception sensors enabling object detection fusion, IMU-GPS fusion provides global pose estimation for navigation and trajectory planning. These are complementary capabilities—extrinsic calibration enables perception-level fusion while state estimation enables ego-motion tracking. Both are necessary for complete autonomous systems, but this research focuses on perception-level sensor fusion rather than navigation state estimation.

Temporal synchronization ensures that sensor observations represent the same world state. Timing offsets create spatial misalignment even when geometric calibration is perfect, as objects move between sensor acquisition times. A vessel moving at 5 meters per second experiences 0.5 meter displacement in 100 milliseconds, potentially causing detection mismatches if timestamps are not aligned. High-precision synchronization protocols such as IEEE 1588 Precision Time Protocol achieve sub-microsecond accuracy when hardware timestamping is supported \textcolor{red}{*}. Network Time Protocol provides millisecond-level accuracy sufficient for many applications but may be inadequate for high-speed platforms or when precise spatial association is required \textcolor{red}{*}.

Temporal synchronization requirements vary with fusion paradigm and platform dynamics. Early fusion demands synchronization within approximately 10 milliseconds because pixel-level alignment is sensitive to motion-induced shifts. Late fusion tolerates larger timing offsets—50 to 100 milliseconds—because spatial association operates on object-level regions rather than pixel correspondences. Maritime platforms introduce additional complexity through wave-induced motion. Pitch and roll produce apparent object movement even when targets are stationary relative to the water surface, requiring motion compensation beyond simple timestamp alignment.

Motion compensation algorithms can partially address platform dynamics by predicting sensor pose at measurement time and correcting observations to a common reference frame. IMU integration provides high-rate attitude and angular velocity measurements enabling precise motion prediction. However, motion compensation introduces processing latency and computational overhead that must be considered when evaluating real-time feasibility. The latency of motion compensation itself can exceed synchronization tolerances if not carefully optimized, creating temporal misalignment despite the compensation attempt.

\section{Maritime Perception Datasets and Benchmarks}

Dataset availability fundamentally shapes algorithm development and validation. The automotive perception community benefits from large-scale benchmarks providing millions of labeled examples spanning diverse conditions. KITTI pioneered 3D object detection benchmarking for autonomous driving, providing synchronized camera and LiDAR data with precise annotations \textcolor{red}{*}. Waymo Open Dataset extends this foundation with orders of magnitude more data spanning varied geographic and weather conditions \textcolor{red}{*}. nuScenes adds comprehensive sensor suites including radar and multiple cameras with 360-degree coverage \textcolor{red}{*}. These benchmarks enable quantitative comparison across algorithms and have accelerated progress in terrestrial autonomous vehicle perception.

Maritime perception lacks comparable dataset resources. Most available maritime datasets provide only monocular RGB imagery without depth information or multimodal observations. Those including LiDAR often lack precise temporal synchronization or provide only sparse annotations covering limited object categories. Weather diversity remains underrepresented—most datasets capture clear or partly cloudy conditions with few examples of rain, fog, or heavy seas. Small object classes such as navigation buoys and floating debris receive minimal annotation coverage despite safety-critical importance for collision avoidance.

Several notable maritime datasets have been released in recent years, though each addresses only portions of the broader data gap. One dataset provides RGB imagery with GPS and IMU data from coastal and harbor scenes, including annotations for vessels, buoys, and docks, but lacks LiDAR data limiting its utility for multimodal fusion research \textcolor{red}{*}. A Singapore-based benchmark includes RGB, infrared, LiDAR, and radar modalities captured during multi-weather day and night conditions, though sparse ground truth and short capture sequences limit its scope for training deep networks (maritime) \textcolor{red}{*}. A hybrid simulation and real-world collection provides RGB, LiDAR, INS, and GPS with annotated trajectories and object classes, but synthetic domain bias and limited small-object labeling create sim-to-real transfer challenges \textcolor{red}{*}.

Real-world synchronized collections from research vessels demonstrate feasibility but remain limited in public availability. One effort collected HDR RGB and LiDAR data from a WAM-V platform during maritime robotics competitions, providing real-world synchronized observations under operational conditions, though limited public access and ongoing annotation restrict immediate utility (maritime) \textcolor{red}{*}. Another dataset provides stereo RGB and LiDAR for day and night maritime detection with diverse vessel types, but static sensor configuration and limited scene diversity reduce environmental coverage (maritime) \textcolor{red}{*}.

These data gaps limit maritime perception algorithm development and validation. Small datasets restrict the complexity of models that can be trained without overfitting. Limited weather coverage prevents robust evaluation under adverse conditions. Sparse annotations for safety-critical object categories leave uncertainty about detection reliability for important edge cases. The absence of standardized benchmarks makes quantitative comparison across methods difficult, slowing progress relative to domains with mature benchmark infrastructure.

This research contributes a synchronized LiDAR-camera dataset collected aboard the WAM-V research vessel during extended operational deployments. The dataset addresses several key gaps through HDR camera imagery capturing extreme dynamic range conditions, synchronized Livox Horizon LiDAR data providing precise geometric observations, diverse maritime object annotations including vessels, buoys, and navigation markers, varied weather and lighting conditions spanning multiple sea states and times of day, and platform motion data enabling motion compensation validation. Detailed dataset characteristics and collection methodology are described in Chapter~\ref{dataset}.

\section{Research Gaps and Dissertation Motivation}

Despite substantial progress in multimodal perception for autonomous systems, several critical gaps remain for maritime applications. These gaps span algorithmic approaches, computational implementation, and empirical validation resources. This dissertation addresses these deficiencies through systematic investigation of late fusion methods tailored to maritime operational constraints.

Most published fusion research emphasizes early or mid-level integration, with limited evaluation of decision-level approaches under maritime conditions. Comparative studies in automotive contexts demonstrate that late fusion offers superior robustness under sensor dropout or environmental degradation—conditions frequently encountered during maritime operations \textcolor{red}{*}. However, these findings derive primarily from terrestrial datasets where environmental characteristics and sensor failure modes differ substantially from maritime scenarios. Whether late fusion maintains similar robustness advantages under maritime-specific challenges including water reflections, HDR lighting extremes, and platform motion remains an open empirical question. This knowledge gap identified in Chapter~\ref{intro} motivates systematic evaluation of late fusion performance across representative maritime conditions. This gap is addressed in Chapter~\ref{methods} through development of multiple late fusion strategies and Chapter~\ref{results} through quantitative comparison against single-modality baselines.

Real-time fusion implementation on embedded maritime hardware remains underexplored. Early and mid fusion methods often exceed computational budgets of embedded platforms, requiring desktop-class GPUs for real-time operation. Typical automotive fusion networks require 200-400 watts of power and achieve 10-30 frames per second inference on high-end GPUs—specifications incompatible with small battery-powered autonomous vessels. Late fusion's reduced computational requirements suggest potential feasibility for embedded deployment, but systematic evaluation of accuracy-latency-power trade-offs on representative hardware is lacking. Power consumption is particularly critical for battery-operated platforms where sensor and computing loads directly affect mission endurance. This dissertation quantifies these trade-offs through detailed profiling on NVIDIA Jetson AGX Xavier hardware representing typical embedded computing capabilities for small maritime platforms, presented in Chapter~\ref{results}.

Few labeled multimodal maritime datasets exist with synchronized LiDAR and camera observations under diverse conditions. Existing datasets are either single-modality, limiting fusion research; lack precise synchronization, preventing temporal analysis; or provide limited weather and lighting diversity, restricting generalization assessment. This data gap directly impedes algorithm development and validation. This dissertation contributes a synchronized LiDAR-camera dataset collected during extended real-world operations aboard a WAM-V research vessel, described in Chapter~\ref{dataset}. The dataset includes diverse maritime object classes, HDR imaging capturing extreme dynamic range scenarios, varied weather conditions spanning multiple sea states, and platform motion data enabling motion compensation validation.

The relationship between calibration accuracy and fusion performance under maritime conditions requires empirical characterization. While automotive research has established general calibration tolerances, maritime platform dynamics and sensor mounting constraints may impose different requirements. Wave-induced motion creates time-varying calibration errors even when initial alignment is precise. Understanding acceptable calibration tolerances and their interaction with temporal synchronization accuracy is essential for practical system deployment. This research systematically evaluates fusion sensitivity to calibration and synchronization errors through controlled degradation experiments presented in Chapter~\ref{results}.

These gaps collectively motivate the following research questions addressed throughout this dissertation:

\textbf{RQ1:} How do LiDAR and camera-based object detection performance compare under representative maritime conditions including HDR lighting, water reflections, and platform motion? Performance comparison establishes baseline capabilities of each modality independently, identifying conditions under which each sensor maintains reliable operation and conditions causing degradation. These baselines enable quantitative assessment of fusion benefits.

\textbf{RQ2:} Can late fusion improve detection robustness compared to single-modality approaches without requiring computational overhead of early or mid fusion? Late fusion's architectural advantages for maritime deployment—modularity, computational efficiency, graceful degradation—are hypothesized to provide practical benefits for embedded platforms. Empirical validation across varied conditions quantifies whether these theoretical advantages translate to measurable performance improvements.

\textbf{RQ3:} What are practical constraints for implementing real-time late fusion on embedded maritime computing hardware in terms of latency, throughput, and power consumption? Understanding these constraints enables informed system design and establishes feasibility boundaries for fusion approaches. Detailed profiling identifies computational bottlenecks and optimization opportunities.

\textbf{RQ4:} How do spatial and temporal calibration accuracy affect late fusion performance, and what tolerances are necessary for reliable maritime operation? Calibration requirements determine operational procedures and system maintenance schedules. Excessive tolerance requirements may prove impractical for long-duration deployments, while overly relaxed requirements risk degraded performance. Empirical characterization establishes practical operating bounds.

This dissertation develops and validates a late-fusion framework for real-time LiDAR-camera perception on embedded maritime platforms. The approach is motivated by late fusion's advantages for maritime deployment including modular architecture that isolates sensor failures, computational efficiency compatible with embedded hardware constraints, and robustness to calibration and synchronization errors compared to tightly-coupled fusion approaches. Research methodology quantitatively evaluates accuracy, timing, and computational efficiency through controlled experiments and real-world validation aboard the WAM-V research platform. Results are analyzed across diverse maritime conditions to assess generalization and identify failure modes, providing actionable guidance for autonomous maritime system designers.

The following chapter describes the experimental methodology employed to address these research questions, including detailed hardware configuration, sensor calibration procedures, data collection protocols, and late fusion algorithm design. Subsequent chapters present empirical results, discuss implications for maritime perception system design, and identify directions for future research advancing autonomous surface vessel capabilities.

\end{document}
