\documentclass[../main.tex]{subfiles}
\graphicspath{{\subfix{../images/}}}
\begin{document}

Real-time perception for autonomous surface vessels has evolved substantially over the past decade, driven by advances in deep learning architectures and sensor technologies initially developed for terrestrial autonomous vehicles.
Vision-based detection methods, particularly single-stage architectures such as YOLO~\cite{ultralytics}, have achieved near-human accuracy on automotive benchmarks~\cite{geiger2012}.
In contrast, \ac{LiDAR}-based approaches offer geometric precision independent of illumination~\cite{zhou2018}.
Recent surveys of maritime autonomy~\cite{bae2023, zhang2021, xue2025, ferreira2022, bai2022} identify perception as a critical enabling technology, yet systematic performance characterization of these methods in maritime operational conditions remains limited.

The maritime environment presents distinct challenges compared to terrestrial scenarios.
Studies of visual detection in coastal and harbor settings report substantial performance degradation due to water reflections, high dynamic range, and low-texture backgrounds~\cite{prasad2017}.
\ac{LiDAR} systems face complementary difficulties with sparse water-surface returns and atmospheric scattering~\cite{kunz2005}.
While sensor fusion has been proposed to address individual modality limitations~\cite{huang2024a, liang2022}, most fusion research focuses on automotive applications using terrestrial datasets~\cite{geiger2012, tufekci2023}, and its direct applicability to maritime platforms operating under power and computational constraints has not been thoroughly validated~\cite{wang2020a}.

This review synthesizes research across vision-based detection, \ac{LiDAR}-based methods, and multimodal fusion to establish the foundation for maritime perception system evaluation.
The organizational structure reflects the progression from established autonomous vehicle research toward maritime-specific applications.
Early sections examine vision and \ac{LiDAR} detection methods independently, drawing primarily from automotive and general robotics literature where mature evaluation frameworks exist.
Subsequent sections address maritime-specific challenges, embedded platform constraints, and fusion architectures, where research remains fragmented and systematic evaluation is scarce.
The review concludes by synthesizing identified gaps into specific, experimentally addressable questions regarding detection performance, fusion benefits, and temporal synchronization requirements for maritime deployment.

\section{Modern Vision-Based Object Detection for Autonomy}

\subsection{Deep Learning Foundations and Single-Stage Detectors}

The application of convolutional neural networks to visual recognition tasks gained widespread adoption following Krizhevsky et al.'s demonstration that deep architectures trained on large-scale datasets could achieve superior performance on the ImageNet classification challenge~\cite{krizhevsky2017}. This breakthrough, achieved through a network with 60 million parameters and five convolutional layers, demonstrated that sufficient computational resources, combined with large annotated datasets, enable the learning of hierarchical feature representations that generalize across diverse object categories. Subsequent architectural innovations, including residual learning frameworks that facilitate training of substantially deeper networks~\cite{he2016}, further demonstrated that network depth directly correlates with representational capacity for complex visual recognition tasks.

Early object detection methods adapted these classification networks into two-stage pipelines. Girshick et al.~\cite{girshick2014} introduced R-CNN, which applies convolutional neural networks to region proposals generated through selective search, achieving substantial improvements over prior feature-based methods on the PASCAL VOC dataset. This approach established that high-capacity CNNs, combined with supervised pre-training on auxiliary tasks followed by domain-specific fine-tuning, yield significant performance gains when labeled training data is limited. However, the computational overhead of processing each region proposal independently through the full network limits throughput, with inference times measured in seconds per image rather than frames per second~\cite{girshick2014}. Ren et al.~\cite{ren2016} addressed these limitations with Faster R-CNN, introducing region proposal networks that enable end-to-end training and substantially improved inference speed while maintaining accuracy.

Single-stage detectors emerged to address latency constraints by reformulating detection as direct regression from image features to object classes and bounding box coordinates. The YOLO family exemplifies this paradigm, treating detection as a unified spatial prediction problem that eliminates the need for separate region proposal generation~\cite{prasad2017, zhang2021}. Subsequent YOLO iterations introduced architectural refinements, including multi-scale detection heads, anchor-free prediction mechanisms, and improved feature pyramid networks~\cite{kim2022}. These modifications enable real-time operation on GPU-accelerated platforms while maintaining competitive accuracy, with reported inference rates of 20-45 frames per second on embedded hardware depending on model scale and input resolution~\cite{guo2023}.

YOLOv8, released in 2023, introduced fundamental architectural innovations that address several limitations of earlier single-stage detectors~\cite{ultralytics}. The anchor-free detection strategy eliminates predefined bounding-box templates, instead employing dual detection heads that independently identify object presence and predict bounding box coordinates. This decoupled architecture simplifies transfer learning and enhances adaptability to objects with varying aspect ratios, thereby alleviating the intensive hyperparameter tuning burden typically required in anchor-based predecessors. The backbone incorporates Cross-Stage Partial Fusion (C2f) modules that enhance feature reuse and gradient flow while reducing computational overhead compared to earlier CSP designs. Modern loss functions, including Complete Intersection-over-Union (CIoU) and Distribution Focal Loss (DFL), improve localization precision and boundary regression stability, particularly benefiting overlapping objects and precise edge detection. However, the multi-scale detection architecture with feature-map strides of 8, 16, and 32 pixels imposes fundamental constraints on minimum detectable object size: objects smaller than approximately 16-32 pixels in width or height produce low-confidence or unstable detections due to progressive downsampling and limited spatial specificity in deeper network layers~\cite{rekavandi2022}. For maritime applications processing 640×640 pixel inputs, this corresponds to objects occupying less than 0.5-1\% of frame area—a relevant limitation for distant targets near the horizon. These architectural properties make YOLOv8 particularly suitable for real-time maritime perception, where computational efficiency is crucial. However, the small-object detection constraints motivate the investigation of resolution scaling strategies and complementary sensing modalities.

Transfer learning from large-scale terrestrial datasets has become standard practice for maritime object detection. Pre-training on ImageNet provides generalizable low-level and mid-level features~\cite{krizhevsky2017}, while the COCO dataset offers instance-level annotations across diverse scene compositions~\cite{lin2015, feng2021}. However, this transfer paradigm introduces challenges related to domain adaptation. Guo et al.~\cite{guo2023} investigated the transferability of YOLOv5 across maritime datasets with similar object classes but distinct environmental features, finding that zero-shot transfer yields poor performance; however, limited fine-tuning samples from the target domain substantially improve detection accuracy. This suggests that learned features encode assumptions about texture diversity, spatial layouts, and illumination conditions that fail to generalize when these distributional properties differ significantly between training and deployment environments~\cite{tzeng2017}.

\subsection{Maritime-Specific Detection Challenges}

Visual detection in maritime environments faces challenges that are fundamentally distinct from those in terrestrial scenarios, which dominate training datasets. Prasad et al.~\cite{prasad2017} provide a comprehensive survey of video processing for maritime object detection, identifying background motion from waves and wakes, low contrast objects, and environmental degradation as primary failure modes. Their evaluation on the Singapore Maritime Dataset revealed that conventional computer vision techniques demonstrate limited robustness under these conditions, motivating exploration of deep learning approaches specifically adapted for maritime applications.

High dynamic range presents a persistent challenge for maritime camera systems. Wang and Zhou~\cite{wang2019a} demonstrated in the context of automotive traffic light recognition that dual-channel imaging—processing both low-exposure and high-exposure frames—enables robust detection under extreme illumination variation; however, studies evaluating HDR imagery specifically for maritime object detection report mixed results. Landaeta~\cite{landaeta} and Liebergall et al.~\cite{liebergall} compared SDR and HDR image networks for maritime detection, finding that SDR-trained networks achieved 10\% higher recall and mAP scores despite HDR's theoretical advantages. This counterintuitive result was attributed to the better feature extraction for SDR images by pre-trained models, suggesting that the scarcity of HDR training data limits the effectiveness of transfer learning. Notably, HDR imagery provided qualitative benefits in extreme exposure cases where SDR images lost critical features entirely~\cite{landaeta}.

Atmospheric and environmental conditions impose additional constraints on visual maritime perception. Zhang et al.~\cite{zhang2022a} addressed object detection in foggy maritime environments, proposing preprocessing with Single Scale Retinex algorithms to reduce haze interference, combined with receptive field modifications and attention mechanisms to improve detection under degraded visibility. Their improved YOLOv4-tiny variant increased mean average precision from 79.56\% to 86.15\% on a fog-augmented maritime dataset, demonstrating that domain-specific preprocessing and architectural adaptations partially compensate for environmental degradation~\cite{zhang2022a}.

Small object detection at a distance remains a fundamental challenge. Rekavandi et al.~\cite{rekavandi2022} provide a comprehensive survey of small object detection methods, noting that objects occupying less than 10\% of image area yield insufficient information for reliable classification. Maritime scenarios exacerbate this difficulty: vessels and navigation markers often appear near the horizon, occupying a minimal pixel area while representing safety-critical detection targets. Shao et al.~\cite{shao2022} addressed this issue through multi-scale detection strategies, introducing deformable convolution modules and modified loss functions to enhance the accuracy of small objects under harsh maritime conditions, including waves, fog, and water reflections. Their VarifocalNet architecture outperformed contemporary methods (SSD, YOLOv3, RetinaNet) on small maritime targets, though at increased computational cost~\cite{shao2022}.

Survey papers by Zhang et al.~\cite{zhang2021} and Yang et al.~\cite{yang2024} emphasize that maritime object detection research remains fragmented, lacking standardized large-scale datasets and consistent evaluation protocols. Zhang et al.~\cite{zhang2021} argue that large-scale, multi-scenario industrialized neural network training is essential for practical maritime applications; however, they propose that widely accepted verification datasets analogous to KITTI or COCO for terrestrial scenarios have yet to emerge. This dataset scarcity constrains algorithm development and limits quantitative comparison across methods~\cite{su2023}.

\subsection{Synthesis: Gaps in Vision-Based Maritime Detection}

The reviewed literature reveals several key findings relevant to this dissertation's focus on real-time embedded maritime perception.
Model performance on small maritime objects degrades substantially at reduced input resolutions, yet native HDR resolution processing exceeds embedded GPU throughput constraints~\cite{shao2022, rekavandi2022}.
The optimal resolution-accuracy-latency trade-off for embedded maritime platforms remains unquantified, creating uncertainty about deployment configurations that balance detection capability with computational feasibility.
Furthermore, despite the theoretical advantages of HDR imaging in high-contrast maritime environments, a systematic evaluation of HDR preprocessing for maritime YOLO variants is lacking in the reviewed literature.
Available studies report contradictory findings regarding HDR benefits~\cite{landaeta, liebergall}, likely attributable to pre-training dataset biases where networks trained on SDR data fail to extract features from HDR inputs~\cite{krizhevsky2017} effectively.

While qualitative descriptions of YOLO model variants exist~\cite{kim2022, guo2023}, quantitative characterization of accuracy-latency-power consumption trade-offs on representative embedded hardware for maritime-specific object classes is missing.
Power consumption directly affects mission endurance for battery-operated vessels, yet few studies report energy efficiency metrics alongside detection accuracy.
Additionally, there is limited evidence regarding the fine-tuning data requirements for adapting terrestrial pre-trained models to maritime environments~\cite{guo2023}.
The relationship between training set size, environmental diversity, and generalization performance warrants a systematic investigation to establish practical guidelines for collecting maritime datasets and developing models.

These gaps motivate the systematic evaluation of YOLO variants under maritime-specific conditions, as presented in subsequent chapters, with a particular emphasis on the feasibility of embedded platforms and real-time performance constraints.

While vision-based detection methods face illumination-dependent challenges, geometric sensing modalities offer complementary capabilities.
The following section examines \ac{LiDAR}-based approaches that provide illumination-independent three-dimensional measurements, their architectural requirements, and maritime-specific performance characteristics.

\section{LiDAR-Based Object Detection for Autonomous Systems}

\subsection{Point Cloud Processing Architectures}

\acl{LiDAR} sensors provide three-dimensional geometric measurements, independent of illumination conditions, by emitting laser pulses and measuring the time-of-flight to determine range. These measurements generate point clouds representing surface geometry with millimeter-scale precision. Kunz et al.~\cite{kunz2005} demonstrated that \ac{LiDAR} reflections from small maritime targets produce significantly stronger returns than sea surface returns, with small buoys detectable at ranges exceeding 9 kilometers under adverse weather conditions. This illumination independence addresses a fundamental limitation of camera-based systems, which experience severe performance degradation under extreme lighting variations common in maritime environments.

Processing point cloud data for object detection presents unique computational challenges that differ from those of image-based approaches. Unlike images with a regular grid structure, enabling efficient convolutional operations, point clouds constitute unordered sets with non-uniform spatial distributions. Point density varies with range and surface orientation, and occlusion creates irregular gaps in object representations~\cite{zhou2018, shi2019}. These properties motivated the development of specialized network architectures explicitly designed for point cloud processing.

Point-based networks process raw three-dimensional coordinates directly without intermediate representations. Qi et al.~\cite{qi2017} introduced PointNet, which learns permutation-invariant features through symmetric aggregation functions, achieving real-time object recognition performance on three-dimensional datasets. Extensions that incorporate hierarchical processing through successive neighborhood grouping enable the extraction of multi-scale geometric features. However, computational complexity scales unfavorably with point cloud size, limiting practical application to long-range outdoor sensing where dense point clouds can contain millions of points per scan~\cite{shi2019}.

Voxelization methods address scalability by discretizing point clouds into regular three-dimensional grids, enabling application of standard convolutional architectures. Zhou and Tuzel~\cite{zhou2018} developed VoxelNet, which transforms sparse point clouds into dense volumetric representations through voxel feature encoding layers that learn unified feature representations from variable numbers of points per voxel. Yan et al.~\cite{yan2018} extended this with SECOND (Sparsely Embedded Convolutional Detection), introducing sparse convolutional operations that improve computational efficiency by processing only occupied voxels. Lang et al.~\cite{lang2019} proposed PointPillars, which encodes point clouds into vertical columns (pillars) rather than whole 3D voxels, enabling efficient 2D convolutional processing while preserving vertical geometric information. The fundamental trade-off inherent in voxelization is quantization error: small objects or fine geometric details may be lost during discretization, and voxel resolution selection creates competing demands for spatial precision versus computational efficiency~\cite{zhou2018}.

Hybrid architectures combine the strengths of point-based and voxel-based processing. Shi et al.~\cite{shi2019} proposed PointRCNN, which generates three-dimensional proposals directly from point clouds through bottom-up segmentation of foreground points, then refines proposals using canonical coordinate transformations that improve local spatial feature learning. Shi et al.~\cite{shi2020} extended this work with PV-RCNN, which integrates both point-based and voxel-based feature extraction in a two-stage architecture that achieves state-of-the-art accuracy on automotive benchmarks. These approaches outperform methods relying solely on dimensional reduction while maintaining computational efficiency through focused processing of proposal regions~\cite{shi2019}.

Large-scale automotive datasets have driven rapid progress in \ac{LiDAR}-based detection. The KITTI benchmark~\cite{geiger2012} established standardized evaluation protocols and provided synchronized camera-LiDAR data with precise annotations, enabling quantitative comparison across algorithms. Subsequent releases, including Waymo Open Dataset and the nuScenes expanded dataset, have expanded scale, sensor diversity, and geographic coverage~\cite{feng2021}. These resources accelerated algorithm development through the use of common evaluation frameworks and abundant training data. However, the environmental assumptions embedded in automotive datasets—dense ground plane returns, structured road environments, predictable motion patterns—limit direct transferability to maritime applications where these conditions do not hold.

Recent architectural innovations demonstrate continued advancement, but with substantial computational requirements. BEVFusion~\cite{liang2022, liu2023b} unifies camera and \ac{LiDAR} features in bird's-eye-view representations, achieving state-of-the-art accuracy on automotive benchmarks. However, Liu et al.~\cite{liu2023a} note that BEVFusion requires approximately 300 watts of power consumption and achieves only 15-20 frames per second on desktop-class GPUs. This power budget alone exceeds the total available power for many small autonomous vessels, rendering such approaches impractical without fundamental architectural modifications or acceptance of offline processing constraints~\cite{liang2022}.

Transformer-based architectures applying self-attention mechanisms directly to point clouds offer theoretical advantages for capturing long-range dependencies~\cite{vaswani2017}. However, these approaches typically require multi-GPU configurations or high-end accelerators (such as NVIDIA RTX 4090 or A100) with power consumption exceeding 200-400 watts during inference~\cite{liu2023a}. More critically, these architectures have not been validated under maritime-specific conditions, including sparse water surface returns, wave-induced platform motion, and variable atmospheric transmission~\cite{xie2024}.

Beyond computational constraints, learned approaches face fundamental limitations for safety-critical maritime applications.
It is difficult to enforce strict rules in neural networks, such as ensuring object separation distances, due to their non-deterministic nature.
The lack of rule enforcement is particularly problematic with spatial data, which is often used for navigational purposes and obstacle avoidance.
As such, the investigation of efficient deterministic approaches is still of high importance to situational awareness of uncrewed vehicles~\cite{coyleE}.

\subsection{Deterministic Geometric Methods}

While learned approaches dominate current \ac{LiDAR} detection research, deterministic geometric algorithms maintain advantages for resource-constrained embedded platforms where runtime predictability and minimal training data requirements are valued. Coyle~\cite{coyleE} developed Grid-Based Clustering and Concave Hull Extraction (GB-CACHE), a deterministic approach specifically designed for unmanned systems perceiving objects on flat surfaces including water. GB-CACHE efficiently segments point clouds through grid-based spatial partitioning, then extracts concave hull boundaries that accurately represent objects with irregular geometries. Computational efficiency analysis using dense \ac{LiDAR} point clouds demonstrates real-time feasibility on low to mid-grade computing platforms~\cite{coyleE}.

The deterministic nature of GB-CACHE provides specific operational advantages. Processing latency remains bounded and predictable, eliminating the variable inference times characteristic of neural networks, where batch size and scene complexity affect throughput. Rule enforcement for spatial constraints—critical for obstacle avoidance and navigation—can be rigorously guaranteed, whereas neural approaches provide probabilistic outputs without strict guarantees~\cite{coyleE}. Training data requirements are minimal; the algorithm requires only geometric parameters rather than extensive labeled datasets. For maritime deployment where training data scarcity persists, this represents a practical advantage despite potential accuracy trade-offs compared to learned methods trained on abundant data.

\subsection{Maritime-Specific LiDAR Challenges}

\ac{LiDAR} performance in maritime environments differs fundamentally from automotive scenarios due to water surface properties and atmospheric conditions. Automotive \ac{LiDAR} systems achieve near 100\% return rates on asphalt and concrete surfaces, providing dense point clouds with abundant ground plane context~\cite{roriz2022}. These high return rates depend on diffuse reflection from rough terrestrial surfaces. Water surfaces produce specular reflection, where incident laser energy reflects away from the sensor rather than back toward it. Kunz et al.~\cite{kunz2005} measured return rates over calm water as low as 10\% of the emitted pulses, noting that water surface characteristics produce significantly weaker returns than small floating targets, creating a favorable contrast for target detection despite a sparse environmental context.

The sparsity of water returns poses a challenge to detection algorithms designed for dense automotive point clouds. Automotive methods rely on abundant ground plane points to establish spatial reference frames and segment foreground objects from background~\cite{zhou2018, shi2019}. Maritime applications lack this dense reference, necessitating alternative spatial reasoning strategies when objects appear isolated without contextual grounding. Detection algorithms must distinguish between sparse target returns and sparse water returns based on geometric consistency and clustering properties, rather than relying on ground plane segmentation.

Atmospheric effects impose additional constraints. Fog, rain, and sea spray scatter laser pulses, reducing effective range and introducing false returns from suspended water droplets. Roriz et al.~\cite{roriz2022} quantify that visibility degradation from 20 kilometers in clear conditions to 2 kilometers in heavy haze corresponds to a 50-70\% reduction in effective \ac{LiDAR} range. While cameras experience similar degradation through contrast loss and blur, \ac{LiDAR} maintains geometric accuracy for returns that penetrate the atmosphere, providing complementary robustness under different environmental conditions~\cite{roriz2022}.

Platform motion introduces maritime-specific complexity absent from stable terrestrial platforms. Automotive vehicles operate on roads with predictable motion patterns and relatively stable sensor orientations. Maritime platforms experience continuous wave-induced pitch, roll, and heave motions that vary dynamically with the sea state. Xie et al.~\cite{xie2024} address this challenge through integration of IMU data with \ac{LiDAR} processing, demonstrating that motion compensation substantially improves detection accuracy in real-world maritime deployments. However, motion compensation introduces processing latency and computational overhead that must be considered when evaluating the feasibility of a real-time system. Ahmed et al.~\cite{ahmed2024} developed specialized Kalman filtering approaches tailored for maritime platform motion, reporting 25-30\% improvement in detection accuracy when compensating for tilt-induced point cloud distortions.

\subsection{Maritime LiDAR Detection: Current State and Gaps}

Recent work specifically addressing maritime \ac{LiDAR} detection has demonstrated growing research interest, yet it reveals substantial validation gaps. Xie et al.~\cite{xie2024} developed a modular framework that integrates state-of-the-art detection networks (PointPillars, SECOND, and PV-RCNN) for ship detection and tracking in busy maritime environments. Evaluation of real-world data collected along the River Thames demonstrates an overall detection accuracy of 74.1%, although the authors note performance degradation for vessels under 5 meters in length and at ranges exceeding 40 meters. Ahmed et al.~\cite{ahmed2024} focus specifically on dynamic object detection for unmanned surface vessels, integrating IMU-based motion compensation with \ac{LiDAR} processing, achieving 25-30\% improvement over baseline methods.

These maritime-specific studies reveal common limitations: datasets remain relatively minor compared to automotive benchmarks; weather diversity is limited; small object detection receives insufficient evaluation; and a systematic characterization of deterministic versus learned approaches under varying maritime conditions is lacking.
Detection methods designed for dense ground plane returns~\cite{zhou2018, shi2019} require fundamental adaptation for sparse maritime point clouds where objects lack grounding context~\cite{kunz2005}.
Automotive algorithms exploit abundant asphalt and concrete returns to establish spatial reference frames and segment foreground objects from background. Still, maritime applications lack this dense reference, requiring alternative spatial reasoning strategies where objects appear isolated.
A systematic evaluation of detection performance versus water surface characteristics—including return rate variations with wave conditions, sun angle, and water turbidity—is absent from the reviewed literature.

While multiple studies identify platform motion as challenging~\cite{xie2024, ahmed2024}, systematic evaluation of detection performance versus motion compensation quality and associated computational overhead remains unreported.
The relationship between IMU update rates, motion prediction accuracy, and resulting detection performance has not been quantitatively characterized across varying sea states.
Furthermore, GB-CACHE~\cite{coyleE} demonstrates the real-time feasibility and cross-domain applicability of deterministic geometric methods, yet maritime-specific validation under varying environmental conditions remains limited.
A comparative evaluation against learned approaches for maritime object classes would clarify the accuracy-simplicity-reliability trade-offs and inform algorithm selection for different operational scenarios.
Additionally, recent maritime studies~\cite{xie2024, ahmed2024} report detection accuracy but omit detailed latency breakdowns, power consumption measurements, and throughput characterization on representative embedded hardware.
Without this profiling data, it remains unclear which methods meet real-time constraints on battery-powered platforms with limited computational resources.

These gaps motivate systematic evaluation of deterministic \ac{LiDAR} detection under controlled maritime conditions with detailed performance profiling on embedded hardware. The complementary limitations of camera and \ac{LiDAR} modalities—vision failing under extreme lighting while LiDAR struggles with sparse returns—suggest potential benefits from sensor fusion, examined in the following sections after first establishing the practical constraints of maritime deployment.

% \section{Constraints of Real-Time Maritime Deployment}

% Autonomous surface vessels operate under constraints fundamentally different from ground vehicles or stationary installations. Understanding these constraints is essential before examining sensor fusion approaches, as power availability, thermal management, processing latency, and physical platform limitations collectively constrain perception system design in ways that favor particular architectural approaches.

% Small autonomous vessels operate on battery power with mission durations measured in hours rather than days.
% Every watt consumed by perception systems reduces available endurance or payload capacity.
% Desktop-class GPUs, which may consume 200-400 watts during inference operations, are incompatible with small platform power budgets.
% Embedded computing platforms, such as NVIDIA Jetson, can provide GPU-accelerated inference with a 30-watt power consumption—an order of magnitude reduction, enabling extended autonomous operation.
% However, this efficiency comes at the cost of reduced computational throughput, limiting the complexity of algorithms that can execute in real-time.

% Thermal management on maritime platforms presents unique challenges.
% Sealed enclosures protect electronics from salt spray and humidity but limit cooling airflow.
% High ambient temperatures during tropical operation reduce thermal headroom for computing systems.
% Passive cooling solutions are preferred to avoid mechanical failure modes of fans and pumps, but passive cooling capacity limits sustainable power dissipation.
% These thermal constraints favor algorithms with lower computational intensity and more consistent power consumption rather than approaches with high peak demands.

% Processing latency directly affects navigation safety.
% Obstacle avoidance requires detection, classification, and path planning within the time available before a potential collision~\cite{moe2017}.
% A vessel traveling at 5 meters per second requires detection and response within 10 seconds to avoid an obstacle 50 meters ahead—accounting for vehicle dynamics and maneuvering time. Perception latency consumes part of this budget, leaving less time for decision-making and control.
% Deterministic latency bounds become important; unpredictable processing delays can lead to late detection, compromising safety margins~\cite{bai2022}.

% Additionally, strict rule enforcement is challenging with neural network-based systems due to their non-deterministic nature.
% Rule enforcement is particularly problematic with spatial data, where ensuring object separation distances is crucial for obstacle avoidance and navigation purposes.
% As such, the investigation of efficient deterministic approaches remains vital for situational awareness of uncrewed vehicles~\cite{coyleE}.

% Wave-induced platform motion introduces additional temporal alignment requirements.
% Sensors must be synchronized within tolerances that account for platform velocity and motion prediction accuracy.
% Poor temporal alignment can create spatial misalignment between modalities, even when extrinsic calibration is perfect.
% Physical platform constraints affect sensor placement and field of view.
% Small vessels offer limited mounting locations with unobstructed views.
% Multiple sensors improve coverage but increase power consumption, data bandwidth, and processing requirements, necessitating careful system-level optimization.

% These operational constraints favor particular architectural approaches for maritime perception. Late fusion architectures align well with these constraints: independent processing pipelines for each sensor modality enable parallel computation and load balancing; computational requirements scale approximately linearly with the number of sensors rather than exponentially as with joint processing approaches; individual sensor failures affect only specific detection streams without corrupting fusion outputs; and processing latency remains more predictable because each modality follows a fixed pipeline without complex cross-modal dependencies during feature extraction. These architectural properties make late fusion particularly attractive for resource-constrained maritime platforms despite potential accuracy trade-offs compared to deeper fusion strategies. Understanding how fusion paradigms have been developed and evaluated—primarily in non-maritime domains—provides context for this dissertation's focus on late fusion for maritime applications.

\section{Sensor Fusion Paradigms}

Sensor fusion combines information from multiple sensing modalities to overcome the limitations of individual sensors and enhance perception robustness. Recent surveys~\cite{feng2021, tufekci2023, huang2024a} provide comprehensive overviews of deep multimodal perception for autonomous driving, identifying the fusion level—early, mid, or late—as the primary architectural distinction that determines computational requirements, calibration sensitivity, and failure mode propagation.
Each fusion paradigm offers distinct trade-offs between peak accuracy potential, robustness to sensor degradation, and real-time implementation feasibility~\cite{feng2021}.

The fundamental challenge in multimodal fusion is determining "what to fuse, when to fuse, and how to fuse"~\cite{feng2021}. These decisions depend critically on application requirements, available computational resources, and expected operational conditions. For maritime autonomous surface vessels operating on battery power with limited computing capacity, these architectural choices have a direct impact on system viability. Understanding the fusion paradigm trade-offs established in automotive research provides context for evaluating maritime-specific fusion approaches, although direct transferability assumptions require empirical validation due to environmental differences.

\subsection{Early Fusion: Data-Level Integration}

Early fusion combines raw sensor data before feature extraction, projecting measurements from different modalities into common representations where unified processing exploits cross-modal correlations. Cui et al.~\cite{cui2022} provide an extensive review of camera-\ac{LiDAR} fusion approaches, noting that early fusion enables networks to learn joint features during training but imposes stringent calibration and synchronization requirements. PointPainting exemplifies this paradigm by projecting \ac{LiDAR} points into camera image coordinates, assigning semantic labels from two-dimensional segmentation networks to three-dimensional points, then processing the semantically enriched point cloud through three-dimensional object detectors~\cite{cui2022}.

Chen et al.~\cite{chen2017} developed Multi-View 3D (MV3D), which converts \ac{LiDAR} point clouds into multiple two-dimensional representations—bird's-eye view and front view—then fuses these with camera images through convolutional feature extraction. Evaluation on KITTI demonstrates that multi-view representations capture complementary geometric perspectives: the bird's-eye view preserves spatial relationships useful for localization, while the front view captures appearance details facilitating classification~\cite{chen2017}. These early fusion approaches validated on automotive datasets achieve high accuracy when calibration is precise and sensors remain synchronized.

However, early fusion's tight coupling between modalities creates practical deployment challenges. Projection errors of even a few degrees of rotation or centimeters of translation misalign features, corrupting the joint representation~\cite{cui2022}. Temporal synchronization must be precise; misaligned timestamps create spatial inconsistencies, degrading fusion quality. Sensor-specific noise and failures propagate directly: corrupted camera images from lens occlusion or extreme lighting produce degraded semantic labels affecting all projected \ac{LiDAR} points~\cite{cui2022}. For maritime platforms experiencing continuous wave-induced motion and intermittent sun glare, the calibration sensitivity and failure propagation of early fusion represent significant operational risks.

\subsection{Mid Fusion: Feature-Level Integration}

Mid fusion extracts features independently from each modality before combining them in intermediate network layers, preserving joint learning benefits while providing sensor-specific processing isolation. Huang et al.~\cite{huang2024a} systematically compare fusion paradigms, finding that mid fusion balances accuracy and robustness: independent feature extraction prevents raw sensor corruption from propagating between modalities, yet learned fusion layers capture cross-modal correlations unavailable to late fusion.

Cui et al.~\cite{cui2022} review feature-level fusion methods, including DeepFusion, which employs cross-attention mechanisms to exchange information between image and \ac{LiDAR} feature pyramids at multiple scales. Attention modules learn to weight features from each modality based on relevance and reliability for each spatial region, adapting to sensor degradation by down-weighting unreliable features. Evaluation on automotive benchmarks demonstrates improved robustness compared to early fusion, though computational requirements for multi-scale attention operations substantially exceed single-modality detection~\cite{cui2022}.

BEVFusion~\cite{liang2022, liu2023b} transforms camera features into bird's-eye-view representations compatible with \ac{LiDAR} BEV features, combining them through efficient convolution operations. By establishing correspondence in BEV space rather than perspective image space, this approach simplifies geometric alignment and enables efficient fusion through standard operations. Liu et al.~\cite{liu2023a} note that BEVFusion achieves state-of-the-art performance on nuScenes and Waymo datasets but requires approximately 300 watts of power consumption, exceeding embedded platform capabilities by an order of magnitude. For battery-powered maritime platforms where 30-50 watts represents the typical total system power budget, mid-fusion's computational demands preclude direct deployment without substantial architectural optimization.

\subsection{Late Fusion: Decision-Level Integration}

Late fusion operates on independent object detections from each modality, combining detection outputs through spatial association, probabilistic weighting, or learned scoring functions. This approach maintains modular detection pipelines, fusing results at the decision level after individual sensor processing is complete~\cite {wang2020a, pang2020}. While sacrificing the joint feature learning enabled by earlier fusion, late fusion offers architectural properties that are advantageous for resource-constrained maritime deployments.

Wang et al.~\cite{wang2020a} survey multi-sensor fusion in automated driving, emphasizing that late fusion enables graceful degradation: sensor failures affect only individual detection streams without corrupting fusion outputs. Pang et al.~\cite{pang2020} developed CLOCs (Camera-LiDAR Object Candidates), which fuse detection candidates before non-maximum suppression, training networks to predict fused detection quality based on geometric and semantic consistency between modalities. Evaluation on KITTI demonstrates substantial improvements, especially at long range, where individual detectors exhibit reduced confidence but fusion resolves ambiguities through cross-modal validation~\cite{pang2020}. Xu et al.~\cite{xu2023} extended these concepts with FusionRCNN, demonstrating that two-stage fusion architectures can achieve competitive accuracy while maintaining modularity.

Late fusion's modular architecture provides several practical advantages for maritime deployment. Computational requirements scale approximately linearly with sensor count rather than exponentially as with joint processing~\cite{wang2020a}. Processing latency remains predictable because each modality follows fixed pipelines without runtime cross-modal dependencies during feature extraction. Adding or removing sensors requires minimal system redesign—new detection streams integrate through updated fusion modules without modifying existing pipelines~\cite{feng2021}. Most critically for maritime applications, individual sensor failures degrade performance gracefully rather than causing catastrophic fusion failure; for instance, camera-based detection failing due to sun glare leaves \ac{LiDAR}-based detection operational~\cite{huang2024a}.

Temporal synchronization requirements are relaxed compared to early fusion. Late fusion spatial association operates on object-level regions rather than pixel-level correspondences, tolerating larger timing offsets~\cite{huang2024a}. For maritime platforms where precise hardware synchronization may be impractical and wave-induced motion creates apparent object movement, relaxed timing tolerances represent operational advantages. However, late fusion sacrifices joint context learning: correlations between image features and \ac{LiDAR} geometry must be inferred indirectly through spatial alignment rather than learned directly during feature extraction~\cite{cui2022}.

% \subsection{Maritime-Specific Fusion Applications}

% Limited research addresses sensor fusion specifically for maritime autonomous surface vessels, revealing substantial validation gaps. Helgesen et al.~\cite{helgesen2019} evaluate sensor combinations for maritime target tracking, integrating \ac{LiDAR}, radar, electro-optical, and infrared cameras through Joint Integrated Probabilistic Data Association (JIPDA). Testing with GPS-equipped reference targets reveals that passive sensors (cameras) help resolve merged measurement issues in radar tracking, and that radar versus \ac{LiDAR} choice involves trade-offs between fast track initiation and reduced false tracks~\cite{helgesen2019}. However, evaluation focuses on tracking performance rather than detection accuracy, and embedded platform implementation feasibility is not addressed.

% Farahnakian and Heikkonen~\cite{farahnakian2020} compare pixel-level, feature-level, and decision-level fusion architectures for maritime vessel detection using visible and infrared cameras. Evaluation on data collected in Finnish archipelago under various climatic conditions demonstrates that feature-level fusion outperforms other paradigms. However, the study excludes \ac{LiDAR}, limiting applicability to geometric-semantic fusion scenarios. Haghbayan et al.~\cite{haghbayan2018} specifically address maritime multi-sensor fusion, combining radar, \ac{LiDAR}, RGB camera, and infrared camera through probabilistic data association. Their approach generates region proposals from fused detections, then applies convolutional neural networks for object classification within proposed regions. Evaluation on ferry driving scenarios demonstrates reliable detection and classification, though specific performance metrics and computational requirements on embedded hardware are not detailed~\cite{haghbayan2018}.

% Clunie et al.~\cite{Clunie2021} developed an open-source perception system for autonomous surface vessels integrating marine radar, \ac{LiDAR}, and camera. The system identifies obstacles from input sensors, estimates their state, and fuses obstacle data into consolidated reports. Validation demonstrates detection and tracking capabilities to 450 meters at 7 frames per second processing rate. Thompson~\cite{thompson2023} contributed neural network fusion approaches for multimodal sensor data on autonomous surface vessels, providing both methodological advances and dataset contributions. Subedi et al.~\cite{subedi2020} demonstrate camera-LiDAR fusion for autonomous mooring operations, showing feasibility for close-range precision tasks in maritime contexts. Ma et al.~\cite{ma2024} recently proposed multimodal information fusion frameworks for LiDAR-based 3D detection targeted at maritime applications.

% These maritime-specific studies collectively demonstrate fusion feasibility yet reveal critical gaps: systematic evaluation of fusion paradigm trade-offs under maritime conditions is absent; embedded platform computational requirements are incompletely characterized; and comparative analysis quantifying fusion benefits versus single-modality baselines across diverse environmental conditions remains limited.

\subsection{Comparative Fusion Paradigm Analysis}

Huang et al.~\cite{huang2024a} systematically evaluate fusion paradigms under simulated sensor degradation and synchronization error, providing quantitative evidence for architectural trade-off claims. Late fusion maintains superior robustness when individual sensors fail or produce degraded outputs: graceful degradation preserves partial perception capability when single modalities experience environmental interference. When synchronization errors vary systematically, late fusion tolerates temporal misalignment better than early fusion due to spatial rather than pixel-level association operating on object-scale regions, which are regions less sensitive to small timing offsets~\cite{huang2024a}.

However, under ideal conditions with clean data and perfect calibration, mid fusion achieves higher peak accuracy by learning cross-modal correlations unavailable to late fusion~\cite{huang2024a}. This accuracy advantage comes at computational cost: Wang et al.~\cite{wang2020a} note that mid-fusion architectures approximately double processing requirements compared to single-modality detection through dual-pathway feature extraction and fusion layers. For automotive applications with abundant power budgets, this represents an acceptable trade-off. For battery-powered maritime platforms, doubled power consumption directly reduces mission endurance, potentially rendering mid-fusion impractical despite accuracy advantages.

These findings from automotive research suggest that operational requirements and constraints should inform the selection of a fusion strategy. Early and mid fusion optimize for peak accuracy under ideal conditions but require precise calibration, tight synchronization, and substantial computational resources~\cite{feng2021, cui2022}. Late fusion prioritizes robustness and computational efficiency under degraded conditions, accepting reduced peak accuracy compared to deeper fusion strategies~\cite{huang2024a, wang2020a}. For maritime applications on embedded hardware where sensor degradation and computational constraints coexist, late fusion represents a pragmatic architecture balancing performance, robustness, and feasibility.

\subsection{Synthesis: Fusion Gaps for Maritime Applications}

The reviewed fusion literature, although extensive for automotive applications, reveals substantial gaps for the deployment of maritime autonomous surface vessels.
Automotive fusion methods~\cite{feng2021, cui2022, huang2024a} evaluate performance on terrestrial datasets where environmental characteristics differ fundamentally from maritime scenarios.
Water reflections, extreme dynamic range, sparse \ac{LiDAR} returns, and platform motion represent maritime-specific conditions requiring empirical validation rather than assumed transferability.
The environmental assumptions embedded in automotive benchmarks—stable sensor mounting, predictable lighting variation within terrestrial ranges, dense ground returns—do not hold for maritime platforms operating in unstructured open-water environments.

Computational requirements, power consumption, and latency characteristics on representative embedded hardware are rarely reported~\cite{liu2023a}.
Understanding feasibility boundaries for real-time maritime deployment requires detailed profiling across fusion paradigms and implementation variants.
While automotive fusion research often assumes the availability of desktop-class GPUs, maritime platforms operate under strict power budgets, where every watt of processing directly reduces mission endurance or payload capacity.
Furthermore, while Huang et al.~\cite{huang2024a} demonstrate that late fusion tolerates synchronization errors better than early fusion, a systematic evaluation sweeping timing offsets to establish acceptable tolerances for maritime platforms is lacking.
The interaction between temporal misalignment and wave-induced motion—which creates apparent object movement even when sensors are perfectly synchronized—has not been characterized.

Automotive studies establish general calibration requirements~\cite{cui2022}, but maritime platform dynamics may impose different tolerances.
Wave-induced motion creates time-varying calibration errors that differ from the static calibration errors typically found in ground vehicles; the acceptable calibration accuracy and repeatability requirements for operational deployment need empirical determination.
Additionally, most fusion studies~\cite{farahnakian2020, haghbayan2018, Clunie2021} report fusion performance without rigorous comparison against optimized single-modality baselines across diverse conditions.
Quantifying when and where fusion provides measurable benefits versus added complexity is essential for informed system design decisions, particularly when evaluating whether fusion improvements justify the additional costs of sensors, processing requirements, calibration maintenance, and system complexity.

These gaps motivate this dissertation's systematic investigation of late fusion for maritime applications, with a particular emphasis on the feasibility of embedded platforms, temporal synchronization requirements, and a quantitative performance comparison against single-modality baselines.

\section{Temporal and Spatial Alignment in Fusion Systems}

Accurate fusion depends on establishing both spatial and temporal correspondence between sensors.
Spatial calibration defines the rigid-body transformation relating sensor coordinate frames, while temporal synchronization ensures that observations represent the exact moment in time.
The precision requirements for calibration and synchronization vary with fusion paradigm—early fusion demands tighter tolerances than late fusion due to its pixel-level or point-level data alignment.

Extrinsic calibration establishes the geometric relationship between sensors, typically represented as a rotation matrix and translation vector that maps coordinates from one sensor frame to another.
Traditional calibration employs fiducial targets, such as checkerboards or AprilTags, visible to both sensors, solving for the transformation through least-squares optimization.
Manual calibration remains the standard for many research systems despite being labor-intensive, as it provides the precision necessary for reliable geometric correspondence.

Automated calibration methods have emerged as alternatives.
Iyer et al.~\cite{iyer2018} developed CalibNet, which uses deep learning to predict calibration parameters directly from sensor data, thereby eliminating the need for special targets.
Methods employing geometric consistency losses refine predictions by enforcing physical constraints on the transformation~\cite{yuan2020, shi2020}.
Xiao et al.~\cite{xiao2024} proposed CalibFormer, a transformer-based automatic LiDAR-camera calibration network demonstrating improved robustness.
Schneider et al.~\cite{schneider2017} introduced RegNet for multimodal sensor registration using deep neural networks.
Wu et al.~\cite{wu2021} developed sensor auto-calibration approaches based on deep learning specifically for self-driving cars.
However, these automated approaches require sufficient scene structure and may fail in textureless maritime environments, which are often dominated by sky and water.

Calibration accuracy directly affects fusion quality.
Rotation errors exceeding one degree or translation errors exceeding 10 millimeters produce misalignments that significantly degrade early and mid-fusion performance.
These tolerances become more stringent as sensor separation increases—cameras and \ac{LiDAR} units mounted meters apart exhibit larger projection errors for distant objects than those mounted closely together.

Temporal synchronization ensures that sensor observations represent the same state of the world~\cite{westenberger2011}.
Timing offsets can create spatial misalignment, even when geometric calibration is perfect, due to the relative motion between the sensor and the object.
A vessel moving at 5 meters per second experiences a 0.5-meter displacement in 100 milliseconds, potentially causing detection mismatches if timestamps are not properly aligned.
High-precision synchronization protocols such as IEEE 1588 Precision Time Protocol achieve sub-microsecond accuracy when hardware timestamping is supported~\cite{ptp2008}.
Network Time Protocol (NTP) provides millisecond-level accuracy, which is sufficient for many applications, but may be inadequate for high-speed platforms or when precise spatial association is required~\cite{furgale2013, liu2021}.

Temporal synchronization requirements vary depending on the fusion paradigm and the dynamics of the platform.
Early fusion requires synchronization within approximately 10 milliseconds, as pixel-level alignment is sensitive to motion-induced shifts.
Late fusion tolerates larger timing offsets—50 to 100 milliseconds—because spatial association operates at the object-level region rather than pixel correspondences.
Maritime platforms introduce additional complexity through wave-induced motion. Pitch and roll produce apparent object movement even when targets are stationary relative to the water surface, requiring motion compensation beyond simple timestamp alignment.

Motion compensation algorithms can partially address platform dynamics by predicting sensor pose at measurement time and correcting observations to a common reference frame.
IMU integration provides high-rate attitude and angular velocity measurements, enabling precise motion prediction. However, motion compensation introduces processing latency and computational overhead that must be considered when evaluating real-time feasibility.

The calibration and synchronization methods reviewed above require validation data to establish achievable accuracy bounds and quantify the sensitivity of fusion performance to alignment errors.
Furthermore, training data-driven detection algorithms and evaluating fusion approaches require labeled multimodal datasets that capture diverse operational conditions.
The availability and characteristics of such datasets fundamentally constrain maritime perception research.

\section{Maritime Perception Datasets and Benchmarks}

Dataset availability fundamentally shapes algorithm development and validation.
The automotive perception community benefits from large-scale benchmarks providing millions of labeled examples spanning diverse conditions.
KITTI pioneered three-dimensional object detection benchmarking for autonomous driving, providing synchronized camera and \ac{LiDAR} data with precise annotations~\cite{geiger2012}.
Waymo Open Dataset extends this foundation with orders of magnitude more data spanning varied geographic and weather conditions~\cite{su2023}.
nuScenes adds comprehensive sensor suites including radar and multiple cameras with 360-degree coverage~\cite{feng2021}.
These benchmarks enable quantitative comparison across algorithms and have accelerated progress in terrestrial autonomous vehicle perception.

Maritime perception lacks comparable dataset resources.
Most available maritime datasets provide only monocular RGB imagery without depth information or multimodal observations.
Those including \ac{LiDAR} often lack precise temporal synchronization or provide only sparse annotations covering limited object categories.
Weather diversity remains underrepresented—most datasets capture clear or partly cloudy conditions with few examples of rain, fog, or heavy seas.

Su et al.~\cite{su2023} provide a comprehensive survey of maritime vision datasets, documenting the landscape of available resources and identifying critical gaps. Several notable maritime datasets have been released in recent years, though each addresses only a portion of the broader data gap.
Kim et al.~\cite{kim2022} improved and extended the Singapore Maritime Dataset, providing RGB, infrared, \ac{LiDAR}, and radar modalities captured during multi-weather day and night conditions. However, sparse ground truth and short capture sequences limit its scope for training deep networks.
Huang et al.~\cite{huang2025} contributed a hybrid simulation and real-world dataset that provides RGB, \ac{LiDAR}, INS, and GPS data with annotated trajectories and object classes. However, synthetic domain bias and limited labeling of small objects create sim-to-real transfer challenges.

Real-world, synchronized collections from research vessels demonstrate feasibility but remain limited in public availability.
Thompson~\cite{thompson2023} collected data for neural network fusion approaches on autonomous surface vessels, providing valuable contributions to maritime-specific datasets. Limited public access and ongoing annotation restrict immediate utility for the broader research community.

These data gaps limit the development and validation of maritime perception algorithms.
Small datasets restrict the complexity of models that can be trained without overfitting.
Limited weather coverage prevents robust evaluation under adverse conditions.
Sparse annotations for safety-critical object categories leave uncertainty about the reliability of detection for essential edge cases.
The absence of standardized benchmarks makes quantitative comparison across methods difficult, slowing progress relative to domains with mature benchmark infrastructure.

This research contributes a synchronized \ac{LiDAR}-camera dataset collected aboard the WAM-V research vessel during extended operational deployments.
The dataset addresses several key gaps through HDR camera imagery capturing extreme dynamic range conditions, synchronized Livox Horizon \ac{LiDAR} data providing precise geometric observations, diverse maritime object annotations including vessels, buoys, and navigation markers, varied weather and lighting conditions spanning multiple sea states and times of day, and platform motion data enabling motion compensation validation.
Detailed dataset characteristics and collection methodology are described in Chapter~\ref{sec:sensor_data_dataset}.

Having surveyed vision-based detection, \ac{LiDAR}-based approaches, fusion paradigms, calibration requirements, and available datasets, it becomes evident that maritime perception research faces interconnected technical and empirical challenges.
The following synthesis consolidates identified gaps into specific research questions that motivate this dissertation's experimental approach and contributions.

\section{Research Gaps and Dissertation Motivation}

Despite substantial progress in multimodal perception for autonomous systems, several critical gaps remain for maritime applications.
These gaps span algorithmic approaches, computational implementation, and empirical validation resources.
This dissertation addresses these deficiencies through a systematic investigation of late fusion methods tailored to maritime operational constraints.

Most published fusion research emphasizes early or mid-level integration, with limited evaluation of decision-level approaches under maritime conditions.
Comparative studies in automotive contexts demonstrate that late fusion offers superior robustness under sensor dropout or environmental degradation—conditions frequently encountered during maritime operations~\cite{huang2024a, wang2020a}.
However, these findings primarily derive from terrestrial datasets, where environmental characteristics and sensor failure modes differ substantially from those in maritime scenarios.
Whether late fusion maintains similar robustness advantages under maritime-specific challenges, including water reflections, HDR lighting extremes, and platform motion, remains an open empirical question.
This gap is addressed through the development of a late fusion strategy in Chapter~\ref{realtime_object_detection} and quantitative comparison against single-modality baselines.

Real-time fusion implementation on embedded maritime hardware remains underexplored.
Early and mid-fusion methods often exceed the computational budgets of embedded platforms, requiring desktop-class GPUs for real-time operation.
Typical automotive fusion networks require 200-400 watts of power and achieve 10-30 frames per second inference on high-end GPUs—specifications incompatible with small battery-powered autonomous vessels.
Late fusion's reduced computational requirements suggest potential feasibility for embedded deployment; however, systematic evaluation of accuracy-latency-power trade-offs on representative hardware is lacking.
Power consumption is particularly critical for battery-operated platforms where sensor and computing loads directly affect mission endurance.
This dissertation quantifies these trade-offs through detailed profiling on NVIDIA Jetson AGX Xavier hardware, which represents typical embedded computing capabilities for small maritime platforms, as presented in Chapter~\ref{chap:recommendations}.

Few labeled multimodal maritime datasets exist with synchronized \ac{LiDAR} and camera observations under diverse conditions.
Existing datasets are either single-modality, limiting fusion research; lack precise synchronization, preventing temporal analysis; or provide limited weather and lighting diversity, restricting generalization assessment.
This data gap directly impedes algorithm development and validation.
This dissertation contributes a synchronized \ac{LiDAR}-camera dataset collected during extended real-world operations aboard a WAM-V research vessel.
The dataset encompasses diverse maritime object classes, HDR imaging that captures extreme dynamic range scenarios, varied weather conditions spanning multiple sea states, and platform motion data enabling motion compensation validation.

The relationship between calibration accuracy and fusion performance under maritime conditions requires empirical characterization.
While automotive research has established general calibration tolerances, maritime platform dynamics and sensor mounting constraints may impose different requirements.
Wave-induced motion can create time-varying calibration errors, even when initial alignment is precise.
Understanding acceptable calibration tolerances and their interaction with temporal synchronization accuracy is crucial for the practical deployment of systems.
This research systematically evaluates fusion sensitivity to calibration and synchronization errors through controlled degradation experiments presented in Chapter~\ref{chap:recommendations}.

This dissertation develops and validates a late-fusion framework for real-time \ac{LiDAR}-camera perception on embedded maritime platforms.
The approach is motivated by the advantages of late fusion for maritime deployment: a modular architecture that isolates sensor failures, computational efficiency compatible with embedded hardware constraints, and robustness to calibration and synchronization errors compared to tightly coupled fusion approaches.
Research methodology quantitatively evaluates accuracy, timing, and computational efficiency through controlled experiments and real-world validation aboard the WAM-V research platform.
Results are analyzed across diverse maritime conditions to assess generalization and identify failure modes, providing actionable guidance for autonomous maritime system designers.

The following chapter describes the experimental methodology employed to address these research questions, including detailed hardware configuration, sensor calibration procedures, data collection protocols, and late fusion algorithm design.
Subsequent chapters present empirical results, discuss implications for maritime perception system design, and identify directions for future research advancing autonomous surface vessel capabilities.

\end{document}
