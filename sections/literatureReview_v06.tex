\documentclass[../main.tex]{subfiles}
\graphicspath{{\subfix{../images/}}}
\begin{document}

\section{Modern Vision-Based Object Detection for Autonomy}
\subsection{Vision-based object detection}
Deep convolutional neural networks (CNNs) established the modern baseline for visual object detection by learning hierarchical feature representations from raw pixels.
Early work adapted image-classification CNNs into detectors by combining feature extraction with region proposal mechanisms.  Two-stage detectors in the R-CNN family formalized that strategy: generate candidate regions, then apply a classifier and bounding-box regressor to each proposal [5].  Two-stage pipelines remain attractive for accuracy but incur runtime overhead from proposal generation and per-proposal processing.

Single-stage detectors reframed detection as a direct, end-to-end regression from image features to object classes and box coordinates.  Early single-stage methods prioritized throughput over peak accuracy and enabled real-time inference on GPU platforms.
The YOLO family exemplifies this approach by predicting object locations and classes in a single forward pass [6].
Subsequent YOLO revisions improved backbone efficiency, introduced multi-scale detection heads, and refined loss and label-assignment strategies to boost accuracy without abandoning the single-pass design [6].  
Recent YOLO variants move toward anchor-free heads and multi-head outputs.
Anchor-free heads predict object centers and scalar size parameters directly instead of regressing offsets relative to predefined anchor boxes.  This removes a set of hyperparameters, simplifies training, and can improve localization for objects with varied aspect ratios and scales.

Model families such as EfficientDet introduced compound scaling to jointly balance network depth, width, and input resolution for a given compute budget [8].  Transformer-based detectors (for example DETR and its derivatives) recast detection as a set-prediction problem solved by attention mechanisms [9].  Attention models can capture long-range context and handle complex occlusion patterns.  They also demand larger memory and compute budgets than compact convolutional detectors, which limits straightforward deployment on embedded edge platforms.

Most practical detectors rely on transfer learning from large, curated datasets.  ImageNet pretraining provides general low-level and mid-level features, while COCO remains the principal large-scale benchmark for instance detection and localization [10–12].  Transfer learning reduces labeled-data requirements but introduces domain-shift vulnerabilities when target domains differ from the pretraining distribution.  In maritime environments these vulnerabilities appear as extreme dynamic range, low-texture backgrounds, and reflective water surfaces.  Such conditions often require domain-specific fine-tuning, specialized preprocessing, or more expressive models, all of which increase computational cost.  For these reasons vision methods are commonly paired with geometric sensors for maritime perception; LiDAR provides complementary spatial precision independent of illumination, discussed next.

\begin{equation}
    =======
\end{equation}
Deep learning has fundamentally transformed visual object detection, establishing convolutional neural networks as the dominant paradigm for autonomous perception. 
Modern architectures extract hierarchical features through successive convolutional layers, learning representations that capture both low-level edges and high-level semantic concepts. 
These models have demonstrated remarkable performance on large-scale benchmarks, achieving human-level accuracy on many object categories \cite{lecun1998, he2016}.

Two-stage detectors such as Faster R-CNN pioneered the region-based approach, first generating candidate object proposals and then refining classifications through dedicated networks \cite{girshick2014}. 
While accurate, these architectures impose computational overhead that challenges real-time operation on embedded hardware. 
Single-stage detectors emerged as an alternative, sacrificing some accuracy for substantial speed improvements. 
The YOLO family exemplifies this approach, treating detection as a direct regression problem from image pixels to bounding box coordinates and class probabilities \cite{ultralytics}.

Iterations of YOLO from version five through eight progressively refined the architecture through innovations in backbone design, feature pyramid networks, and anchor-free detection heads \cite{ultralytics}. 
These models achieve real-time frame rates on GPU-accelerated platforms while maintaining competitive accuracy \cite{kim2022}. 
% For example, YOLOv8-nano achieves approximately 45 frames per second on the Jetson AGX Xavier at 15-watt power consumption, while YOLOv8-medium reduces to 20 frames per second at 25 watts \cite{ultralytics, kim2022}. 
\textcolor{red}{However, achieving such performance typically requires model quantization, reduced input resolution, or architectural modifications. 
The computational constraints become particularly acute when processing high-resolution imagery or when scenes contain many small objects that require fine-grained feature extraction.}
\textcolor{blue}{Maritime-specific challenges such as HDR scenes, low-texture backgrounds, and specular reflections often necessitate larger models or specialized preprocessing, further exacerbating the computational burden.}

EfficientDet introduced compound scaling that jointly optimizes network depth, width, and input resolution to maximize accuracy within computational budgets \cite{tan2020}. 
\textcolor{blue}{This approach demonstrates substantial performance-efficiency trade-offs on terrestrial datasets; however, the reliance on dense texture features limits its effectiveness in environments dominated by low-texture regions. }
Transformer-based architectures such as DETR have recently emerged, reformulating detection as a set prediction problem through attention mechanisms \cite{vaswani2017}. 
While these models show promise for capturing long-range dependencies and handling occlusion, their computational requirements substantially exceed those of convolutional approaches, limiting their deployment on resource-constrained platforms.

The success of these architectures depends critically on transfer learning from large-scale datasets. 
ImageNet provides millions of labeled images spanning thousands of object categories, enabling networks to learn generalizable visual representations \cite{krizhevsky2017}. 
The COCO dataset builds upon this foundation with instance segmentation annotations and diverse scene compositions, serving as the primary benchmark for detection research \cite{lin2015, feng2021}. 
\textcolor{blue}{Models pre-trained on these large-scale datasets enable transfer learning by allowing adaptation to new domains with substantially less manually labeled data than would be required if training from scratch.}

However, this transfer learning paradigm introduces domain adaptation challenges when \textcolor{blue}{applied beyond terrestrial environments}.
The learned features encode assumptions about object appearance, texture diversity, spatial layouts, and illumination conditions characteristic of the training distribution.
When these assumptions fail—as they do in many specialized environments—performance degrades substantially.
Domain-specific fine-tuning or specialized training datasets become necessary to recover acceptable detection accuracy.
While visual detection provides semantic richness and texture-based recognition, geometric precision independent of illumination requires complementary \ac{LiDAR} sensing, discussed next.

\subsection{Challenges in Specialized Environments}

The maritime environment presents conditions fundamentally different from the terrestrial scenes that dominate training datasets.
Sky and water surfaces comprise the majority of the image area, providing minimal texture gradients for feature extraction.
Horizon lines create strong edges that can interfere with object localization algorithms designed for scenes with distributed visual complexity.
The high dynamic range of maritime scenes—where bright sky coexists with shadowed vessel hulls—exceeds the capture capability of standard cameras, forcing trade-offs between overexposed highlights and underexposed shadows.

Sun glare produces saturated image regions where all feature information is lost, creating detection blind spots.
\textcolor{red}{Morning and evening operation faces low-angle illumination that produces extended glare periods during crucial navigation times}.
Specular reflections from water surfaces generate mirror-image artifacts that detection networks may misclassify as objects, increasing false positive rates.
The relatively uniform appearance of water and sky provides little contextual information to disambiguate true objects from reflections or sensor artifacts.
These characteristics collectively degrade the performance of models trained primarily on terrestrial imagery, where texture diversity and consistent lighting are implicit assumptions.

These maritime-specific challenges directly motivate the focus of this dissertation on sensor fusion approaches that can compensate for the limitations of vision-only systems under extreme environmental conditions.

\section{Modern LiDAR-Based Object Detection for Autonomy}

\acl{LiDAR} sensors provide three-dimensional geometry independent of illumination conditions, making them complementary to camera-based perception. \ac{LiDAR} systems emit laser pulses and measure time-of-flight to determine range, generating point clouds that represent surface geometry with millimeter-scale precision. Unlike cameras, \ac{LiDAR} performance remains consistent across lighting conditions—from direct sunlight to complete darkness—eliminating a primary failure mode of vision-based systems.

Processing point cloud data for object detection presents unique challenges that differ from those of image-based approaches. 
\textcolor{red}{Point clouds are unordered sets lacking the regular grid structure that enables efficient convolution operations in image processing. 
Points are distributed non-uniformly across space, with density varying by range and surface orientation. Occlusion creates irregular gaps in object representations.} 
These properties motivated the development of specialized architectures for processing point clouds.

Point-based networks such as PointNet operate directly on raw point coordinates, learning permutation-invariant features through symmetric aggregation functions \cite{garcia-garcia2016}. 
PointNet++ extends this approach with hierarchical feature extraction, capturing multi-scale geometric patterns through successive neighborhood grouping \cite{garcia-garcia2016}. 
\textcolor{blue}{While elegant in their handling of unordered data, these networks face computational scalability challenges as point cloud size increases—a common scenario in long-range outdoor sensing.}

Grid-based clustering methods discretize point clouds into regular three-dimensional grids or voxels, enabling efficient processing through three-dimensional convolutions. 
VoxelNet pioneered this approach, using voxel feature encoding to transform sparse point clouds into dense volumetric representations suitable for convolutional processing \cite{zhou2018}. 
\textcolor{red}{The trade-off is quantization error—small objects or fine geometric details may be lost during voxelization, and the choice of voxel resolution creates a fundamental accuracy-efficiency trade-off.}

Hybrid architectures combine voxel and point-based processing to leverage their complementary strengths. 
PV-RCNN generates region proposals from voxel features, then refines predictions using point-based feature extraction within candidate regions \cite{shi2020, shi2019}. 
This two-stage approach achieves high accuracy while maintaining computational efficiency by focusing on point processing only in regions likely to contain objects.

The automotive industry has driven substantial advances in \ac{LiDAR}-based detection, with large-scale datasets such as KITTI, Waymo Open Dataset, and nuScenes providing the foundation for algorithm development \cite{geiger2012}. 
These benchmarks enable quantitative comparison across architectures and have accelerated progress in three-dimensional object detection for road vehicles. 
However, the environmental assumptions embedded in these datasets and their trained models limit direct transferability to other domains.

Recent developments in bird's-eye-view representations and transformer architectures offer promising directions. 
BEVFusion unifies camera and \ac{LiDAR} features in a shared bird's-eye-view space, enabling efficient spatial alignment without explicit geometric projection \cite{liang2022, liu2023b}. 
TransFusion employs transformer attention mechanisms to aggregate multi-scale features across modalities \cite{chitta2023}. 
Point Transformer applies self-attention directly to point clouds, capturing long-range dependencies that may help distinguish objects from complex backgrounds \cite{vaswani2017}. 
\textcolor{blue}{However, these architectures typically require multi-GPU configurations or high-end accelerators such as the NVIDIA RTX 4090 or A100, making them unsuitable for small autonomous platforms without substantial computing infrastructure or offboard processing capabilities.} 
BEVFusion, for instance, requires approximately 300 watts and achieves only 15-20 frames per second on desktop GPUs \cite{liang2022}. \textcolor{red}{specifications that exceed embedded platform capabilities by an order of magnitude.
The power consumption alone—often exceeding 200-400 watts during inference—surpasses the total power budget of many small autonomous vehicles.}
\textcolor{blue}{These transformer-based architectures have not been validated for real-time maritime operation on embedded platforms, nor have they been tested under maritime-specific conditions such as sparse water-surface returns and wave-induced platform motion.}

\subsection{Domain-Specific LiDAR Challenges}

While \ac{LiDAR} eliminates illumination-dependent failures of cameras, it introduces modality-specific challenges that vary substantially across operating environments. 
\textcolor{blue}{Automotive \ac{LiDAR} systems achieve near 100\% return rates on asphalt and concrete surfaces, providing dense geometric representations ideal for detection algorithms trained on road scenes. 
However, these high return rates depend on diffuse reflection from rough surfaces—a property not universal across environments.}

Water surfaces produce sparse, inconsistent returns due to specular reflection. 
Calm water may return less than 10\% of emitted pulses, with most laser energy reflecting away from the sensor rather than back toward it \cite{kunz2005, thompson2019}.
Choppy water improves return rates slightly by creating varied surface orientations, but introduces noise and measurement uncertainty. 
This sparsity challenges detection algorithms designed for dense point clouds with abundant ground-plane reference points, which are typical of terrestrial scenes.

Atmospheric conditions affect \ac{LiDAR} performance through beam attenuation and scattering. 
Fog, rain, and spray scatter laser pulses, reducing effective range and introducing false returns from suspended water droplets. 
\textcolor{blue}{Visibility degradation from 20 kilometers in clear conditions to 2 kilometers in heavy haze corresponds to a 50\% to 70\% reduction in effective \ac{LiDAR} range \cite{roriz2022}. 
While cameras face similar degradation, the failure modes differ—cameras lose contrast and semantic information while \ac{LiDAR} maintains geometric accuracy for returns that do penetrate the atmosphere.}

Platform motion introduces additional complexity for mobile \ac{LiDAR} systems. 
Automotive platforms operate on roads with predictable motion patterns and relatively stable sensor orientations. 
Maritime platforms experience continuous wave-induced pitch, roll, and heave motion that varies dynamically with sea state. 
Unless compensated for through motion correction algorithms or IMU integration, this motion appears as a distortion or "smear" within the point cloud.
% The computational cost of motion compensation and the latency it introduces must be considered when evaluating real-time performance feasibility.

Object reflectance properties vary substantially across target categories. 
Metallic surfaces produce strong returns across most \ac{LiDAR} wavelengths. 
Painted surfaces exhibit wavelength-dependent reflectivity. 
Dark, non-reflective materials such as rubber may produce weak or absent returns despite being geometrically visible. 
These reflectance variations affect detection reliability differently than camera-based systems, where color and texture dominate appearance. 
Having established the capabilities and limitations of both vision and \ac{LiDAR} sensing independently, the following section examines how these modalities perform when deployed in maritime environments.

\section{Applicability to Maritime Autonomous Surface Vessels}

The transition from automotive and general unmanned vehicle perception to maritime applications reveals substantial performance gaps. 
Both vision and \ac{LiDAR} systems face environment-specific failure modes that degrade the effectiveness of algorithms developed for terrestrial operation. 
Understanding these maritime-specific challenges is essential for developing robust perception systems for autonomous surface vessels.

Maritime scenes violate many implicit assumptions of terrestrial perception systems. 
The horizon divides most images into two dominant regions—sky above and water below—with objects typically appearing near this boundary. 
This composition differs fundamentally from terrestrial scenes, where objects appear distributed across the image with varied backgrounds providing contextual cues. 
Detection networks trained on terrestrial datasets learn spatial priors about where objects typically appear; these priors become misleading in maritime environments where most objects occupy narrow horizon regions.

The extreme dynamic range of maritime scenes poses a challenge to standard imaging systems. 
A bright sky often exceeds sensor saturation limits, while shadowed portions of vessels remain underexposed. 
Standard cameras with 8-12 bits per pixel cannot simultaneously capture detail in both regions, forcing exposure compromises that degrade detection performance. 
\acl{HDR} imaging techniques—whether through multi-exposure fusion or specialized sensors—can extend the capture range but introduce computational overhead and may affect the feasibility of real-time processing.

Sun glare creates intermittent detection blind spots as platform and target orientations vary relative to the sun. 
Specular reflections from water surfaces generate mirror images that may be misclassified as objects, increasing false positive rates. 
Detection networks must actively distinguish actual objects from reflections—a task that requires them to understand the underlying physics of reflection geometry, which typical terrestrial training data do not provide.

LiDAR systems face complementary maritime challenges. 
Water surface returns are sparse and inconsistent compared to the dense ground-plane returns typical of road environments. 
Detection algorithms designed for automotive applications rely on abundant ground context to establish spatial reference frames and disambiguate objects from the background. 
Maritime applications lack this dense ground-plane reference, requiring alternative spatial reasoning strategies. Objects appear isolated in space rather than grounded on dense surfaces.

Atmospheric interference affects both modalities but through different mechanisms. 
Fog and spray reduce camera contrast and blur object boundaries, degrading semantic feature extraction. 
These same conditions scatter \ac{LiDAR} beams, reducing range and introducing false returns from suspended water droplets. 
The correlation between visibility degradation and detection performance differs across modalities—cameras may fail under dense fog. At the same time, \ac{LiDAR} maintains reduced-range functionality, or vice versa, under extreme glare conditions.

Wave-induced platform motion introduces temporal alignment challenges absent from stable terrestrial platforms.
A vessel moving at 5 meters per second experiences 0.5 meters of displacement in 100 milliseconds, which is sufficient to create detection mismatches if sensors are not precisely synchronized. 
Pitch and roll motion create apparent object movement, even for stationary targets, which complicates tracking and motion prediction. Motion compensation through IMU integration is necessary but introduces additional processing latency and computational cost.

These maritime-specific challenges have measurable impacts on performance. 
Studies report 2-3 times higher false positive rates for camera-based detection during midday operation compared to overcast conditions \cite{prasad2017, landaeta, liebergall}. 
\ac{LiDAR} return rates over water surfaces drop to 10-30\% of the rates achieved on terrestrial surfaces \cite{kunz2005}. 
Detection confidence scores decrease substantially under haze conditions, even when objects remain geometrically visible. 
These quantitative degradations motivate the development of maritime-specific perception approaches rather than the direct application of terrestrial methods.

The complementary failure modes of camera and \ac{LiDAR} systems suggest potential benefits from sensor fusion. 
Cameras maintain detection capability for low-reflectance objects that produce weak \ac{LiDAR} returns, while  \ac{LiDAR} provides geometric precision under extreme lighting conditions where camera-based detection fails. 
This compatibility drives interest in multimodal fusion architectures that leverage the strengths of each sensor while compensating for individual weaknesses. 
However, deploying such fusion systems on maritime platforms introduces practical constraints that significantly impact architectural choices, as examined in the following section.

\section{Constraints of Real-Time Maritime Deployment}

Autonomous surface vessels operate under constraints that are fundamentally different from those of ground vehicles or stationary installations. 
Power availability, thermal management, processing latency, and physical platform limitations collectively constrain perception system design in ways that favor particular architectural approaches over others.

\textcolor{blue}{Small autonomous vessels operate on battery power with mission durations measured in hours rather than days. 
Every watt consumed by perception systems reduces available endurance or payload capacity. 
Desktop-class GPUs, which may consume 200-400 watts during inference operations, are incompatible with small platform power budgets. 
Embedded computing platforms (such as the NVIDIA Jetson platform) can provide GPU-accelerated inference at 30-watt power consumption—an order of magnitude reduction, enabling extended autonomous operation. 
However, this efficiency comes at the cost of reduced computational throughput, limiting the complexity of algorithms that can execute in real-time.}

Thermal management on maritime platforms presents unique challenges. 
Sealed enclosures protect electronics from salt spray and humidity but limit cooling airflow. 
High ambient temperatures during tropical operation reduce thermal headroom for computing systems. 
Passive cooling solutions are preferred to avoid mechanical failure modes of fans and pumps, but passive cooling capacity limits sustainable power dissipation. 
These thermal constraints favor algorithms with lower computational intensity and more consistent power consumption rather than approaches with high peak demands.

Processing latency directly affects navigation safety. 
Obstacle avoidance requires detection, classification, and path planning within the time available before a potential collision. 
A vessel traveling at 5 meters per second requires detection and response within 10 seconds to avoid an obstacle 50 meters ahead—accounting for vehicle dynamics and maneuvering time. Perception latency consumes part of this budget, leaving less time for decision-making and control. 
Deterministic latency bounds become important; unpredictable processing delays can lead to late detection, compromising safety margins.

Additionally, strict rule enforcement is challenging with neural network-based systems due to their non-deterministic nature. 
Rule enforcement is particularly problematic with spatial data, where ensuring object separation distances is crucial for obstacle avoidance and navigation purposes. 
As such, the investigation of efficient deterministic approaches is still vital for situational awareness of uncrewed vehicles \cite{coyleE}.

Wave-induced platform motion introduces additional temporal alignment requirements. 
Sensors must be synchronized within tolerances that account for platform velocity and motion prediction accuracy. 
Poor temporal alignment can create spatial misalignment between modalities, even when extrinsic calibration is perfect. 
The processing latency introduced by complex fusion algorithms can exceed acceptable synchronization windows, necessitating careful algorithm design to maintain alignment under motion.

Physical platform constraints affect sensor placement and field of view. 
Small vessels offer limited mounting locations with unobstructed views. 
Sensor positions high above the water reduce wave interference but create blind zones near the vessel. 
Multiple sensors improve coverage but increase power consumption, data bandwidth, and processing requirements. 
These trade-offs require careful system-level optimization, balancing coverage, performance, and resource constraints.

% \textcolor{red}{These operational constraints favor particular architectural approaches for maritime perception. 
% Algorithms must operate within embedded computing power budgets—typically 30-50 watts for the entire perception system, including all sensors and processing. 
% Latency must remain bounded and predictable to support safety-critical obstacle avoidance. 
% Power consumption should be consistent rather than highly variable to simplify thermal management. 
% Modularity is valuable—sensor failures should degrade performance gracefully rather than causing complete system failure.}

Late fusion architectures align well with these constraints. 
Independent processing pipelines for each sensor modality enable parallel computation and load balancing, allowing for efficient resource utilization. 
Computational requirements scale approximately linearly with the number of sensors rather than exponentially as with joint processing approaches. 
Individual sensor failures affect only specific detection streams without corrupting fusion outputs. 
Processing latency remains more predictable because each modality follows a fixed pipeline without complex cross-modal dependencies during feature extraction. 
These architectural properties make late fusion particularly attractive for resource-constrained maritime platforms despite potential accuracy trade-offs compared to deeper fusion strategies. 
Understanding how these various fusion paradigms have been developed and evaluated—primarily in non-maritime domains—provides context for this dissertation's focus on late fusion for maritime applications.

\section{Sensor Fusion Paradigms}

Due to the limited availability of maritime datasets and calibrated multimodal sensor suites, most existing fusion works adopt strategies originally proposed for ground-based vehicles. 
The following survey presents fusion paradigms as developed in those mature domains, with explicit identification of maritime-specific applications where they exist. 
The literature on multimodal fusion for maritime autonomous surface vessels is comparatively sparse. 
Unless stated otherwise, the fusion approaches discussed originate from automotive or general unmanned system research; maritime-specific work is explicitly identified where available.

Sensor fusion combines multiple sensing modalities to overcome individual limitations and improve perception robustness. 
Fusion strategies are categorized by the stage at which information from different sensors is combined, with each level offering distinct trade-offs between accuracy, robustness, and computational efficiency.

\subsection{Early Fusion: Data-Level Integration}

Early fusion combines raw sensor data before feature extraction, enabling joint learning from multimodal inputs. 
This approach projects data from different modalities into a common representation space where unified processing can exploit correlations between sensory channels. 
PointPainting exemplifies this strategy, projecting \ac{LiDAR} points into camera image coordinates and assigning each three-dimensional point the semantic label predicted by a two-dimensional image segmentation network. 
The semantically labeled point cloud then serves as input to a three-dimensional object detector, enabling the detector to leverage both geometric and semantic information simultaneously \cite{cui2022}.

MV3D takes an alternative approach, converting \ac{LiDAR} point clouds into multiple two-dimensional representations—bird's-eye view and front view—then fusing these with camera images through convolutional feature extraction \cite{chen2017}. 
The different representations capture complementary geometric perspectives, with the bird's-eye view preserving spatial relationships that are useful for localization and the front view capturing object appearance details. 
These approaches demonstrate that early fusion can effectively combine the geometric precision of \ac{LiDAR} with the semantic richness from cameras, validated primarily on automotive datasets including KITTI and KITTI-360 \cite{geiger2012}.

Early fusion offers the potential for joint feature learning where networks can discover cross-modal correlations during training. 
The unified representation enables gradient flow across modalities during backpropagation, potentially learning features that exploit subtle relationships between geometric and photometric observations.

However, early fusion imposes stringent requirements on spatial and temporal calibration. 
Projection errors of even a few degrees in rotation or centimeters in translation misalign features between modalities, corrupting the joint representation. 
Temporal synchronization must be precise—misaligned timestamps between sensors create spatial inconsistencies that degrade the quality of fusion. 
These calibration requirements become particularly challenging on platforms subject to vibration or deformation. 
Sensor-specific noise and failures propagate directly into the fused representation. 
Corrupted camera images due to lens occlusion or extreme lighting produce degraded semantic labels that affect all projected \ac{LiDAR} points. 
The tight coupling between modalities means that failure or degradation of one sensor can significantly compromise the entire perception system.

\subsection{Mid Fusion: Feature-Level Integration}

Mid fusion extracts features independently from each modality before combining them in intermediate network layers. 
This approach preserves the benefits of joint learning while providing some isolation between sensor-specific processing stages. 
AVOD generates region proposals from both \ac{LiDAR} bird's-eye-view features and camera image features, then fuses these proposals through concatenation in a shared feature space for joint classification and localization refinement \cite{huang2024a}. 
This architecture, evaluated on the KITTI dataset, demonstrates that independent feature extraction followed by fusion can achieve competitive accuracy while providing some robustness to individual sensor noise.

DeepFusion employs cross-attention mechanisms to exchange information between image and \ac{LiDAR} feature pyramids at multiple scales \cite{cui2022}. 
Rather than simple concatenation, attention modules learn to weight features from each modality based on their relevance and reliability for each spatial region. 
This learned weighting can adapt to sensor degradation, down-weighting unreliable features while emphasizing high-quality inputs. 
The approach has been validated on automotive benchmarks but requires substantial computational resources for the multi-scale attention operations.

BEVFusion transforms camera features into a \acl{BEV} representation compatible with \ac{LiDAR} BEV features, then combines them through efficient convolution operations in this unified spatial representation \cite{liang2022, liu2023b}. 
By establishing correspondence in BEV space rather than perspective image space, this method simplifies the geometric alignment problem and enables efficient fusion through standard convolutional operations. 
The approach achieves state-of-the-art performance on the nuScenes and Waymo datasets, although its computational requirements exceed the capabilities of embedded platforms without optimization.

Mid fusion balances joint learning with modular robustness. 
Independent feature extractors isolate sensor-specific noise, preventing corrupted raw data from one sensor from directly affecting the features of the other modality. 
However, this architecture increases computational cost substantially.
Dual-pathway feature extractors and fusion layers approximately double the processing requirements compared to single-modality detection. 
The feature alignment problem remains challenging: correspondences between image features and three-dimensional geometric features must be established either through explicit geometric projection or learned implicitly through network training. 
Computational overhead limits mid-fusion deployment on embedded platforms—running dual feature extraction backbones simultaneously may exceed the embedded GPU's capabilities when real-time constraints are imposed.

\subsection{Late Fusion: Decision-Level Integration}

Late fusion operates on independent object detections from each modality, combining detection outputs through spatial association, probabilistic weighting, or learned scoring functions. 
This approach maintains separate detection pipelines for each sensor and then fuses their results at the decision level, rather than during feature extraction or raw data processing.

Simple late fusion approaches employ non-maximum suppression to remove duplicate detections, keeping the highest-confidence prediction when multiple detections overlap in space. 
Probabilistic methods model detection confidence as uncertainty distributions, combining detections through Bayesian inference or Dempster-Shafer evidence theory to compute fused confidence scores \cite{wang2020a}. 
Learned scoring functions train classifiers to predict fused detection quality based on features of individual detections, such as confidence scores, bounding box overlap, and geometric consistency metrics \cite{pang2020}.

Late fusion's modular architecture provides several practical advantages for maritime deployment. 
Sensor failures affect only individual detection streams without corrupting the fusion process—if camera-based detection fails due to sun glare, \ac{LiDAR}-based detection continues operating independently. 
This graceful degradation maintains partial perception capability under adverse conditions. 
Computational load is distributed across independent pipelines, enabling parallel processing and load balancing across available hardware resources. 
Adding or removing sensor modalities requires minimal system redesign—new detection streams can be integrated by updating the fusion module without modifying existing sensor pipelines.

Processing latency remains more predictable in late fusion architectures because each modality follows a fixed pipeline without runtime dependencies on other sensors during feature extraction. 
Temporal synchronization requirements are relaxed compared to early fusion—spatial association can tolerate larger timing offsets because association operates on object-level regions rather than pixel-level correspondences.

However, late fusion sacrifices the joint context learning enabled by early and mid fusion. 
Instead, the correlations between image features and \ac{LiDAR} geometry must be inferred indirectly through spatial alignment rather than learned directly during feature extraction. 
Detection quality at the decision level determines fusion performance; poor individual detectors produce poor fusion results, regardless of the sophistication of the fusion algorithm. 
Recent work has begun exploring hybrid approaches that blur distinctions between fusion levels. 
TransFusion employs transformer attention mechanisms that can be interpreted as either mid-fusion or late-fusion, depending on their position in the processing pipeline \cite{chitta2023}.

\subsection{Comparative Insights Across Fusion Paradigms}

Comparative studies offer insight into the trade-offs of the fusion paradigm under various operational conditions. %, although most are evaluated using automotive rather than maritime datasets. 
Studies examining fusion under simulated sensor dropout have found that late fusion maintains superior robustness when individual sensors fail or produce degraded outputs \cite{huang2024a, wang2020a}. 
When synchronization error was systematically varied, late fusion tolerated temporal misalignment better than early fusion due to its spatial rather than pixel-level association \cite{huang2024a}. 
However, under ideal conditions with clean data and perfect calibration, mid fusion achieved higher peak accuracy by learning cross-modal correlations unavailable to late fusion \cite{huang2024a}.

These findings suggest that operational requirements and constraints should inform the selection of fusion strategy. 
Early and mid fusion optimize for accuracy under ideal conditions but require precise calibration, tight synchronization, and substantial computational resources. 
Late fusion prioritizes robustness and computational efficiency under degraded conditions at the cost of reduced peak accuracy. 
For maritime applications on embedded hardware where sensor degradation and computational constraints coexist, late fusion represents a pragmatic choice; it preserves robustness and real-time performance while accepting potential accuracy trade-offs compared to deeper fusion strategies.

The scarcity of maritime-specific fusion research leaves open questions about whether these trade-offs hold in maritime environments. 
The relative importance of geometric versus semantic information may differ from automotive scenarios. 
The correlation structure between camera and \ac{LiDAR} observations over water surfaces may not match terrestrial environments. 
Platform motion introduces temporal dynamics that are distinct from those of road vehicle operation. 
These uncertainties motivate empirical evaluation of fusion approaches, specifically in maritime contexts, rather than assuming direct transferability from automotive results. 
These limitations underscore the need for an empirical evaluation of late-fusion performance under maritime constraints, which is addressed in this dissertation through systematic validation on representative embedded hardware and environmental conditions.

\section{Temporal and Spatial Alignment in Fusion Systems}

Accurate fusion depends on establishing both spatial and temporal correspondence between sensors. 
Spatial calibration defines the rigid-body transformation relating sensor coordinate frames, while temporal synchronization ensures that observations represent the exact moment in time. 
The precision requirements for calibration and synchronization vary with fusion paradigm—early fusion demands tighter tolerances than late fusion due to its pixel-level or point-level data alignment.

Extrinsic calibration establishes the geometric relationship between sensors, typically represented as a rotation matrix and translation vector that maps coordinates from one sensor frame to another. 
Traditional calibration employs fiducial targets, such as checkerboards or AprilTags, visible to both sensors, solving for the transformation through the least-squares optimization of corresponding point pairs \cite{zotero-1745}. 
\textcolor{blue}{Manual calibration remains the standard for many research systems despite being labor-intensive, as it provides the precision necessary for reliable geometric correspondence.}

Automated calibration methods have emerged as alternatives. 
Approaches using deep learning predict calibration parameters directly from sensor data, eliminating the need for special targets \cite{iyer2018}. 
Methods employing geometric consistency losses refine predictions by enforcing physical constraints on the transformation \cite{yuan2020, xiao2024, shi2020}. 
However, these automated approaches require sufficient scene structure and may fail in textureless maritime environments, which are often dominated by sky and water.

Calibration accuracy directly affects fusion quality. 
Rotation errors exceeding one degree or translation errors exceeding 10 millimeters produce misalignments that significantly degrade early and mid-fusion performance. 
These tolerances become more stringent as sensor separation increases—cameras and \ac{LiDAR} units mounted meters apart exhibit larger projection errors for distant objects than those mounted closely together.

Temporal synchronization ensures that sensor observations represent the same state of the world. 
Timing offsets can create spatial misalignment, even when geometric calibration is perfect, due to the relative motion between the sensor and the object. 
\textcolor{blue}{A vessel moving at 5 meters per second experiences 0.5 meter displacement in 100 milliseconds, potentially causing detection mismatches if timestamps are not aligned. }
High-precision synchronization protocols such as IEEE 1588 Precision Time Protocol achieve sub-microsecond accuracy when hardware timestamping is supported \cite{ptp2008}. 
Network Time Protocol provides millisecond-level accuracy, sufficient for many applications \cite{mills1991}, but may be inadequate for high-speed platforms or when precise spatial association is required \cite{furgale2013, liu2021}.

Temporal synchronization requirements vary depending on the fusion paradigm and platform dynamics. 
Early fusion requires synchronization within approximately 10 milliseconds, as pixel-level alignment is sensitive to motion-induced shifts. 
Late fusion tolerates larger timing offsets—50 to 100 milliseconds—because spatial association operates on object-level regions rather than pixel correspondences. 
Maritime platforms introduce additional complexity through wave-induced motion. Pitch and roll produce apparent object movement even when targets are stationary relative to the water surface, requiring motion compensation beyond simple timestamp alignment.

Motion compensation algorithms can partially address platform dynamics by predicting sensor pose at measurement time and correcting observations to a common reference frame. 
IMU integration provides high-rate attitude and angular velocity measurements, enabling precise motion prediction. However, motion compensation introduces processing latency and computational overhead that must be considered when evaluating real-time feasibility. 
With calibration and synchronization requirements established, the availability of suitable datasets for maritime fusion research becomes the next critical consideration.

\section{Maritime Perception Datasets and Benchmarks}

Dataset availability fundamentally shapes algorithm development and validation. 
The automotive perception community benefits from large-scale benchmarks providing millions of labeled examples spanning diverse conditions. 
KITTI pioneered three-dimensional object detection benchmarking for autonomous driving, providing synchronized camera and \ac{LiDAR} data with precise annotations \cite{geiger2012}. 
Waymo Open Dataset extends this foundation with orders of magnitude more data spanning varied geographic and weather conditions \cite{su2023}. 
nuScenes adds comprehensive sensor suites including radar and multiple cameras with 360-degree coverage \cite{feng2021}. 
These benchmarks enable quantitative comparison across algorithms and have accelerated progress in terrestrial autonomous vehicle perception.

Maritime perception lacks comparable dataset resources. 
Most available maritime datasets provide only monocular RGB imagery without depth information or multimodal observations. 
Those including \ac{LiDAR} often lack precise temporal synchronization or provide only sparse annotations covering limited object categories. 
Weather diversity remains underrepresented—most datasets capture clear or partly cloudy conditions with few examples of rain, fog, or heavy seas. 
% Small object classes such as navigation buoys and floating debris receive minimal annotation coverage despite safety-critical importance for collision avoidance.

Several notable maritime datasets have been released in recent years, although each addresses only a portion of the broader data gap. 
One dataset provides RGB imagery with GPS and IMU data from coastal and harbor scenes, including annotations for vessels, buoys, and docks. Still, it lacks \ac{LiDAR} data, limiting its utility for multimodal fusion research \cite{su2023}. 
A Singapore-based benchmark includes RGB, infrared, \ac{LiDAR}, and radar modalities captured during multi-weather day and night conditions. However, sparse ground truth and short capture sequences limit its scope for training deep networks \cite{kim2022}. 
A hybrid simulation and real-world collection provides RGB, \ac{LiDAR}, INS, and GPS with annotated trajectories and object classes, but synthetic domain bias and limited small-object labeling create sim-to-real transfer challenges \cite{huang2025}.
\textcolor{red}{What about Thompson24?}

Real-world, synchronized collections from research vessels demonstrate feasibility but remain limited in terms of public availability. 
\textcolor{red}{One effort collected HDR RGB and \ac{LiDAR} data from a WAM-V platform during maritime robotics competitions, providing real-world, synchronized observations under operational conditions. However, limited public access and ongoing annotation restrict the immediate utility of this data (maritime). 
Another dataset provides stereo RGB and \ac{LiDAR} for day and night maritime detection, featuring diverse vessel types. However, its static sensor configuration and limited scene diversity reduce environmental coverage (maritime)}.

\textcolor{blue}{These data gaps limit the development and validation of maritime perception algorithms. 
Small datasets restrict the complexity of models that can be trained without overfitting. 
Limited weather coverage prevents robust evaluation under adverse conditions. 
Sparse annotations for safety-critical object categories leave uncertainty about detection reliability for essential edge cases. 
The absence of standardized benchmarks makes quantitative comparison across methods difficult, slowing progress relative to domains with mature benchmark infrastructure.}

This research contributes a synchronized \ac{LiDAR}-camera dataset collected aboard the WAM-V research vessel during extended operational deployments (maritime). 
The dataset addresses several key gaps through HDR camera imagery capturing extreme dynamic range conditions, synchronized Livox Horizon \ac{LiDAR} data providing precise geometric observations, \textcolor{blue}{diverse maritime object annotations including vessels, buoys, and navigation markers, varied weather and lighting conditions spanning multiple sea states and times of day, and platform motion data enabling motion compensation validation.} 
Detailed dataset characteristics and collection methodology are described in Chapter~\ref{sec:sensor_data_dataset}. 
These dataset limitations, combined with the algorithmic and computational gaps identified in prior sections, motivate this dissertation's research focus on practical late fusion approaches for maritime autonomous surface vessels.

\section{Research Gaps and Dissertation Motivation}

Despite substantial progress in multimodal perception for autonomous systems, several critical gaps remain for maritime applications. 
These gaps span algorithmic approaches, computational implementation, and empirical validation resources. 
This dissertation addresses these deficiencies through a systematic investigation of late fusion methods tailored to maritime operational constraints.

Most published fusion research emphasizes early or mid-level integration, with limited evaluation of decision-level approaches under maritime conditions. 
Comparative studies in automotive contexts demonstrate that late fusion offers superior robustness under sensor dropout or environmental degradation—conditions frequently encountered during maritime operations \cite{huang2024a, wang2020a}. 
However, these findings primarily derive from terrestrial datasets, where environmental characteristics and sensor failure modes differ substantially from those in maritime scenarios. 
Whether late fusion maintains similar robustness advantages under maritime-specific challenges, including water reflections, HDR lighting extremes, and platform motion, remains an open empirical question. 
This knowledge gap, identified in Chapter~\ref{introduction}, motivates a systematic evaluation of late fusion performance across representative maritime conditions. 
This gap is addressed through the development of a late fusion strategy in Chapter~\ref{realtime_object_detection} and quantitative comparison against single-modality baselines.

Real-time fusion implementation on embedded maritime hardware remains underexplored. 
Early and mid-fusion methods often exceed the computational budgets of embedded platforms, requiring desktop-class GPUs for real-time operation. 
Typical automotive fusion networks require 200-400 watts of power and achieve 10-30 frames per second inference on high-end GPUs—specifications incompatible with small battery-powered autonomous vessels. 
Late fusion's reduced computational requirements suggest potential feasibility for embedded deployment; however, a systematic evaluation of accuracy-latency-power trade-offs on representative hardware is lacking. 
Power consumption is particularly critical for battery-operated platforms where sensor and computing loads directly affect mission endurance. 
This dissertation quantifies these trade-offs through detailed profiling on NVIDIA Jetson AGX Xavier hardware, which represents typical embedded computing capabilities for small maritime platforms, as presented in Chapter~\ref{chap:recommendations}.

Few labeled multimodal maritime datasets exist with synchronized \ac{LiDAR} and camera observations under diverse conditions. 
Existing datasets are either single-modality, limiting fusion research; lack precise synchronization, preventing temporal analysis; or provide limited weather and lighting diversity, restricting generalization assessment. 
This data gap directly impedes algorithm development and validation. 
This dissertation contributes a synchronized \ac{LiDAR}-camera dataset collected during extended real-world operations aboard a WAM-V research vessel. 
The dataset encompasses diverse maritime object classes, HDR imaging that captures extreme dynamic range scenarios, varied weather conditions spanning multiple sea states, and platform motion data, enabling motion compensation validation.

The relationship between calibration accuracy and fusion performance under maritime conditions requires empirical characterization. 
While automotive research has established general calibration tolerances, maritime platform dynamics and sensor mounting constraints may impose different requirements. 
Wave-induced motion can create time-varying calibration errors, even when the initial alignment is precise. 
Understanding acceptable calibration tolerances and their interaction with temporal synchronization accuracy is crucial for the practical deployment of systems. 
This research systematically evaluates fusion sensitivity to calibration and synchronization errors through controlled degradation experiments presented in Chapter~\ref{chap:recommendations}.

% These gaps collectively motivate the following research questions addressed throughout this dissertation:

% \textbf{RQ1:} How do \ac{LiDAR} and camera-based object detection performance compare under representative maritime conditions, including HDR lighting, water reflections, and platform motion? Performance comparison establishes the baseline capabilities of each modality independently, identifying the conditions under which each sensor maintains reliable operation and those that cause degradation. These baselines enable quantitative assessment of fusion benefits.

% \textbf{RQ2:} Can late fusion improve detection robustness compared to single-modality approaches without requiring the computational overhead of early or mid fusion? Late fusion's architectural advantages for maritime deployment—modularity, computational efficiency, graceful degradation—are hypothesized to provide practical benefits for embedded platforms. Empirical validation across varied conditions quantifies whether these theoretical advantages translate to measurable performance improvements.

% \textbf{RQ3:} What are practical constraints for implementing real-time late fusion on embedded maritime computing hardware in terms of latency, throughput, and power consumption? Understanding these constraints enables informed system design and establishes feasibility boundaries for fusion approaches. Detailed profiling identifies computational bottlenecks and optimization opportunities.

% \textbf{RQ4:} How do spatial and temporal calibration accuracy affect late fusion performance, and what tolerances are necessary for reliable maritime operation? Calibration requirements determine operational procedures and system maintenance schedules. Excessive tolerance requirements may prove impractical for long-duration deployments, while overly relaxed requirements risk degraded performance. Empirical characterization establishes practical operating bounds.

This dissertation develops and validates a late-fusion framework for real-time \ac{LiDAR}-camera perception on embedded maritime platforms. 
The approach is motivated by the advantages of late fusion for maritime deployment, including modular architecture that isolates sensor failures, computational efficiency compatible with embedded hardware constraints, and robustness to calibration and synchronization errors compared to tightly coupled fusion approaches. 
Research methodology quantitatively evaluates accuracy, timing, and computational efficiency through controlled experiments and real-world validation aboard the WAM-V research platform. 
Results are analyzed across diverse maritime conditions to assess generalization and identify failure modes, providing actionable guidance for autonomous maritime system designers.

The following chapter describes the experimental methodology employed to address these research questions, including detailed hardware configuration, sensor calibration procedures, data collection protocols, and late fusion algorithm design. 
Subsequent chapters present empirical results, discuss implications for maritime perception system design, and identify directions for future research advancing autonomous surface vessel capabilities.

\end{document}