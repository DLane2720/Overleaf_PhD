\documentclass[../main.tex]{subfiles}
\graphicspath{{\subfix{../images/}}}
\begin{document}
\section{Literature Review}

\subsection{Modern Vision-Based Object Detection}
Vision-based detection for autonomous and unmanned systems is dominated by deep learning methods that perform object localization and classification in a single pipeline. Architectures such as Faster R-CNN, SSD, and the YOLO family have demonstrated that real-time operation is achievable when convolutional feature hierarchies are carefully designed for efficiency \cite{he2016, ren2016, ultralytics}. Later YOLO variants improved feature aggregation and prediction heads, making them suitable baselines for embedded perception. Even so, inference speed and accuracy are tightly coupled to image resolution and model depth, which means that, on small maritime platforms, models must often be pruned or run at reduced resolution to stay within latency and power limits \cite{tan2020}.

Transferring these vision models from terrestrial to maritime scenes is not straightforward. Maritime environments exhibit horizon-dominated imagery, low texture, specular water reflections, and extreme lighting variation that degrade features learned from datasets such as COCO or ImageNet \cite{bovcon2020, prasad2017}. Cameras remain attractive because they provide color and semantic cues that are indispensable for recognizing navigational aids or classifying small vessels, and HDR sensors can partly mitigate glare and shadowing \cite{thompson2023}. However, the cost of HDR imaging and the need for domain-specific fine-tuning mean that camera-only solutions do not yet offer uniformly reliable performance at sea.

\subsection{Modern LiDAR-Based Object Detection}
LiDAR sensing complements vision by providing dense, illumination-independent 3D structure. Three main processing strategies are used in current research: point-based networks that operate directly on irregular point sets, voxel-based networks that discretize space to enable 3D convolutions, and hybrid approaches that use voxel features but refine results at the point level \cite{garcia-garcia2016, zhou2018a, shi2021}. Recent work has introduced bird's-eye-view and transformer-style fusion of LiDAR and image features to create unified spatial maps \cite{liu2023bevfusion, bai2022transfusion, zhao2021}. These approaches achieve high accuracy in automotive benchmarks but require computational budgets that exceed what is typically available on embedded maritime hardware.

Unlike road scenes, maritime LiDAR must contend with specular reflections from water, sparse or missing returns over calm surfaces, and high variance in object geometry. Experiments have shown that return rates can drop dramatically over water, reducing the reliability of geometric clustering \cite{halterman2015, yeong2021}. Small navigation buoys, inflatable craft, and narrow masts do not produce the same stable point patterns as cars or pedestrians, further complicating object-level reasoning. As a result, LiDAR alone is rarely sufficient for robust classification in maritime environments.

\subsection{Maritime Perception Challenges and Operational Constraints}
Unmanned and autonomous surface vessels operate in a domain where sensing performance is driven as much by environmental variability as by sensor quality \cite{bai2022, yeong2021}. Glare, haze, and partial occlusion reduce camera effectiveness, while platform motion introduces apparent target motion that is not present in automotive datasets. LiDAR partially decouples perception from illumination but cannot recover color or texture. Because neither modality is consistently reliable, combining them becomes the more defensible strategy.

Operational constraints reinforce this need. Maritime vehicles are frequently battery powered and must run perception, navigation, and communications on a shared compute budget \cite{zotero-item-1911, huang}. Maintaining perception latencies below roughly 100~ms usually requires lighter models, downsampled inputs, or the offloading of noncritical processing to shore-based systems \cite{liang2022}. Methods that assume high-end GPUs or multi-sensor rigs with perfect synchronization are therefore difficult to adopt directly on small ASVs.

\subsection{Sensor Fusion Approaches}
Sensor fusion methods can be described according to when information from different modalities is combined. Early fusion projects LiDAR information into the image frame or otherwise merges raw measurements before feature extraction, which allows the network to learn joint spatial-semantic representations but makes the entire pipeline highly sensitive to calibration and time alignment errors \cite{vora2020, chen2017}. Mid-level fusion extracts features independently from the image and LiDAR streams, then merges them in a shared latent space; this balances robustness and learned cross-modal context, but still increases computational cost and memory usage compared to single-modality detectors \cite{ku2018, li2022deepfusion, liu2023bevfusion}. Late fusion combines detection results after each sensor has been processed, typically through spatial association, score-level weighting, or probabilistic reasoning; it does not require strict pixel-level alignment and it tolerates temporary sensor degradation or dropout \cite{liang2022, xu2023, qi2021}. Comparative studies in non-maritime domains indicate that early and mid fusion deliver the highest peak accuracy when sensors are perfectly aligned, while late fusion delivers the highest robustness when synchronization is imperfect or one modality underperforms \cite{bai2022transfusion, li2022deepfusion}. For maritime use, where both motion-induced misalignment and intermittent sensing are common, late fusion provides a pragmatic balance of performance and deployability.

\subsection{Calibration and Synchronization for Fusion}
All fusion strategies rely on accurate spatial and temporal alignment. Extrinsic calibration establishes the rigid-body transform between the LiDAR and camera frames, often through checkerboard or AprilTag targets and least-squares optimization \cite{iyer2018, yuan2020}. Errors on the order of a degree or a centimeter are enough to produce visible misprojections in camera–LiDAR overlays. Temporal synchronization is equally important: at vessel speeds of several meters per second, a 100~ms offset can create object displacements large enough to prevent successful association. Protocols such as PTP or carefully designed hardware triggering reduce this offset to the 1–10~ms range, which is generally adequate for late-fusion pipelines \cite{bijelic2020}. Maritime operation adds wave-induced pitch and roll that can upset previously valid extrinsics, so periodic validation is necessary, especially on modular platforms \cite{halterman2015}.

\subsection{Maritime Datasets and Domain Coverage}
Progress in maritime perception is limited by the small number of public, well-annotated datasets. MODD2 provides RGB imagery, GPS, and IMU data for coastal and harbor scenes but no LiDAR channel \cite{bovcon2020}. The Singapore Maritime dataset includes RGB, infrared, LiDAR, and radar, but its sequences are short and ground truth is sparse \cite{jun-hwa2022}. Other efforts such as the ERAU dataset collected on the WAM-V platform add HDR imagery and Livox LiDAR in real operating conditions but have restricted availability and platform diversity \cite{thompson2023}. Su et al. present stereo RGB and LiDAR data for vessel detection, but sensor placement is static and the variety of weather and sea states is limited \cite{su2023}. Together, these datasets show that maritime researchers recognize the need for synchronized multimodal data, yet they also highlight underrepresented cases such as small buoys, adverse visibility, and aggressive platform motion. The dataset collected for this dissertation was designed to fill precisely those gaps.

\subsection{Synthesis and Remaining Gaps}
The literature consistently shows that cameras provide semantic richness but fail in HDR and low-contrast maritime scenes, while LiDAR maintains geometric fidelity but cannot classify color-coded or visually ambiguous objects \cite{bovcon2020, prasad2017, yeong2021}. Fusion is therefore not a convenience but a requirement. At the same time, most fusion architectures in the literature were developed for terrestrial autonomous driving and assume stable platforms, high-bandwidth networking, and powerful GPUs \cite{liang2022, xu2023, qi2021}. Embedded maritime systems rarely meet those assumptions. Furthermore, publicly available datasets do not yet provide the synchronized, multimodal, maritime-specific data needed to benchmark real-time fusion methods \cite{thompson2023, su2023}. These gaps—algorithmic, computational, and data-oriented—motivate the work presented in this dissertation, which focuses on late fusion for LiDAR–camera perception on resource-constrained autonomous surface vessels.
\end{document}