
@article{thompson2023,
	title = {Neural {Network} {Fusion} of {Multi}-{Modal} {Sensor} {Data} {For} {Autonomous} {Surface} {Vessels}},
	url = {https://commons.erau.edu/edt/741},
	journal = {Doctoral Dissertations and Master's Theses},
	author = {Thompson, David},
	month = apr,
	year = {2023},
	keywords = {Phd Dissertation},
	file = {"Neural Network Fusion of Multi-Modal Sensor Data For Autonomous Surfac" by David J. Thompson:/Users/dplane/Zotero/storage/AULNLBPS/741.html:text/html;Thompson - Neural Network Fusion of Multi-Modal Sensor Data F.pdf:/Users/dplane/Zotero/storage/VB7LSWXG/Thompson - Neural Network Fusion of Multi-Modal Sensor Data F.pdf:application/pdf},
}

@article{holland,
	title = {Design of the {Minion} {Research} {Platform} for the 2022 {Maritime} {RobotX} {Challenge}},
	abstract = {For the 2022 Maritime RobotX Challenge Embry-Riddle Aeronautical University (ERAU) has made significant improvements to their fully autonomous research platform, Minion. To complete mission tasks, Minion uses sophisticated sensory and perception algorithms fusing data from a suite consisting of multi-beam LiDARs, multi-modal imagery sensors, and a high precision GPS/INS. This data feeds decision-making algorithms that include neural network visual detection, long-range LiDAR-based object detection, and dynamic path planning. These processes are all tied together using a unique tasking system designed to initiate tasks when perception queues are received and select a task order to maximum time usage. The research team has also developed an autonomous drone platform, for completing the tasks that require aerial reconnaissance.},
	language = {en},
	author = {Holland, David and Landaeta, Erasmo and Montagnoli, Charles and Ayars, Taylor and Barnes, Jamie and Barthelemy, Kesmir and Brown, Robert and Delp, Grady and Garnier, Thomas and Halleran, Juan and Helms, Matthew and Hendrickson, James and Kay, James and Kuennen, Kurt and Lachguar, Adam and Perskin, Jennifer and Schoener, Marco and Thomas, Ryan and Thompson, David and Vail, Devon and Coyle, Dr Eric J and Currier, Dr Patrick N and Reinholtz, Dr Charles F},
	file = {Holland et al. - Design of the Minion Research Platform for the 202.pdf:/Users/dplane/Zotero/storage/DBSZFZ2X/Holland et al. - Design of the Minion Research Platform for the 202.pdf:application/pdf;Minion_Journal_Paper_appendices:/Users/dplane/Zotero/storage/X9IY8QUP/Minion_Journal_Paper_appendices.pdf:application/pdf},
}

@misc{coyleE,
	title = {Efficient {Grid}-{Based} {Clustering} and {Concave} {Hull} {Extraction} for {Unstructured} {Point} {Clouds}},
	abstract = {This paper presents a situational awareness technique for point clouds, called Grid-Based Clustering and Concave Hull Extraction (GB-CACHE), which is shown to be suitable for real-time implementation. GB-CACHE is able to efficiently segment and extract concave hulls from unstructured point clouds by sub-sampling these point clouds to a structured grid. The technique is specifically design for unmanned systems perceiving objects that are assumed to lie on a flat surface, such as the ground, water, or sea bed. In addition to segmentation and hull extraction, the technique is shown to enable the implementation of optional processes of point filtering, object classification, mapping, and object tracking. Example GB-CACHE results are shown from four separate unmanned case studies covering three different domains (surface, aerial, underwater), using three different sensing modalities (LiDAR, radar, sonar). Dense LiDAR point clouds are then used to analyze the computational efficiency of each main processes of GB-CACHE. Collectively, GB-CACHE is shown to be efficient enough for real-time implementation even with these dense point clouds and low to mid-grade computing solutions.},
	urldate = {2023-11-09},
	author = {Coyle, Eric},
	keywords = {Sensors, Global Positioning System, Laser radar, LiDAR, mapping, object segmentation, Object segmentation, occupancy grid, Path planning, Sea surface},
	file = {IEEE Xplore Abstract Record:/Users/dplane/Zotero/storage/VWKZE93P/8656488.html:text/html;Thompson et al. - 2019 - Efficient LiDAR-Based Object Segmentation and Mapp.pdf:/Users/dplane/Zotero/storage/P9WUJSX4/Efficient_Grid_Based_Clustering_and_Concave_Hull_Extraction_for_Unsegmented_Point_Clouds.pdf:application/pdf},
}

@misc{shi2021,
	title = {{PV}-{RCNN}: {Point}-{Voxel} {Feature} {Set} {Abstraction} for {3D} {Object} {Detection}},
	shorttitle = {{PV}-{RCNN}},
	url = {http://arxiv.org/abs/1912.13192},
	doi = {10.48550/arXiv.1912.13192},
	abstract = {We present a novel and high-performance 3D object detection framework, named PointVoxel-RCNN (PV-RCNN), for accurate 3D object detection from point clouds. Our proposed method deeply integrates both 3D voxel Convolutional Neural Network (CNN) and PointNet-based set abstraction to learn more discriminative point cloud features. It takes advantages of efficient learning and high-quality proposals of the 3D voxel CNN and the flexible receptive fields of the PointNet-based networks. Specifically, the proposed framework summarizes the 3D scene with a 3D voxel CNN into a small set of keypoints via a novel voxel set abstraction module to save follow-up computations and also to encode representative scene features. Given the high-quality 3D proposals generated by the voxel CNN, the RoI-grid pooling is proposed to abstract proposal-specific features from the keypoints to the RoI-grid points via keypoint set abstraction with multiple receptive fields. Compared with conventional pooling operations, the RoI-grid feature points encode much richer context information for accurately estimating object confidences and locations. Extensive experiments on both the KITTI dataset and the Waymo Open dataset show that our proposed PV-RCNN surpasses state-of-the-art 3D detection methods with remarkable margins by using only point clouds. Code is available at https://github.com/open-mmlab/OpenPCDet.},
	urldate = {2024-05-19},
	publisher = {arXiv},
	author = {Shi, Shaoshuai and Guo, Chaoxu and Jiang, Li and Wang, Zhe and Shi, Jianping and Wang, Xiaogang and Li, Hongsheng},
	month = apr,
	year = {2021},
	note = {arXiv:1912.13192 [cs, eess]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
	file = {arXiv Fulltext PDF:/Users/dplane/Zotero/storage/MU9GRBXN/Shi et al. - 2021 - PV-RCNN Point-Voxel Feature Set Abstraction for 3.pdf:application/pdf;arXiv.org Snapshot:/Users/dplane/Zotero/storage/YVMUCYY7/1912.html:text/html},
}

@article{liebergall,
	title = {Comparing {Standard} and {High} {Dynamic} {Range} {Imagery} for {Maritime} {Object} {Detection}},
	volume = {136},
	language = {en},
	number = {3},
	journal = {Naval Engineers Journal},
	author = {Liebergall, Erik and Landaeta, Erasmo and Coyle, Dr Eric},
	month = sep,
	year = {2024},
	keywords = {disertation},
	pages = {117--124},
	file = {Liebergall et al. - Comparing Standard and High Dynamic Range Imagery .pdf:/Users/dplane/Zotero/storage/FPCS7BMN/Liebergall et al. - Comparing Standard and High Dynamic Range Imagery .pdf:application/pdf},
}

@misc{landaeta,
	title = {Assessing {High} {Dynamic} {Range} {Imagery} {Performance} for {Object} {Detection} in {Maritime} {Environments}},
	language = {en},
	author = {Landaeta, Erasmo},
	month = apr,
	year = {2024},
	keywords = {disertation},
	file = {Landaeta - Assessing High Dynamic Range Imagery Performance f.pdf:/Users/dplane/Zotero/storage/E56CK556/Landaeta - Assessing High Dynamic Range Imagery Performance f.pdf:application/pdf},
}

@misc{eckstein2024,
	title = {{US} {Navy}’s four unmanned ships return from {Pacific} deployment},
	url = {https://www.defensenews.com/naval/2024/01/16/us-navys-four-unmanned-ships-return-from-pacific-deployment/},
	abstract = {Four medium USV prototypes spent five months in the region working with the Navy-Marine team and allies to push the limits of concepts of operations.},
	language = {en},
	urldate = {2025-02-14},
	journal = {Defense News},
	author = {Eckstein, Megan},
	month = jan,
	year = {2024},
	note = {Section: name},
	keywords = {News Article},
	file = {Snapshot:/Users/dplane/Zotero/storage/VKARHDCY/us-navys-four-unmanned-ships-return-from-pacific-deployment.html:text/html},
}

@misc{zotero-1645,
	title = {Autonomous {Vessels} are {Becoming} a {Commercial} {Reality}},
	url = {https://maritime-executive.com/editorials/autonomous-vessels-are-becoming-a-commercial-reality},
	abstract = {TheAutonomous Ship Technology Symposium 2021conference brought together the largest public and priva...},
	language = {en},
	urldate = {2025-02-14},
	journal = {The Maritime Executive},
	keywords = {News Article},
	file = {Snapshot:/Users/dplane/Zotero/storage/3Y3NY458/autonomous-vessels-are-becoming-a-commercial-reality.html:text/html},
}

@article{shao2022,
	title = {Multi-{Scale} {Object} {Detection} {Model} for {Autonomous} {Ship} {Navigation} in {Maritime} {Environment}},
	volume = {10},
	copyright = {https://creativecommons.org/licenses/by/4.0/},
	issn = {2077-1312},
	url = {https://www.mdpi.com/2077-1312/10/11/1783},
	doi = {10.3390/jmse10111783},
	abstract = {Accurate detection of sea-surface objects is vital for the safe navigation of autonomous ships. With the continuous development of artiﬁcial intelligence, electro-optical (EO) sensors such as video cameras are used to supplement marine radar to improve the detection of objects that produce weak radar signals and small sizes. In this study, we propose an enhanced convolutional neural network (CNN) named VarifocalNet * that improves object detection in harsh maritime environments. Speciﬁcally, the feature representation and learning ability of the VarifocalNet model are improved by using a deformable convolution module, redesigning the loss function, introducing a soft non-maximum suppression algorithm, and incorporating multi-scale prediction methods. These strategies improve the accuracy and reliability of our CNN-based detection results under complex sea conditions, such as in turbulent waves, sea fog, and water reﬂection. Experimental results under different maritime conditions show that our method signiﬁcantly outperforms similar methods (such as SSD, YOLOv3, RetinaNet, Faster R-CNN, Cascade R-CNN) in terms of the detection accuracy and robustness for small objects. The maritime obstacle detection results were obtained under harsh imaging conditions to demonstrate the performance of our network model.},
	language = {en},
	number = {11},
	urldate = {2025-02-14},
	journal = {JMSE},
	author = {Shao, Zeyuan and Lyu, Hongguang and Yin, Yong and Cheng, Tao and Gao, Xiaowei and Zhang, Wenjun and Jing, Qianfeng and Zhao, Yanjie and Zhang, Lunping},
	month = nov,
	year = {2022},
	pages = {1783--1803},
	file = {Shao et al. - 2022 - Multi-Scale Object Detection Model for Autonomous .pdf:/Users/dplane/Zotero/storage/45Y2JPIU/Shao et al. - 2022 - Multi-Scale Object Detection Model for Autonomous .pdf:application/pdf},
}

@article{wang2019a,
	title = {Traffic {Light} {Recognition} {With} {High} {Dynamic} {Range} {Imaging} and {Deep} {Learning}},
	volume = {20},
	issn = {1558-0016},
	url = {https://ieeexplore.ieee.org/document/8419782/?arnumber=8419782},
	doi = {10.1109/TITS.2018.2849505},
	abstract = {Traffic light recognition (TLR) detects the traffic light from an image and then estimates the state of the light signal. TLR is important for autonomous vehicles because running against a red light could cause a deadly car accident. For a practical TLR system, computation time, varying illumination conditions, and false positives are three key challenges. In this paper, a novel real-time method is proposed to recognize a traffic light with high dynamic imaging and deep learning. In our approach, traffic light candidates are robustly detected from low exposure/dark frames and accurately classified using a deep neural network in consecutive high exposure/bright frames. This dual-channel mechanism can make full use of undistorted color and shape information in dark frames as well as the rich context in bright frames. In the dark channel, a non-parametric multi-color saliency model is proposed to simultaneously extract lights with different colors. A multiclass classifier with convolutional neural network (CNN) model is then adopted to reduce the number of false positives in the bright channel. The performance is further boosted by incorporating temporal trajectory tracking. In order to speed up the algorithm, a prior detection mask is generated to limit the potential search regions. Intensive experiments on a large dual-channel dataset show that the proposed approach outperforms the state-of-the-art real-time deep learning object detector, which could cause more false positives because it uses bright images only. The algorithm has been integrated into our autonomous vehicle and can work robustly on real roads.},
	number = {4},
	urldate = {2025-02-14},
	journal = {IEEE Transactions on Intelligent Transportation Systems},
	author = {Wang, Jian-Gang and Zhou, Lu-Bing},
	month = apr,
	year = {2019},
	note = {Conference Name: IEEE Transactions on Intelligent Transportation Systems},
	keywords = {Machine learning, deep learning, Cameras, Robustness, autonomous vehicle, high dynamic range imaging, Histograms, Image color analysis, Image recognition, Lighting, Traffic light recognition, automotive},
	pages = {1341--1352},
	file = {IEEE Xplore Abstract Record:/Users/dplane/Zotero/storage/3M6X3JUK/8419782.html:text/html;Wang_Zhou_2019_Traffic Light Recognition With High Dynamic Range Imaging and Deep Learning.pdf:/Users/dplane/Zotero/storage/GE3JC7PF/Wang_Zhou_2019_Traffic Light Recognition With High Dynamic Range Imaging and Deep Learning.pdf:application/pdf},
}

@misc{zotero-1652,
	title = {More {Data} with {Less} {Effort} and {Risk}},
	url = {https://www.hydro-international.com/case-study/sea-machines-autonomy-enables-dea-marine-services-to-collect-more-data-with-less-effort-and-risk},
	abstract = {DEA Marine Services, a division of David Evans and Associates, Inc. (DEA), of Vancouver, Wash., has invested in and utilized Sea Machines Robotics\&rsq...},
	language = {en},
	urldate = {2025-02-14},
	keywords = {News Article},
	file = {Snapshot:/Users/dplane/Zotero/storage/AVQXU927/sea-machines-autonomy-enables-dea-marine-services-to-collect-more-data-with-less-effort-and-ris.html:text/html},
}

@misc{ultralytics,
	title = {Yolo - {Ultralytics} {Home} {Page}},
	url = {https://docs.ultralytics.com/},
	abstract = {Discover Ultralytics YOLO - the latest in real-time object detection and image segmentation. Learn its features and maximize its potential in your projects.},
	language = {en},
	urldate = {2025-02-14},
	author = {Ultralytics},
	keywords = {Documentation, Yolo},
	file = {Snapshot:/Users/dplane/Zotero/storage/YQZY7FTG/docs.ultralytics.com.html:text/html},
}

@article{krizhevsky2017,
	title = {{ImageNet} classification with deep convolutional neural networks},
	volume = {60},
	issn = {0001-0782, 1557-7317},
	url = {point},
	doi = {10.1145/3065386},
	abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of ﬁve convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a ﬁnal 1000-way softmax. To make training faster, we used non-saturating neurons and a very efﬁcient GPU implementation of the convolution operation. To reduce overﬁtting in the fully-connected layers we employed a recently-developed regularization method called “dropout” that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
	language = {en},
	number = {6},
	urldate = {2025-02-14},
	journal = {Commun. ACM},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
	month = may,
	year = {2017},
	keywords = {nn\_core, alexnet},
	pages = {84--90},
	file = {Krizhevsky et al. - 2017 - ImageNet classification with deep convolutional ne.pdf:/Users/dplane/Zotero/storage/H7KD8E5V/Krizhevsky et al. - 2017 - ImageNet classification with deep convolutional ne.pdf:application/pdf},
}

@inproceedings{girshick2014,
	title = {Rich {Feature} {Hierarchies} for {Accurate} {Object} {Detection} and {Semantic} {Segmentation}},
	url = {https://ieeexplore.ieee.org/document/6909475/?arnumber=6909475},
	doi = {10.1109/CVPR.2014.81},
	abstract = {Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30\% relative to the previous best result on VOC 2012 – achieving a mAP of 53.3\%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also present experiments that provide insight into what the network learns, revealing a rich hierarchy of image features. Source code for the complete system is available at http://www.cs.berkeley.edu/ rbg/rcnn.},
	urldate = {2025-02-14},
	booktitle = {2014 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
	month = jun,
	year = {2014},
	note = {ISSN: 1063-6919},
	keywords = {Feature extraction, Training, Vectors, Support vector machines, Object detection, Proposals, Visualization, nn\_core, object deteciton},
	pages = {580--587},
	file = {Girshick et al_2014_Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation.pdf:/Users/dplane/Zotero/storage/CJRLEN2A/Girshick et al_2014_Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/dplane/Zotero/storage/N9STWHI6/6909475.html:text/html},
}

@article{lecun1998,
	title = {Gradient-based learning applied to document recognition},
	volume = {86},
	issn = {1558-2256},
	url = {https://ieeexplore.ieee.org/document/726791/?arnumber=726791},
	doi = {10.1109/5.726791},
	abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day.},
	number = {11},
	urldate = {2025-02-14},
	journal = {Proceedings of the IEEE},
	author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
	month = nov,
	year = {1998},
	note = {Conference Name: Proceedings of the IEEE},
	keywords = {Neural networks, Feature extraction, Machine learning, nn\_core, Character recognition, Hidden Markov models, Multi-layer neural network, Optical character recognition software, Optical computing, Pattern recognition, Principal component analysis, gradient descent},
	pages = {2278--2324},
	file = {IEEE Xplore Abstract Record:/Users/dplane/Zotero/storage/YBE8NWC2/726791.html:text/html;Lecun et al_1998_Gradient-based learning applied to document recognition.pdf:/Users/dplane/Zotero/storage/3UZN4EJL/Lecun et al_1998_Gradient-based learning applied to document recognition.pdf:application/pdf},
}

@inproceedings{he2016,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	url = {https://ieeexplore.ieee.org/document/7780459},
	doi = {10.1109/CVPR.2016.90},
	abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
	urldate = {2025-02-14},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = jun,
	year = {2016},
	note = {ISSN: 1063-6919},
	keywords = {Neural networks, Training, Image recognition, Visualization, nn\_core, Complexity theory, Degradation, Image segmentation, DNN},
	pages = {770--778},
	file = {He et al_2016_Deep Residual Learning for Image Recognition.pdf:/Users/dplane/Zotero/storage/5PUFB45X/He et al_2016_Deep Residual Learning for Image Recognition.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/dplane/Zotero/storage/I79LPQ6F/7780459.html:text/html},
}

@article{kim2022,
	title = {Object {Detection} and {Classification} {Based} on {YOLO}-{V5} with {Improved} {Maritime} {Dataset}},
	volume = {10},
	copyright = {https://creativecommons.org/licenses/by/4.0/},
	issn = {2077-1312},
	url = {https://www.mdpi.com/2077-1312/10/3/377},
	doi = {10.3390/jmse10030377},
	abstract = {SMD (Singapore Maritime Dataset) is a public dataset with annotated videos, and it is almost unique in the training of deep neural networks (DNN) for the recognition of maritime objects. However, there are noisy labels and imprecisely located bounding boxes in the ground truth of the SMD. In this paper, for the benchmark of DNN algorithms, we correct the annotations of the SMD dataset and present an improved version, which we coined SMD-Plus. We also propose augmentation techniques designed especially for the SMD-Plus. More speciﬁcally, an online transformation of training images via Copy \& Paste is applied to solve the class-imbalance problem in the training dataset. Furthermore, the mix-up technique is adopted in addition to the basic augmentation techniques for YOLO-V5. Experimental results show that the detection and classiﬁcation performance of the modiﬁed YOLO-V5 with the SMD-Plus has improved in comparison to the original YOLO-V5. The ground truth of the SMD-Plus and our experimental results are available for download.},
	language = {en},
	number = {3},
	urldate = {2025-02-14},
	journal = {JMSE},
	author = {Kim, Jun-Hwa and Kim, Namho and Park, Yong Woon and Won, Chee Sun},
	month = mar,
	year = {2022},
	keywords = {Neural networks, Deep learning, deep learning, Artificial neural networks, Training, Algorithms, Datasets, Object detection, Annotations, Augmentation, Boxes, Classification, data relabel, Detection, Environmental, maritime dataset, object detection, Object recognition},
	pages = {377},
	file = {Jun-Hwa et al_2022_Object Detection and Classification Based on YOLO-V5 with Improved Maritime.pdf:/Users/dplane/Zotero/storage/QEHUDF5F/Jun-Hwa et al_2022_Object Detection and Classification Based on YOLO-V5 with Improved Maritime.pdf:application/pdf},
}

@inproceedings{haghbayan2018,
	title = {An {Efficient} {Multi}-sensor {Fusion} {Approach} for {Object} {Detection} in {Maritime} {Environments}},
	url = {https://ieeexplore.ieee.org/document/8569890/?arnumber=8569890},
	doi = {10.1109/ITSC.2018.8569890},
	abstract = {Robust real-time object detection and tracking are challenging problems in autonomous transportation systems due to operation of algorithms in inherently uncertain and dynamic environments and rapid movement of objects. Therefore, tracking and detection algorithms must cooperate with each other to achieve smooth tracking of detected objects that later can be used by the navigation system. In this paper, we first present an efficient multi-sensor fusion approach based on the probabilistic data association method in order to achieve accurate object detection and tracking results. The proposed approach fuses the detection results obtained independently from four main sensors: radar, LiDAR, RGB camera and infrared camera. It generates object region proposals based on the fused detection result. Then, a Convolutional Neural Network (CNN) approach is used to identify the object categories within these regions. The CNN is trained on a real dataset from different ferry driving scenarios. The experimental results of tracking and classification on real datasets show that the proposed approach provides reliable object detection and classification results in maritime environments.},
	urldate = {2025-02-14},
	booktitle = {2018 21st {International} {Conference} on {Intelligent} {Transportation} {Systems} ({ITSC})},
	author = {Haghbayan, Mohammad-Hashem and Farahnakian, Fahimeh and Poikonen, Jonne and Laurinen, Markus and Nevalainen, Paavo and Plosila, Juha and Heikkonen, Jukka},
	month = nov,
	year = {2018},
	note = {ISSN: 2153-0017},
	keywords = {Feature extraction, Cameras, LiDAR, convolutional neural networks, favorite, Object detection, Proposals, object detection, Infrared, autonomous vessel, maritime environment, multi-sensor fusion, Radar, region proposals, Sensor fusion, Fusion, RGB Camera, multisensor},
	pages = {2163--2170},
	file = {Haghbayan et al_2018_An Efficient Multi-sensor Fusion Approach for Object Detection in Maritime.pdf:/Users/dplane/Zotero/storage/XGTZHTAP/Haghbayan et al_2018_An Efficient Multi-sensor Fusion Approach for Object Detection in Maritime.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/dplane/Zotero/storage/3XDUSZAU/8569890.html:text/html},
}

@misc{rekavandi2022,
	title = {A {Guide} to {Image} and {Video} based {Small} {Object} {Detection} using {Deep} {Learning} : {Case} {Study} of {Maritime} {Surveillance}},
	shorttitle = {A {Guide} to {Image} and {Video} based {Small} {Object} {Detection} using {Deep} {Learning}},
	url = {http://arxiv.org/abs/2207.12926},
	doi = {10.48550/arXiv.2207.12926},
	abstract = {Small object detection (SOD) in optical images and videos is a challenging problem that even state-of-the-art generic object detection methods fail to accurately localize and identify such objects. Typically, small objects appear in real-world due to large camera-object distance. Because small objects occupy only a small area in the input image (e.g., less than 10\%), the information extracted from such a small area is not always rich enough to support decision making. Multidisciplinary strategies are being developed by researchers working at the interface of deep learning and computer vision to enhance the performance of SOD deep learning based methods. In this paper, we provide a comprehensive review of over 160 research papers published between 2017 and 2022 in order to survey this growing subject. This paper summarizes the existing literature and provide a taxonomy that illustrates the broad picture of current research. We investigate how to improve the performance of small object detection in maritime environments, where increasing performance is critical. By establishing a connection between generic and maritime SOD research, future directions have been identified. In addition, the popular datasets that have been used for SOD for generic and maritime applications are discussed, and also well-known evaluation metrics for the state-of-the-art methods on some of the datasets are provided.},
	urldate = {2025-02-15},
	publisher = {arXiv},
	author = {Rekavandi, Aref Miri and Xu, Lian and Boussaid, Farid and Seghouane, Abd-Krim and Hoefs, Stephen and Bennamoun, Mohammed},
	month = jul,
	year = {2022},
	note = {arXiv:2207.12926 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {Rekavandi et al_2022_A Guide to Image and Video based Small Object Detection using Deep Learning.pdf:/Users/dplane/Zotero/storage/WA2PRAAR/Rekavandi et al_2022_A Guide to Image and Video based Small Object Detection using Deep Learning.pdf:application/pdf;Snapshot:/Users/dplane/Zotero/storage/V2C82P7U/2207.html:text/html},
}

@article{zhang2021,
	title = {Survey on {Deep} {Learning}-{Based} {Marine} {Object} {Detection}},
	volume = {2021},
	copyright = {Copyright © 2021 Ruolan Zhang et al.},
	issn = {2042-3195},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1155/2021/5808206},
	doi = {10.1155/2021/5808206},
	abstract = {We present a survey on marine object detection based on deep neural network approaches, which are state-of-the-art approaches for the development of autonomous ship navigation, maritime surveillance, shipping management, and other intelligent transportation system applications in the future. The fundamental task of maritime transportation surveillance and autonomous ship navigation is to construct a reachable visual perception system that requires high efficiency and high accuracy of marine object detection. Therefore, high-performance deep learning-based algorithms and high-quality marine-related datasets need to be summarized. This survey focuses on summarizing the methods and application scenarios of maritime object detection, analyzes the characteristics of different marine-related datasets, highlights the marine detection application of the YOLO series model, and also discusses the current limitations of object detection based on deep learning and possible breakthrough directions. The large-scale, multiscenario industrialized neural network training is an indispensable link to solve the practical application of marine object detection. A widely accepted and standardized large-scale marine object verification dataset should be proposed.},
	language = {en},
	number = {1},
	urldate = {2025-02-15},
	journal = {Journal of Advanced Transportation},
	author = {Zhang, Ruolan and Li, Shaoxi and Ji, Guanfeng and Zhao, Xiuping and Li, Jing and Pan, Mingyang},
	year = {2021},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1155/2021/5808206},
	pages = {5808206},
	file = {Snapshot:/Users/dplane/Zotero/storage/Y2B7SNZ7/5808206.html:text/html;Zhang et al_2021_Survey on Deep Learning-Based Marine Object Detection.pdf:/Users/dplane/Zotero/storage/H9VJR5BM/Zhang et al_2021_Survey on Deep Learning-Based Marine Object Detection.pdf:application/pdf},
}

@article{prasad2017,
	title = {Video {Processing} {From} {Electro}-{Optical} {Sensors} for {Object} {Detection} and {Tracking} in a {Maritime} {Environment}: {A} {Survey}},
	volume = {18},
	issn = {1558-0016},
	shorttitle = {Video {Processing} {From} {Electro}-{Optical} {Sensors} for {Object} {Detection} and {Tracking} in a {Maritime} {Environment}},
	url = {https://ieeexplore.ieee.org/abstract/document/7812788?casa_token=uLm31TcJ_ZAAAAAA:z_AM44Qg46Tey4YtiI-gPH9T90zdqN5v8pRkgOE567TJrtgeEx1opsMAd6kGFP2F36vjbsSB8zw},
	doi = {10.1109/TITS.2016.2634580},
	abstract = {We present a survey on maritime object detection and tracking approaches, which are essential for the development of a navigational system for autonomous ships. The electro-optical (EO) sensor considered here is a video camera that operates in the visible or the infrared spectra, which conventionally complements radar and sonar for situational awareness at sea and has demonstrated its effectiveness over the last few years. This paper provides a comprehensive overview of various approaches of video processing for object detection and tracking in the maritime environment. We follow an approach-based taxonomy wherein the advantages and limitations of each approach are compared. The object detection system consists of the following modules: horizon detection, static background subtraction, and foreground segmentation. Each of these has been studied extensively in maritime situations and has been shown to be challenging due to the presence of background motion especially due to waves and wakes. The key processes involved in object tracking include video frame registration, dynamic background subtraction, and the object tracking algorithm itself. The challenges for robust tracking arise due to camera motion, dynamic background, and low contrast of tracked object, possibly due to environmental degradation. The survey also discusses multisensor approaches and commercial maritime systems that use EO sensors. The survey also highlights methods from computer vision research, which hold promise to perform well in maritime EO data processing. Performance of several maritime and computer vision techniques is evaluated on Singapore Maritime Dataset.},
	number = {8},
	urldate = {2025-02-15},
	journal = {IEEE Transactions on Intelligent Transportation Systems},
	author = {Prasad, Dilip K. and Rajan, Deepu and Rachmawati, Lily and Rajabally, Eshan and Quek, Chai},
	month = aug,
	year = {2017},
	note = {Conference Name: IEEE Transactions on Intelligent Transportation Systems},
	keywords = {Cameras, video signal processing, Radar tracking, Marine vehicles, Object detection, autonomous automobiles, computer vision, Image edge detection, Intelligent sensors, maritime navigation, Maritime vehicles},
	pages = {1993--2016},
	file = {IEEE Xplore Abstract Record:/Users/dplane/Zotero/storage/XGZHX9U5/7812788.html:text/html;Prasad et al_2017_Video Processing From Electro-Optical Sensors for Object Detection and Tracking.pdf:/Users/dplane/Zotero/storage/2BXQTRNP/Prasad et al_2017_Video Processing From Electro-Optical Sensors for Object Detection and Tracking.pdf:application/pdf},
}

@article{su2023,
	title = {A survey of maritime vision datasets},
	volume = {82},
	issn = {1573-7721},
	url = {https://doi.org/10.1007/s11042-023-14756-9},
	doi = {10.1007/s11042-023-14756-9},
	abstract = {The field of computer vision has been applied in many topics and scenes, especially in the shipping business which occupies a large position in the world trade. With the development of ship intellectualization, the task of detection, tracking, segmentation and classification of interested targets become more and more important. Publicly available dataset is the foundation to promote research in shipping. Based on this intention, we systematically present a review of maritime datasets on maritime perception. In this paper, comparison is made in terms of data type, environment, ground authenticity, and applicable research directions. The aim of writing this paper is to help researchers quickly identify the most suitable dataset for their work.},
	language = {en},
	number = {19},
	urldate = {2025-02-15},
	journal = {Multimed Tools Appl},
	author = {Su, Li and Chen, Yusheng and Song, Hao and Li, Wanyi},
	month = aug,
	year = {2023},
	keywords = {Computer vision, Maritime perception, Maritime ship dataset, Ship intellectualization},
	pages = {28873--28893},
	file = {Su et al_2023_A survey of maritime vision datasets.pdf:/Users/dplane/Zotero/storage/WEQJLWZZ/Su et al_2023_A survey of maritime vision datasets.pdf:application/pdf},
}

@inproceedings{guo2023,
	address = {Singapore},
	title = {Investigating the {Transferability} of {YOLOv5}-{Based} {Water} {Surface} {Object} {Detection} {Model} in {Maritime} {Applications}},
	isbn = {978-981-99-5847-4},
	doi = {10.1007/978-981-99-5847-4_8},
	abstract = {Object detection on the water surface is crucial for unmanned surface vehicles in maritime environments. Despite the challenges posed by variable lighting and ocean conditions, advancements in this field are necessary. In this paper, we investigate the transferability of YOLOv5-based water surface object detection models in cross-domain scenarios. The evaluation is based on publicly available datasets and two newly proposed datasets, Taihu Trial Dataset(TTD) and Fuxian Trial Dataset(FTD), which contain similar target classes but distinct scene and features. Results from extensive experiments indicate that zero-shot transfer is challenging, but a limited number of samples from the target domain can greatly enhance model performance.},
	language = {en},
	booktitle = {International {Conference} on {Neural} {Computing} for {Advanced} {Applications}},
	publisher = {Springer Nature},
	author = {Guo, Yu and Chen, Zhuo and Wang, Qi and Bao, Tao and Zhou, Zexing},
	editor = {Zhang, Haijun and Ke, Yinggen and Wu, Zhou and Hao, Tianyong and Zhang, Zhao and Meng, Weizhi and Mu, Yuanyuan},
	year = {2023},
	keywords = {Yolo, Object detection, Intelligent Perception, Model Transferability, Maritime},
	pages = {103--115},
	file = {Guo et al_2023_Investigating the Transferability of YOLOv5-Based Water Surface Object.pdf:/Users/dplane/Zotero/storage/L6EJCEGE/Guo et al_2023_Investigating the Transferability of YOLOv5-Based Water Surface Object.pdf:application/pdf},
}

@article{kunz2005,
	title = {Detection of small targets in a marine environment using laser radar},
	volume = {5885},
	url = {http://proceedings.spiedigitallibrary.org/proceeding.aspx?doi=10.1117/12.614914},
	doi = {10.1117/12.614914},
	abstract = {Small maritime targets, e.g., periscope tubes, jet skies, swimmers and small boats, are potential threats for naval ships under many conditions, but are difficult to detect with current radar systems due to their limited radar cross section and the presence of sea clutter. On the other hand, applications of lidar systems have shown that the reflections from small targets are significantly stronger than reflections from the sea surface. As a result, dedicated lidar systems are potential tools for the detection of small maritime targets. A geometric approach is used to compare the diffuse reflection properties of cylinders and spheres with flat surfaces, which is used to estimate the maximum detectable range of such objects for a given lidar system. Experimental results using lasers operating at 1.06 µm and 1.57 µm confirm this theory and are discussed. Small buoys near Scheveningen harbor could be detected under adverse weather over more than 9 km. Extrapolation of these results indicates that small targets can be detected out to ranges of approximately 20 km.},
	language = {en},
	urldate = {2025-02-15},
	journal = {SPIE},
	author = {Kunz, Gerard J. and Bekman, Herman H. P. T. and Benoist, Koen W. and Coen, Leo H. and Van Den Heuvel, Johan C. and Van Putten, Frank J. M.},
	editor = {Frouin, Robert J. and Babin, Marcel and Sathyendranath, Shubha},
	month = aug,
	year = {2005},
	keywords = {multiseonsor},
	pages = {58850F},
	file = {Kunz et al. - 2005 - Detection of small targets in a marine environment.pdf:/Users/dplane/Zotero/storage/LM29JWG2/Kunz et al. - 2005 - Detection of small targets in a marine environment.pdf:application/pdf},
}

@inproceedings{carthel2007,
	title = {Multisensor tracking and fusion for maritime surveillance},
	url = {https://ieeexplore.ieee.org/document/4408025?denied=},
	doi = {10.1109/ICIF.2007.4408025},
	abstract = {Over the past several years, the NATO Undersea Research Centre has conducted extensive research in multisensor networks for undersea surveillance, culminating in the development of the DMHT tracker. In this paper, we discuss upgrades to this technology and its application to maritime surveillance.},
	urldate = {2025-02-15},
	booktitle = {2007 10th {International} {Conference} on {Information} {Fusion}},
	author = {Carthel, Craig and Coraluppi, Stefano and Grignan, Patrick},
	month = jul,
	year = {2007},
	keywords = {Target tracking, Radar tracking, Sea measurements, Fusion, Anomaly Detection, Automatic, Coastal Radar, Fusion power generation, Identification System (AIS), Imagery, Maritime Surveillance, Multisensor Fusion and Tracking, Radar detection, Radar imaging, SAR, Signal processing, Sonar equipment, Surveillance, Synthetic aperture radar, Track, multisensor},
	pages = {1--6},
	file = {Carthel et al_2007_Multisensor tracking and fusion for maritime surveillance.pdf:/Users/dplane/Zotero/storage/KC479BI2/Carthel et al_2007_Multisensor tracking and fusion for maritime surveillance.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/dplane/Zotero/storage/625RP48V/4408025.html:text/html},
}

@inproceedings{farahnakian2018,
	address = {Orlando, FL},
	title = {Object {Detection} {Based} on {Multi}-sensor {Proposal} {Fusion} in {Maritime} {Environment}},
	isbn = {978-1-5386-6805-4},
	url = {https://ieeexplore.ieee.org/document/8614183/},
	doi = {10.1109/ICMLA.2018.00158},
	abstract = {In this paper, we propose an effective object detection framework based on proposal fusion of multiple sensors such as infrared camera, RGB cameras, radar and LiDAR. Our framework ﬁrst applies the Selective Search (SS) method on RGB image data to extract possible candidate proposals which likely contain the objects of interest. Then it uses the information from other sensors in order to reduce the number of generated proposals by SS and ﬁnd more dense proposals. Finally, the class of objects within the ﬁnal proposals are identiﬁed by Convolutional Neural Network (CNN). Experimental results on real dataset demonstrate that our framework can precisely detect meaningful object regions using a smaller number of proposals than other object proposals methods. Further, our framework can achieve reliable object detection and classiﬁcation results in maritime environments.},
	language = {en},
	urldate = {2025-02-15},
	booktitle = {2018 17th {IEEE} {International} {Conference} on {Machine} {Learning} and {Applications} ({ICMLA})},
	publisher = {IEEE},
	author = {Farahnakian, Fahimeh and Haghbayan, Mohammad-Hashem and Poikonen, Jonne and Laurinen, Markus and Nevalainen, Paavo and Heikkonen, Jukka},
	month = dec,
	year = {2018},
	keywords = {multisensor},
	pages = {971--976},
	file = {Farahnakian et al. - 2018 - Object Detection Based on Multi-sensor Proposal Fu.pdf:/Users/dplane/Zotero/storage/T6N9CJ4I/Farahnakian et al. - 2018 - Object Detection Based on Multi-sensor Proposal Fu.pdf:application/pdf;Object Detection Based on Multi-sensor Proposal Fusion in Maritime Environment | IEEE Conference Publication | IEEE Xplore:/Users/dplane/Zotero/storage/3VHX9UEK/8614183.html:text/html},
}

@misc{thompson2017,
	title = {Maritime {Object} {Detection}, {Tracking}, and {Classification} {Using} {Lidar} and {Vision}-{Based} {Sensor} {Fusion}},
	url = {https://commons.erau.edu/edt/377},
	author = {Thompson, David},
	month = nov,
	year = {2017},
	keywords = {Masters Thesis},
	file = {"Maritime Object Detection, Tracking, and Classification Using Lidar an" by David John Thompson:/Users/dplane/Zotero/storage/WRPX37BP/377.html:text/html;Thompson_2017_Maritime Object Detection, Tracking, and Classification Using Lidar and.pdf:/Users/dplane/Zotero/storage/GHDNDT5U/Thompson_2017_Maritime Object Detection, Tracking, and Classification Using Lidar and.pdf:application/pdf},
}

@article{farahnakian2020,
	title = {Deep {Learning} {Based} {Multi}-{Modal} {Fusion} {Architectures} for {Maritime} {Vessel} {Detection}},
	volume = {12},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2072-4292},
	url = {https://www.mdpi.com/2072-4292/12/16/2509},
	doi = {10.3390/rs12162509},
	abstract = {Object detection is a fundamental computer vision task for many real-world applications. In the maritime environment, this task is challenging due to varying light, view distances, weather conditions, and sea waves. In addition, light reflection, camera motion and illumination changes may cause to false detections. To address this challenge, we present three fusion architectures to fuse two imaging modalities: visible and infrared. These architectures can provide complementary information from two modalities in different levels: pixel-level, feature-level, and decision-level. They employed deep learning for performing fusion and detection. We investigate the performance of the proposed architectures conducting a real marine image dataset, which is captured by color and infrared cameras on-board a vessel in the Finnish archipelago. The cameras are employed for developing autonomous ships, and collect data in a range of operation and climatic conditions. Experiments show that feature-level fusion architecture outperforms the state-of-the-art other fusion level architectures.},
	language = {en},
	number = {16},
	urldate = {2025-02-15},
	journal = {Remote Sensing},
	author = {Farahnakian, Fahimeh and Heikkonen, Jukka},
	month = jan,
	year = {2020},
	note = {Number: 16
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {deep learning, convolutional neural networks, autonomous vehicles, favorite, object detection, multi-sensor fusion, marine environment},
	pages = {2509},
	file = {Deep Learning Based Multi-Modal Fusion Architectures for Maritime Vessel Detection:/Users/dplane/Zotero/storage/V3MP3HIL/2509.html:text/html;Farahnakian_Heikkonen_2020_Deep Learning Based Multi-Modal Fusion Architectures for Maritime Vessel.pdf:/Users/dplane/Zotero/storage/ZT9I8YBH/Farahnakian_Heikkonen_2020_Deep Learning Based Multi-Modal Fusion Architectures for Maritime Vessel.pdf:application/pdf},
}

@article{norbye,
	title = {Camera-{Lidar} sensor fusion in real time for autonomous surface vehicles},
	language = {en},
	author = {Norbye, Håkon Gjertsen},
	keywords = {favorite},
	file = {Norbye - Camera-Lidar sensor fusion in real time for autono.pdf:/Users/dplane/Zotero/storage/CB8XH9TV/Norbye - Camera-Lidar sensor fusion in real time for autono.pdf:application/pdf},
}

@inproceedings{das2022,
	title = {Sensor fusion in autonomous vehicle using {LiDAR} and camera {Sensor}},
	url = {https://ieeexplore.ieee.org/abstract/document/9929588?casa_token=_Rhnd7QNdQgAAAAA:IzpAXUsoSSdE9Y7-IcI7lo_03dVSJNz11iD-QR6lX5798GjkZSXXe0d-bZ6x_fIUg-ioORirhCA},
	doi = {10.1109/R10-HTC54060.2022.9929588},
	abstract = {This paper presents a sensor fusion methodology for autonomous vehicles (AVs) using Light Detection and Ranging (LiDAR) and camera. Mostly sensors like camera or LiDAR is used only as the sensor for visual perception in AVs. But the hindrance comes during bad weather conditions, dim light or night time. To alleviate this problem, a method which combines both LiDAR and camera sensor using odometry is explored in this paper. The study also attempts to employ Extended Kalman Filter (EKF) to reduce error in position estimate of the vehicle.},
	urldate = {2025-02-15},
	booktitle = {2022 {IEEE} 10th {Region} 10 {Humanitarian} {Technology} {Conference} ({R10}-{HTC})},
	author = {Das, Diptadip and Adhikary, Nabanita and Chaudhury, Saurabh},
	month = sep,
	year = {2022},
	note = {ISSN: 2572-7621},
	keywords = {Autonomous vehicles, Kalman filters, Real-time systems, Cameras, Laser radar, LiDAR, Sensor fusion, Camera, Distance measurement, GPS, Kalman Filter, Odometry, Point cloud compression},
	pages = {336--341},
	file = {Das et al_2022_Sensor fusion in autonomous vehicle using LiDAR and camera Sensor.pdf:/Users/dplane/Zotero/storage/U7P8B57B/Das et al_2022_Sensor fusion in autonomous vehicle using LiDAR and camera Sensor.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/dplane/Zotero/storage/8EY6A4VX/9929588.html:text/html},
}

@inproceedings{subedi2020,
	title = {Camera-{LiDAR} {Data} {Fusion} for {Autonomous} {Mooring} {Operation}},
	url = {https://ieeexplore.ieee.org/document/9248089?utm_source=chatgpt.com},
	doi = {10.1109/ICIEA48937.2020.9248089},
	abstract = {The use of camera and LiDAR sensors to sense the environment has gained increasing popularity in robotics. Individual sensors, such as cameras and LiDARs, fail to meet the growing challenges in complex autonomous systems. One such scenario is autonomous mooring, where the ship has to be tied to a fixed rigid structure (bollard) to keep it stationary safely. The detection and pose estimation of the bollard based on data fusion from the camera and LiDAR are presented here. Firstly, a single shot extrinsic calibration of LiDAR with the camera is presented. Secondly, the camera-LiDAR data fusion method using camera intrinsic parameters and camera to LiDAR extrinsic parameters is proposed. Finally, the use of an image-based segmentation method to segment the corresponding point cloud from the fused camera-LiDAR data is developed and tailored for its application in autonomous mooring operation.},
	urldate = {2025-02-15},
	booktitle = {2020 15th {IEEE} {Conference} on {Industrial} {Electronics} and {Applications} ({ICIEA})},
	author = {Subedi, Dipendra and Jha, Ajit and Tyapin, Ilya and Hovland, Geir},
	month = nov,
	year = {2020},
	note = {ISSN: 2158-2297},
	keywords = {Sensors, Cameras, Laser radar, favorite, Image segmentation, autonomous mooring, camera calibration, Data integration, LiDAR calibration, Robot vision systems, sensor fusion, Three-dimensional displays},
	pages = {1176--1181},
	file = {IEEE Xplore Abstract Record:/Users/dplane/Zotero/storage/PU6PNWZ6/9248089.html:text/html;Subedi et al_2020_Camera-LiDAR Data Fusion for Autonomous Mooring Operation.pdf:/Users/dplane/Zotero/storage/Z6HBKJUK/Subedi et al_2020_Camera-LiDAR Data Fusion for Autonomous Mooring Operation.pdf:application/pdf},
}

@article{tufekci2023,
	title = {Review of lidar-image sensor fusion methods in {3D} object detection and the experiment of camera-lidar fusion in autonomous driving},
	url = {https://repository.library.northeastern.edu/files/neu:4f21nv64c},
	abstract = {In Autonomous driving, the LiDAR sensor has great importance to catch 3D shapes and visualize the environment around cars. These cars have also mounted camera systems calibrated with LiDAR sensors. These camera systems have multiple cameras with overlapping fields of view which totally cover 360°. In this research, we focus on 3D Object Detection and Semantic Segmentation frameworks adopting sensor fusion technologies specifically camera-LiDAR and LiDAR-only frameworks. Also, how the camera view helps LiDAR point cloud is discussed. Early fusion approach by fusing point cloud and pseudo-LiDAR in the early step before the object detection network is experimented within the implementation. The depth map is generated from a single camera view. Depth map from front-view camera image which has forward direction information is adopted in this experiment while merging multi-camera images and merging depth maps are discussed at the same time. Pseudo-LiDAR is generated from a depth map while encountering distortion problems due to an unknown depth shift. To overcome this problem and find exact metric reconstruction, some possible solutions are offered. One possible solution is that pseudo-LiDAR uses a 3D point cloud to learn this unknown depth shift. To align two 3D point clouds, one original and one virtual, into one common base, ground extracted from both point clouds. Pandaset [1] which has 360° and forward-facing LiDAR point cloud and multi-view images is exploited as an autonomous driving dataset.--Author's abstract},
	urldate = {2025-02-15},
	author = {Tufekci, Zeynep},
	year = {2023},
	keywords = {automotive, fusion},
	file = {Tufekci_2023_Review of lidar-image sensor fusion methods in 3D object detection and the.pdf:/Users/dplane/Zotero/storage/F9GASTS4/Tufekci_2023_Review of lidar-image sensor fusion methods in 3D object detection and the.pdf:application/pdf},
}

@article{zhu2024,
	title = {Camera, {LiDAR}, and {IMU} {Based} {Multi}-{Sensor} {Fusion} {SLAM}: {A} {Survey}},
	volume = {29},
	issn = {1007-0214},
	shorttitle = {Camera, {LiDAR}, and {IMU} {Based} {Multi}-{Sensor} {Fusion} {SLAM}},
	url = {https://ieeexplore.ieee.org/document/10258154?utm_source=chatgpt.com},
	doi = {10.26599/TST.2023.9010010},
	abstract = {In recent years, Simultaneous Localization And Mapping (SLAM) technology has prevailed in a wide range of applications, such as autonomous driving, intelligent robots, Augmented Reality (AR), and Virtual Reality (VR). Multi-sensor fusion using the most popular three types of sensors (e.g., visual sensor, LiDAR sensor, and IMU) is becoming ubiquitous in SLAM, in part because of the complementary sensing capabilities and the inevitable shortages (e.g., low precision and long-term drift) of the stand-alone sensor in challenging environments. In this article, we survey thoroughly the research efforts taken in this field and strive to provide a concise but complete review of the related work. Firstly, a brief introduction of the state estimator formation in SLAM is presented. Secondly, the state-of-the-art algorithms of different multi-sensor fusion algorithms are given. Then we analyze the deficiencies associated with the reviewed approaches and formulate some future research considerations. This paper can be considered as a brief guide to newcomers and a comprehensive reference for experienced researchers and engineers to explore new interesting orientations.},
	number = {2},
	urldate = {2025-02-15},
	journal = {Tsinghua Science and Technology},
	author = {Zhu, Jun and Li, Hongyi and Zhang, Tao},
	month = apr,
	year = {2024},
	note = {Conference Name: Tsinghua Science and Technology},
	keywords = {survey, Laser radar, Visualization, multi-sensor fusion, Data integration, Robot vision systems, fusion, localization, Location awareness, navigation, Simultaneous localization and mapping, Simultaneous Localization And Mapping (SLAM), Surveys},
	pages = {415--429},
	file = {IEEE Xplore Abstract Record:/Users/dplane/Zotero/storage/5F6LAGFY/10258154.html:text/html;Zhu et al_2024_Camera, LiDAR, and IMU Based Multi-Sensor Fusion SLAM.pdf:/Users/dplane/Zotero/storage/ZFDKWZ9Z/Zhu et al_2024_Camera, LiDAR, and IMU Based Multi-Sensor Fusion SLAM.pdf:application/pdf},
}

@inproceedings{helgesen2019,
	title = {Sensor {Combinations} in {Heterogeneous} {Multi}-sensor {Fusion} for {Maritime} {Target} {Tracking}},
	url = {https://ieeexplore.ieee.org/document/9011297?utm_source=chatgpt.com},
	doi = {10.23919/FUSION43075.2019.9011297},
	abstract = {Safe navigation for autonomous surface vehicles requires a robust and reliable tracking system that maintains and estimates position and velocity of other vessels. This paper demonstrates a measurement level sensor fusion system for tracking in a maritime environment using lidar, radar, electrooptical and infrared cameras. The backbone of the system is a multi-sensor version of the Joint Integrated Probabilistic Data Association (JIPDA) with both existence and visibility probabilities. Using reference targets equipped with GPS receivers, the performance of different sensors and sensor combinations are evaluated for autonomous surface vehicles (ASVs), Several interesting observations are made, among them that passive sensors can help resolve merged measurements issues in radar tracking, and that the choice between radar and lidar may boil down to a trade-off between fast track initiation and large numbers of false tracks.},
	urldate = {2025-02-15},
	booktitle = {2019 22th {International} {Conference} on {Information} {Fusion} ({FUSION})},
	author = {Helgesen, Øystein Kaarstad and Brekke, Edmund Fϕrland and Helgesen, Håkon Hagen and Engelhardtsen, Øystein},
	month = jul,
	year = {2019},
	keywords = {Target tracking, Robot sensing systems, Cameras, Radar tracking, Detectors, unmanned surface vehicle, Sensor fusion, target tracking},
	pages = {1--9},
	file = {Helgesen et al_2019_Sensor Combinations in Heterogeneous Multi-sensor Fusion for Maritime Target.pdf:/Users/dplane/Zotero/storage/HPC86MI5/Helgesen et al_2019_Sensor Combinations in Heterogeneous Multi-sensor Fusion for Maritime Target.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/dplane/Zotero/storage/UWXSBENQ/9011297.html:text/html},
}

@inproceedings{clunie2021,
	title = {Development of a {Perception} {System} for an {Autonomous} {Surface} {Vehicle} using {Monocular} {Camera}, {LIDAR}, and {Marine} {RADAR}},
	url = {https://ieeexplore.ieee.org/document/9561275?utm_source=chatgpt.com},
	doi = {10.1109/ICRA48506.2021.9561275},
	abstract = {This paper describes a set of software modules and algorithms for maritime object detection and tracking. The approach described here is designed to work in conjunction with various sensors from a maritime surface vessel (e.g. marine RADAR, LIDAR, camera). The described system identifies obstacles from the input sensors, estimates their state, and fuses the obstacle data into a consolidated report. The system is verified using experiments conducted on a live system and successfully demonstrates the ability to detect and track obstacles up to 450m away while operating at 7 fps. The software is open source and available at https://github.com/uml-marine-robotics/asv\_perception.},
	urldate = {2025-02-15},
	booktitle = {2021 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Clunie, Thomas and DeFilippo, Michael and Sacarny, Michael and Robinette, Paul},
	month = may,
	year = {2021},
	note = {ISSN: 2577-087X},
	keywords = {Autonomous systems, intelligent robots, Robot sensing systems, Cameras, Laser radar, Radar tracking, classification, LIDAR, Object detection, object detection, Sensor fusion, sensor fusion, autonomous surface vehicles, calibration, marine RADAR, marine robotics, mobile agents unmanned autonomous vehicles, segmentation, Software algorithms},
	pages = {14112--14119},
	file = {Clunie et al_2021_Development of a Perception System for an Autonomous Surface Vehicle using.pdf:/Users/dplane/Zotero/storage/ELDJNRXK/Clunie et al_2021_Development of a Perception System for an Autonomous Surface Vehicle using.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/dplane/Zotero/storage/DLQXEKQU/9561275.html:text/html},
}

@misc{zotero-1745,
	title = {Camera {Calibration}},
	url = {https://www.mathworks.com/help/vision/camera-calibration.html},
	urldate = {2025-02-17},
	keywords = {matlab},
	file = {Camera Calibration:/Users/dplane/Zotero/storage/XKH2REGE/camera-calibration.html:text/html},
}

@article{zhang2022a,
	title = {Research of {Maritime} {Object} {Detection} {Method} in {Foggy} {Environment} {Based} on {Improved} {Model} {SRC}-{YOLO}},
	volume = {22},
	copyright = {© 2022 by the authors.  Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (https://creativecommons.org/licenses/by/4.0/).  Notwithstanding the ProQuest Terms and Conditions, you may use this content in accordance with the terms of the License.},
	url = {https://www.proquest.com/docview/2728531819/abstract/3629C63C879482APQ/1},
	doi = {10.3390/s22207786},
	abstract = {An improved maritime object detection algorithm, SRC-YOLO, based on the YOLOv4-tiny, is proposed in the foggy environment to address the issues of false detection, missed detection, and low detection accuracy in complicated situations. To confirm the model’s validity, an ocean dataset containing various concentrations of haze, target angles, and sizes was produced for the research. Firstly, the Single Scale Retinex (SSR) algorithm was applied to preprocess the dataset to reduce the interference of the complex scenes on the ocean. Secondly, in order to increase the model’s receptive field, we employed a modified Receptive Field Block (RFB) module in place of the standard convolution in the Neck part of the model. Finally, the Convolutional Block Attention Module (CBAM), which integrates channel and spatial information, was introduced to raise detection performance by expanding the network model’s attention to the context information in the feature map and the object location points. The experimental results demonstrate that the improved SRC-YOLO model effectively detects marine targets in foggy scenes by increasing the mean Average Precision (mAP) of detection results from 79.56\% to 86.15\%.},
	language = {English},
	number = {20},
	urldate = {2025-02-17},
	journal = {Sensors},
	author = {Zhang, Yihong and Ge, Hang and Lin, Qin and Zhang, Ming and Sun, Qiantao},
	year = {2022},
	note = {Num Pages: 7786
Place: Basel, Switzerland
Publisher: MDPI AG},
	keywords = {Deep learning, Algorithms, Datasets, Object detection, Detection, object detection, Accuracy, convolutional block attention module, Evacuations \& rescues, Feature maps, Haze, receptive field block, Spatial data, YOLOv4-tiny, CNN, inclement weather},
	pages = {7786},
	file = {Full Text PDF:/Users/dplane/Zotero/storage/8IGZLRX6/Zhang et al. - 2022 - Research of Maritime Object Detection Method in Fo.pdf:application/pdf},
}

@inproceedings{biasutti2019,
	title = {{LU}-{Net}: {An} {Efficient} {Network} for {3D} {LiDAR} {Point} {Cloud} {Semantic} {Segmentation} {Based} on {End}-to-{End}-{Learned} {3D} {Features} and {U}-{Net}},
	shorttitle = {{LU}-{Net}},
	url = {https://ieeexplore.ieee.org/document/9022291/?arnumber=9022291},
	doi = {10.1109/ICCVW.2019.00123},
	abstract = {We propose LU-Net - for LiDAR U-Net, a new method for the semantic segmentation of a 3D LiDAR point cloud. Instead of applying some global 3D segmentation method such as PointNet, we propose an end-to-end architecture for LiDAR point cloud semantic segmentation that efficiently solves the problem as an image processing problem. We first extract high-level 3D features for each point given its 3D neighbors. Then, these features are projected into a 2D multichannel range-image by considering the topology of the sensor. Thanks to these learned features and this projection, we can finally perform the segmentation using a simple U-Net segmentation network, which performs very well while being very efficient. In this way, we can exploit both the 3D nature of the data and the specificity of the LiDAR sensor. This approach outperforms the state-of-the-art by a large margin on the KITTI dataset, as our experiments show. Moreover, this approach operates at 24fps on a single GPU. This is above the acquisition rate of common LiDAR sensors which makes it suitable for real-time applications.},
	urldate = {2025-02-17},
	booktitle = {2019 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} {Workshop} ({ICCVW})},
	author = {Biasutti, Pierre and Lepetit, Vincent and Aujol, Jean-François and Brédif, Mathieu and Bugeau, Aurélie},
	month = oct,
	year = {2019},
	note = {ISSN: 2473-9944},
	keywords = {Feature extraction, deep learning, Robot sensing systems, Laser radar, LiDAR, Image segmentation, Three-dimensional displays, 3D point cloud, cnn, semantic segmentation, Semantics, Two dimensional displays},
	pages = {942--950},
	file = {Full Text PDF:/Users/dplane/Zotero/storage/MJR2QDS3/Biasutti et al. - 2019 - LU-Net An Efficient Network for 3D LiDAR Point Cl.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/dplane/Zotero/storage/ZB336YVH/9022291.html:text/html},
}

@article{kumar2020,
	title = {{LiDAR} and {Camera} {Fusion} {Approach} for {Object} {Distance} {Estimation} in {Self}-{Driving} {Vehicles}},
	volume = {12},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2073-8994},
	url = {https://www.mdpi.com/2073-8994/12/2/324},
	doi = {10.3390/sym12020324},
	abstract = {The fusion of light detection and ranging (LiDAR) and camera data in real-time is known to be a crucial process in many applications, such as in autonomous driving, industrial automation, and robotics. Especially in the case of autonomous vehicles, the efficient fusion of data from these two types of sensors is important to enabling the depth of objects as well as the detection of objects at short and long distances. As both the sensors are capable of capturing the different attributes of the environment simultaneously, the integration of those attributes with an efficient fusion approach greatly benefits the reliable and consistent perception of the environment. This paper presents a method to estimate the distance (depth) between a self-driving car and other vehicles, objects, and signboards on its path using the accurate fusion approach. Based on the geometrical transformation and projection, low-level sensor fusion was performed between a camera and LiDAR using a 3D marker. Further, the fusion information is utilized to estimate the distance of objects detected by the RefineDet detector. Finally, the accuracy and performance of the sensor fusion and distance estimation approach were evaluated in terms of quantitative and qualitative analysis by considering real road and simulation environment scenarios. Thus the proposed low-level sensor fusion, based on the computational geometric transformation and projection for object distance estimation proves to be a promising solution for enabling reliable and consistent environment perception ability for autonomous vehicles.},
	language = {en},
	number = {2},
	urldate = {2025-02-21},
	journal = {Symmetry},
	author = {Kumar, G. Ajay and Lee, Jin Hee and Hwang, Jongrak and Park, Jaehyeong and Youn, Sung Hoon and Kwon, Soon},
	month = feb,
	year = {2020},
	note = {Number: 2
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {autonomous vehicle, sensor fusion, computational geometry transformation, depth sensing, point cloud to image mapping, projection, self-driving vehicle, sensor calibration, image source},
	pages = {324},
	file = {Kumar et al_2020_LiDAR and Camera Fusion Approach for Object Distance Estimation in Self-Driving.pdf:/Users/dplane/Zotero/storage/MYAW7BJS/Kumar et al_2020_LiDAR and Camera Fusion Approach for Object Distance Estimation in Self-Driving.pdf:application/pdf},
}

@inproceedings{garcia-garcia2016,
	title = {{PointNet}: {A} {3D} {Convolutional} {Neural} {Network} for real-time object class recognition},
	shorttitle = {{PointNet}},
	url = {https://ieeexplore.ieee.org/document/7727386/?arnumber=7727386},
	doi = {10.1109/IJCNN.2016.7727386},
	abstract = {During the last few years, Convolutional Neural Networks are slowly but surely becoming the default method solve many computer vision related problems. This is mainly due to the continuous success that they have achieved when applied to certain tasks such as image, speech, or object recognition. Despite all the efforts, object class recognition methods based on deep learning techniques still have room for improvement. Most of the current approaches do not fully exploit 3D information, which has been proven to effectively improve the performance of other traditional object recognition methods. In this work, we propose PointNet, a new approach inspired by VoxNet and 3D ShapeNets, as an improvement over the existing methods by using density occupancy grids representations for the input data, and integrating them into a supervised Convolutional Neural Network architecture. An extensive experimentation was carried out, using ModelNet - a large-scale 3D CAD models dataset - to train and test the system, to prove that our approach is on par with state-of-the-art methods in terms of accuracy while being able to perform recognition under real-time constraints.},
	urldate = {2025-02-22},
	booktitle = {2016 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	author = {Garcia-Garcia, A. and Gomez-Donoso, F. and Garcia-Rodriguez, J. and Orts-Escolano, S. and Cazorla, M. and Azorin-Lopez, J.},
	month = jul,
	year = {2016},
	note = {ISSN: 2161-4407},
	keywords = {Neural networks, Machine learning, Solid modeling, Computer architecture, Object recognition, Three-dimensional displays, Two dimensional displays, PointNet},
	pages = {1578--1584},
	file = {Garcia-Garcia et al_2016_PointNet.pdf:/Users/dplane/Zotero/storage/B5E8SE2I/Garcia-Garcia et al_2016_PointNet.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/dplane/Zotero/storage/QY3UAIDX/7727386.html:text/html},
}

@article{thompson2019,
	title = {Efficient {LiDAR}-{Based} {Object} {Segmentation} and {Mapping} for {Maritime} {Environments}},
	volume = {44},
	issn = {1558-1691},
	url = {https://ieeexplore.ieee.org/document/8656488/?arnumber=8656488},
	doi = {10.1109/JOE.2019.2898762},
	abstract = {This paper proposes a method that utilizes a 3-D occupancy grid to efficiently map a large area while retaining simple representations of objects for path planning and provide spatial characteristics of objects, which may be used for object classification. To enable large-scale mapping of objects, a region around the unmanned surface vehicle (USV) is defined where a high density of LiDAR returns is expected, termed the visibility horizon. The polygon intersection between the visibility horizon and the newly detected objects is computed, as well as the polygon subtraction of the visibility horizon from the mapped list of polygons. The two polygon lists are then combined using a polygon union operation, with the objects retaining class designations. The result is a 2-D map that contains polygon representations of objects, where the object is described with a tunable number of vertices and may have an associated object class. Thus, providing necessary information for path planning and tasking. The resultant polygons are shown here to be accurate to 20 cm using a 10-cm occupancy grid and 16-ft-long unmanned surface vehicles with four multibeam LiDAR sensors.},
	number = {2},
	urldate = {2025-02-22},
	journal = {IEEE Journal of Oceanic Engineering},
	author = {Thompson, David and Coyle, Eric and Brown, Jeremy},
	month = apr,
	year = {2019},
	note = {Conference Name: IEEE Journal of Oceanic Engineering},
	keywords = {Sensors, Global Positioning System, Laser radar, LiDAR, mapping, object segmentation, Object segmentation, occupancy grid, Path planning, Sea surface},
	pages = {352--362},
	file = {IEEE Xplore Abstract Record:/Users/dplane/Zotero/storage/RIEJ7BRH/8656488.html:text/html;Thompson et al_2019_Efficient LiDAR-Based Object Segmentation and Mapping for Maritime Environments.pdf:/Users/dplane/Zotero/storage/TVJDZG8K/Thompson et al_2019_Efficient LiDAR-Based Object Segmentation and Mapping for Maritime Environments.pdf:application/pdf},
}

@article{lachguar,
	title = {Minion: {Design} and {Competition} {Strategy} for the 2024 {Maritime} {RobotX} {Challenge} {Minion}},
	abstract = {Embry-Riddle Aeronautical University’s Team Minion is returning to RobotX with significant improvements to its defending champion fully autonomous surface vessel (ASV), Minion. Team Minion’s new design strategy and systems engineering approach, called Minion Process, has allowed a balance of academics, research, and team objectives throughout the team. This design strategy combined with a rigorous multistep testing process that values safety and innovation has led to an ever-improving toolset for Minion and its Uncrewed Aerial Vehicle (UAV), Kevin. These include software enhancements of a new patent-pending control scheme and better integration of computer vision throughout the system, as well as hardware improvements of azimuthing motor control, new UAV capabilities, and a new ball launcher.},
	language = {en},
	author = {Lachguar, Adam and Ucles, Giovanna and Sarkar, Sagar and Lane, Dan and Liebergall, Erik and Aggarwal, Sarthak and Proper, Willis and Saravis, Michael and Dcruz, Dean and Kay, Isaac and Abe, Matis and Jagwani, Bharat and Vinnakota, Rohit and Prudencio, Mateus and Yelvington, Marshall and Young, Jacob and Pathi, Neel and Park, Brian and Anand, Aarambh and Lam, Andrew and Jones, Thomas and Coyle, Dr Eric and Currier, Dr Patrick},
	file = {Lachguar et al. - Minion Design and Competition Strategy for the 20.pdf:/Users/dplane/Zotero/storage/UJDUIAW2/Lachguar et al. - Minion Design and Competition Strategy for the 20.pdf:application/pdf;Lachguar et al. - Minion Design and Competition Strategy for the 20.pdf:/Users/dplane/Zotero/storage/IGRH95RN/Lachguar et al. - Minion Design and Competition Strategy for the 20.pdf:application/pdf},
}

@article{ahmed2024,
	title = {Seal {Pipeline}: {Enhancing} {Dynamic} {Object} {Detection} and {Tracking} for {Autonomous} {Unmanned} {Surface} {Vehicles} in {Maritime} {Environments}},
	volume = {8},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2504-446X},
	shorttitle = {Seal {Pipeline}},
	url = {https://www.mdpi.com/2504-446X/8/10/561},
	doi = {10.3390/drones8100561},
	abstract = {This study addresses the dynamic object detection problem for Unmanned Surface Vehicles (USVs) in marine environments, which is complicated by boat tilting and camera illumination sensitivity. A novel pipeline named “Seal” is proposed to enhance detection accuracy and reliability. The approach begins with an innovative preprocessing stage that integrates data from the Inertial Measurement Unit (IMU) with LiDAR sensors to correct tilt-induced distortions in LiDAR point cloud data and reduce ripple effects around objects. The adjusted data are grouped using clustering algorithms and bounding boxes for precise object localization. Additionally, a specialized Kalman filter tailored for maritime environments mitigates object discontinuities between successive frames and addresses data sparsity caused by boat tilting. The methodology was evaluated using the VRX simulator, with experiments conducted on the Volga River using real USVs. The preprocessing effectiveness was assessed using the Root Mean Square Error (RMSE) and tracking accuracy was evaluated through detection rate metrics. The results demonstrate a 25\% to 30\% improvement in detection accuracy and show that the pipeline can aid industry even with sparse object representation across different frames. This study highlights the potential of integrating sensor fusion with specialized tracking for accurate dynamic object detection in maritime settings, establishing a new benchmark for USV navigation systems’ accuracy and reliability.},
	language = {en},
	number = {10},
	urldate = {2025-03-14},
	journal = {Drones},
	author = {Ahmed, Mohamed and Rasheed, Bader and Salloum, Hadi and Hegazy, Mostafa and Bahrami, Mohammad Reza and Chuchkalov, Mikhail},
	month = oct,
	year = {2024},
	note = {Number: 10
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {dynamic object detection, Kalman filter, LiDAR point cloud, unmanned surface vehicles},
	pages = {561},
	file = {Ahmed et al_2024_Seal Pipeline.pdf:/Users/dplane/Zotero/storage/CSB4LEXB/Ahmed et al_2024_Seal Pipeline.pdf:application/pdf},
}

@article{li2023,
	title = {A survey of maritime unmanned search system: {Theory}, applications and future directions},
	volume = {285},
	issn = {0029-8018},
	shorttitle = {A survey of maritime unmanned search system},
	url = {https://www.sciencedirect.com/science/article/pii/S0029801823017432},
	doi = {10.1016/j.oceaneng.2023.115359},
	abstract = {The rising frequency of ocean activities, such as ocean transportation and marine resources development, inevitably leads to a higher incidence of sudden accidents at sea. Maritime search and rescue (MSAR) plays a crucial role in marine transportation. Specifically, maritime search serves as a preparatory mission before executing the actual rescue operations. In 2018, the International Maritime Organization (IMO) made significant legal amendments to the international convention for the Safety of Life at Sea (SOLAS), emphasizing the critical importance of MSAR operations. Nevertheless, the conventional maritime search systems, such as independent surface ships, underwater vehicles, or aerial platforms, fall short of meeting the demands of modern shipping and can impose significant energy and economic burdens. Meanwhile, unmanned systems are gaining prominence in maritime search due to their notable advantages, including high efficiency, cost-effectiveness, and rapid deployment. One observes that the research of unmanned maritime search is still in an early stage. This paper provides a comprehensive survey of the state of the art of maritime unmanned search system development with four dimensions, including the unmanned aerial vehicle (UAV), unmanned surface vessel (USV) and underwater unmanned vehicle (UUV) and its cooperative heterogeneous vehicles. Firstly, the search types, the superiorities and the application scenarios of the unmanned search system are summarized. Then, the theoretical progress, engineering applications and limitations for the unmanned search system are investigated. To further discuss the advantages of the unmanned search system, a novel cooperative platform is established that uses an USV and two UAVs. In the proposed platform, the sensor, communication, guidance and control subsystems are described to illustrate the convenience and the effectiveness of the proposed cooperative system. Finally, the future research directions are examined to expedite the practical implementation of theoretical advancements in the field of maritime unmanned search.},
	urldate = {2025-03-14},
	journal = {Ocean Engineering},
	author = {Li, Jiqiang and Zhang, Guoqing and Jiang, Changyan and Zhang, Weidong},
	month = oct,
	year = {2023},
	keywords = {Unmanned surface vessel, Autonomous guidance and control, Cooperative unmanned systems, Maritime unmanned search system, Underwater unmanned vehicle, Unmanned aerial vehicle},
	pages = {115359},
	file = {Li et al_2023_A survey of maritime unmanned search system.pdf:/Users/dplane/Zotero/storage/9PQ6WKBM/Li et al_2023_A survey of maritime unmanned search system.pdf:application/pdf;ScienceDirect Snapshot:/Users/dplane/Zotero/storage/TDL5W3SE/S0029801823017432.html:text/html},
}

@article{bai2022,
	title = {A {Review} of {Current} {Research} and {Advances} in {Unmanned} {Surface} {Vehicles}},
	volume = {21},
	issn = {1993-5048},
	url = {https://doi.org/10.1007/s11804-022-00276-9},
	doi = {10.1007/s11804-022-00276-9},
	abstract = {Following developments in artificial intelligence and big data technology, the level of intelligence in intelligent vessels has been improved. Intelligent vessels are being developed into unmanned surface vehicles (USVs), which have widely interested scholars in the shipping industry due to their safety, high efficiency, and energy-saving qualities. Considering the current development of USVs, the types of USVs and applications domestically and internationally are being investigated. USVs emerged with technological developments and their characteristics show some differences from traditional vessels, which brings some problems and advantages for their application. Certain maritime regulations are not applicable to USVs and must be changed. The key technologies in the current development of USVs are being investigated. While the level of intelligence is improving, the protection of cargo cannot be neglected. An innovative approach to the internal structure of USVs is proposed, where the inner hull can automatically recover its original state in case of outer hull tilting. Finally, we summarize the development status of USVs, which are an inevitable direction of development in the marine field.},
	language = {en},
	number = {2},
	urldate = {2025-03-14},
	journal = {J. Marine. Sci. Appl.},
	author = {Bai, Xiangen and Li, Bohan and Xu, Xiaofeng and Xiao, Yingjie},
	month = jun,
	year = {2022},
	keywords = {Unmanned surface vehicle, Artificial Intelligence, Intelligent vessel, Internal structure, Maritime supervision, Ship automation level, Shipping industry},
	pages = {47--58},
	file = {Bai et al_2022_A Review of Current Research and Advances in Unmanned Surface Vehicles.pdf:/Users/dplane/Zotero/storage/K76MBVJN/Bai et al_2022_A Review of Current Research and Advances in Unmanned Surface Vehicles.pdf:application/pdf},
}

@article{ma2024,
	title = {Multi-modal information fusion for {LiDAR}-based {3D} object detection framework},
	volume = {83},
	issn = {1573-7721},
	url = {https://doi.org/10.1007/s11042-023-15452-4},
	doi = {10.1007/s11042-023-15452-4},
	abstract = {With the rapid development of water transportation, ship safety supervision is facing more severe pressures and challenges. Precise and efficient detection of ship targets is becoming more and more important, which urgently requires intelligent detection methods to ultimately improves shipping management efficiency. However, the surveillance video of waterway transportation is often influenced by fog and rain, which can affect the performance of object detection and reduce the efficiency of management. The current traditional object approaches are hard to handle these problems. In this paper, we propose a novel multi-modal information fusion method to handle multi-object detection in waterway transportation, which introduces the LiDAR (Light Detection And Ranging) dataset to add spatial information and handle the interference of fog and rain. The target ROI (Region Of Interest) point cloud and image data are initially fused in the pre-fusion stage. This phase can efficiently direct the network’s attention to the region with the highest target probability, increasing the target recall rate. The 3D bounding box in the point cloud and 2D bounding boxes in the image retrieved are then fused in the post-fusion stage to improve target precision and enrich target detection information. Finally, using time synchronization and a space transformation matrix, the detection result is transferred to the picture coordinate system to create a ship image target with 3D depth information. This technique overcomes the constraints of single-sensor environment perception, adapts to the detection of ship targets in a variety of situations, and is more precise and robust. The algorithm’s superiority is also demonstrated by the experiments.},
	language = {en},
	number = {3},
	urldate = {2025-03-14},
	journal = {Multimed Tools Appl},
	author = {Ma, Ruixin and Yin, Yong and Chen, Jing and Chang, Rihao},
	month = jan,
	year = {2024},
	keywords = {LiDAR, Computer vision, Object detection, Data fusion, Multi-modality},
	pages = {7995--8012},
	file = {Ma et al_2024_Multi-modal information fusion for LiDAR-based 3D object detection framework.pdf:/Users/dplane/Zotero/storage/2CPN2KY2/Ma et al_2024_Multi-modal information fusion for LiDAR-based 3D object detection framework.pdf:application/pdf},
}

@article{xie2024,
	title = {Reliable {LiDAR}-based ship detection and tracking for {Autonomous} {Surface} {Vehicles} in busy maritime environments},
	volume = {312},
	issn = {0029-8018},
	url = {https://www.sciencedirect.com/science/article/pii/S002980182402626X},
	doi = {10.1016/j.oceaneng.2024.119288},
	abstract = {Environmental perception is a crucial requirement of Autonomous Surface Vehicles (ASVs) if required to perform tasks safely in a dynamically complex operational environment. Most existing methods for ship detection rely on camera-based methods, which are sensitive to environmental conditions and cannot directly provide spatial location information related to detected targets. To overcome this limitation, we propose a LiDAR-based ship detection and tracking framework that can be applied to busy maritime environments. The proposed framework consists of two functional modules: a ship detection and multi-object tracking. For ship detection, a modularised network structure was adapted, allowing for ease of switching between different types of detection network to prioritise either detection accuracy, detection speed or a compromise of both, depending on the task requirements. A Kalman Filter-based multi-object tracking method is also implemented to compensate for any detections that may have been missed as a result of ship motions or occlusions, relying solely on the detection results. We also collected the first-ever real-world LiDAR dataset for maritime applications across the River Thames and marinas, including a range of ship types, with lengths ranging from 5 m up to 40 m, and different hull types. The datasets are organised in a similar manner to the KITTI datasets, which can be easily applied to the well-developed point cloud detection networks. Remarkably, our methods achieve an overall detection accuracy of 74.1\% in the collected datasets. The proposed framework and dataset make LiDAR-based environmental perception feasible for implementation in ASVs and support development in the autonomous maritime navigation field.},
	urldate = {2025-03-15},
	journal = {Ocean Engineering},
	author = {Xie, Yongchang and Nanlal, Cassandra and Liu, Yuanchang},
	month = nov,
	year = {2024},
	keywords = {Deep learning, Autonomous Surface Vehicle, LiDAR-based perception, Object tracking, Ship detection},
	pages = {119288},
	file = {ScienceDirect Snapshot:/Users/dplane/Zotero/storage/CU2A8UMG/S002980182402626X.html:text/html;Xie et al_2024_Reliable LiDAR-based ship detection and tracking for Autonomous Surface.pdf:/Users/dplane/Zotero/storage/L2YDM7K3/Xie et al_2024_Reliable LiDAR-based ship detection and tracking for Autonomous Surface.pdf:application/pdf},
}

@inproceedings{chen2017,
	title = {Multi-view {3D} {Object} {Detection} {Network} for {Autonomous} {Driving}},
	url = {https://ieeexplore.ieee.org/document/8100174/?arnumber=8100174},
	doi = {10.1109/CVPR.2017.691},
	abstract = {This paper aims at high-accuracy 3D object detection in autonomous driving scenario. We propose Multi-View 3D networks (MV3D), a sensory-fusion framework that takes both LIDAR point cloud and RGB images as input and predicts oriented 3D bounding boxes. We encode the sparse 3D point cloud with a compact multi-view representation. The network is composed of two subnetworks: one for 3D object proposal generation and another for multi-view feature fusion. The proposal network generates 3D candidate boxes efficiently from the birds eye view representation of 3D point cloud. We design a deep fusion scheme to combine region-wise features from multiple views and enable interactions between intermediate layers of different paths. Experiments on the challenging KITTI benchmark show that our approach outperforms the state-of-the-art by around 25\% and 30\% AP on the tasks of 3D localization and 3D detection. In addition, for 2D detection, our approach obtains 14.9\% higher AP than the state-of-the-art on the hard data among the LIDAR-based methods.},
	urldate = {2025-03-16},
	booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Chen, Xiaozhi and Ma, Huimin and Wan, Ji and Li, Bo and Xia, Tian},
	month = jul,
	year = {2017},
	note = {ISSN: 1063-6919},
	keywords = {Laser radar, Object detection, Proposals, Three-dimensional displays, Two dimensional displays, Birds},
	pages = {6526--6534},
	file = {Chen et al_2017_Multi-view 3D Object Detection Network for Autonomous Driving.pdf:/Users/dplane/Zotero/storage/76YSMPB6/Chen et al_2017_Multi-view 3D Object Detection Network for Autonomous Driving.pdf:application/pdf;Full Text PDF:/Users/dplane/Zotero/storage/7495ACUV/Chen et al. - 2017 - Multi-view 3D Object Detection Network for Autonom.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/dplane/Zotero/storage/UX966S6R/8100174.html:text/html},
}

@article{cui2022,
	title = {Deep {Learning} for {Image} and {Point} {Cloud} {Fusion} in {Autonomous} {Driving}: {A} {Review}},
	volume = {23},
	issn = {1558-0016},
	shorttitle = {Deep {Learning} for {Image} and {Point} {Cloud} {Fusion} in {Autonomous} {Driving}},
	url = {https://ieeexplore.ieee.org/document/9380166/?arnumber=9380166},
	doi = {10.1109/TITS.2020.3023541},
	abstract = {Autonomous vehicles were experiencing rapid development in the past few years. However, achieving full autonomy is not a trivial task, due to the nature of the complex and dynamic driving environment. Therefore, autonomous vehicles are equipped with a suite of different sensors to ensure robust, accurate environmental perception. In particular, the camera-LiDAR fusion is becoming an emerging research theme. However, so far there has been no critical review that focuses on deep-learning-based camera-LiDAR fusion methods. To bridge this gap and motivate future research, this article devotes to review recent deep-learning-based data fusion approaches that leverage both image and point cloud. This review gives a brief overview of deep learning on image and point cloud data processing. Followed by in-depth reviews of camera-LiDAR fusion methods in depth completion, object detection, semantic segmentation, tracking and online cross-sensor calibration, which are organized based on their respective fusion levels. Furthermore, we compare these methods on publicly available datasets. Finally, we identified gaps and over-looked challenges between current academic researches and real-world applications. Based on these observations, we provide our insights and point out promising research directions.},
	number = {2},
	urldate = {2025-03-16},
	journal = {IEEE Transactions on Intelligent Transportation Systems},
	author = {Cui, Yaodong and Chen, Ren and Chu, Wenbo and Chen, Long and Tian, Daxin and Li, Ying and Cao, Dongpu},
	month = feb,
	year = {2022},
	note = {Conference Name: IEEE Transactions on Intelligent Transportation Systems},
	keywords = {Deep learning, Feature extraction, deep learning, Laser radar, object detection, sensor fusion, Three-dimensional displays, semantic segmentation, Semantics, Camera-LiDAR fusion, Convolution, depth completion, Geometry, tracking},
	pages = {722--739},
	file = {Cui et al_2022_Deep Learning for Image and Point Cloud Fusion in Autonomous Driving.pdf:/Users/dplane/Zotero/storage/YR2LPNE4/Cui et al_2022_Deep Learning for Image and Point Cloud Fusion in Autonomous Driving.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/dplane/Zotero/storage/ZBD64QLB/9380166.html:text/html},
}

@article{feng2021,
	title = {Deep {Multi}-{Modal} {Object} {Detection} and {Semantic} {Segmentation} for {Autonomous} {Driving}: {Datasets}, {Methods}, and {Challenges}},
	volume = {22},
	issn = {1558-0016},
	shorttitle = {Deep {Multi}-{Modal} {Object} {Detection} and {Semantic} {Segmentation} for {Autonomous} {Driving}},
	url = {https://ieeexplore.ieee.org/document/9000872/?arnumber=9000872},
	doi = {10.1109/TITS.2020.2972974},
	abstract = {Recent advancements in perception for autonomous driving are driven by deep learning. In order to achieve robust and accurate scene understanding, autonomous vehicles are usually equipped with different sensors (e.g. cameras, LiDARs, Radars), and multiple sensing modalities can be fused to exploit their complementary properties. In this context, many methods have been proposed for deep multi-modal perception problems. However, there is no general guideline for network architecture design, and questions of “what to fuse”, “when to fuse”, and “how to fuse” remain open. This review paper attempts to systematically summarize methodologies and discuss challenges for deep multi-modal object detection and semantic segmentation in autonomous driving. To this end, we first provide an overview of on-board sensors on test vehicles, open datasets, and background information for object detection and semantic segmentation in autonomous driving research. We then summarize the fusion methodologies and discuss challenges and open questions. In the appendix, we provide tables that summarize topics and methods. We also provide an interactive online platform to navigate each reference: https://boschresearch.github.io/multimodalperception/.},
	number = {3},
	urldate = {2025-03-16},
	journal = {IEEE Transactions on Intelligent Transportation Systems},
	author = {Feng, Di and Haase-Schütz, Christian and Rosenbaum, Lars and Hertlein, Heinz and Gläser, Claudius and Timm, Fabian and Wiesbeck, Werner and Dietmayer, Klaus},
	month = mar,
	year = {2021},
	note = {Conference Name: IEEE Transactions on Intelligent Transportation Systems},
	keywords = {Autonomous vehicles, deep learning, Sensors, Cameras, Laser radar, autonomous driving, Object detection, object detection, semantic segmentation, Multi-modality, Fuses},
	pages = {1341--1360},
	file = {Feng et al_2021_Deep Multi-Modal Object Detection and Semantic Segmentation for Autonomous.pdf:/Users/dplane/Zotero/storage/UHGPBCSA/Feng et al_2021_Deep Multi-Modal Object Detection and Semantic Segmentation for Autonomous.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/dplane/Zotero/storage/MS47RZGG/9000872.html:text/html},
}

@inproceedings{geiger2012,
	title = {Are we ready for autonomous driving? {The} {KITTI} vision benchmark suite},
	shorttitle = {Are we ready for autonomous driving?},
	url = {https://ieeexplore.ieee.org/document/6248074/?arnumber=6248074},
	doi = {10.1109/CVPR.2012.6248074},
	abstract = {Today, visual recognition systems are still rarely employed in robotics applications. Perhaps one of the main reasons for this is the lack of demanding benchmarks that mimic such scenarios. In this paper, we take advantage of our autonomous driving platform to develop novel challenging benchmarks for the tasks of stereo, optical flow, visual odometry/SLAM and 3D object detection. Our recording platform is equipped with four high resolution video cameras, a Velodyne laser scanner and a state-of-the-art localization system. Our benchmarks comprise 389 stereo and optical flow image pairs, stereo visual odometry sequences of 39.2 km length, and more than 200k 3D object annotations captured in cluttered scenarios (up to 15 cars and 30 pedestrians are visible per image). Results from state-of-the-art algorithms reveal that methods ranking high on established datasets such as Middlebury perform below average when being moved outside the laboratory to the real world. Our goal is to reduce this bias by providing challenging benchmarks with novel difficulties to the computer vision community. Our benchmarks are available online at: www.cvlibs.net/datasets/kitti},
	urldate = {2025-03-16},
	booktitle = {2012 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Geiger, Andreas and Lenz, Philip and Urtasun, Raquel},
	month = jun,
	year = {2012},
	note = {ISSN: 1063-6919},
	keywords = {Cameras, Measurement, Visualization, Benchmark testing, Optical imaging, Optical sensors},
	pages = {3354--3361},
	file = {Geiger et al_2012_Are we ready for autonomous driving.pdf:/Users/dplane/Zotero/storage/Y666P9UW/Geiger et al_2012_Are we ready for autonomous driving.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/dplane/Zotero/storage/7DKIWKUK/6248074.html:text/html},
}

@article{liu2023a,
	title = {Real time object detection using {LiDAR} and camera fusion for autonomous driving},
	volume = {13},
	copyright = {2023 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-023-35170-z},
	doi = {10.1038/s41598-023-35170-z},
	abstract = {Autonomous driving has been widely applied in commercial and industrial applications, along with the upgrade of environmental awareness systems. Tasks such as path planning, trajectory tracking, and obstacle avoidance are strongly dependent on the ability to perform real-time object detection and position regression. Among the most commonly used sensors, camera provides dense semantic information but lacks accurate distance information to the target, while LiDAR provides accurate depth information but with sparse resolution. In this paper, a LiDAR-camera-based fusion algorithm is proposed to improve the above-mentioned trade-off problems by constructing a Siamese network for object detection. Raw point clouds are converted to camera planes to obtain a 2D depth image. By designing a cross feature fusion block to connect the depth and RGB processing branches, the feature-layer fusion strategy is applied to integrate multi-modality data. The proposed fusion algorithm is evaluated on the KITTI dataset. Experimental results demonstrate that our algorithm has superior performance and real-time efficiency. Remarkably, it outperforms other state-of-the-art algorithms at the most important moderate level and achieves excellent performance at the easy and hard levels.},
	language = {en},
	number = {1},
	urldate = {2025-03-16},
	journal = {Sci Rep},
	author = {Liu, Haibin and Wu, Chao and Wang, Huanjie},
	month = may,
	year = {2023},
	note = {Publisher: Nature Publishing Group},
	keywords = {Civil engineering, Mechanical engineering},
	pages = {8056},
	file = {Liu et al_2023_Real time object detection using LiDAR and camera fusion for autonomous driving.pdf:/Users/dplane/Zotero/storage/56MBH3R2/Liu et al_2023_Real time object detection using LiDAR and camera fusion for autonomous driving.pdf:application/pdf},
}

@inproceedings{pang2020,
	address = {Las Vegas, NV, USA},
	title = {{CLOCs}: {Camera}-{LiDAR} {Object} {Candidates} {Fusion} for {3D} {Object} {Detection}},
	shorttitle = {{CLOCs}},
	url = {https://doi.org/10.1109/IROS45743.2020.9341791},
	doi = {10.1109/IROS45743.2020.9341791},
	abstract = {There have been significant advances in neural networks for both 3D object detection using LiDAR and 2D object detection using video. However, it has been surprisingly difficult to train networks to effectively use both modalities in a way that demonstrates gain over single-modality networks. In this paper, we propose a novel Camera-LiDAR Object Candidates (CLOCs) fusion network. CLOCs fusion provides a low-complexity multi-modal fusion framework that significantly improves the performance of single-modality detectors. CLOCs operates on the combined output candidates before Non-Maximum Suppression (NMS) of any 2D and any 3D detector, and is trained to leverage their geometric and semantic consistencies to produce more accurate final 3D and 2D detection results. Our experimental evaluation on the challenging KITTI object detection benchmark, including 3D and bird's eye view metrics, shows significant improvements, especially at long distance, over the state-of-the-art fusion based methods. At time of submission, CLOCs ranks the highest among all the fusion-based methods in the official KITTI leaderboard. We will release our code upon acceptance.},
	urldate = {2025-03-16},
	booktitle = {2020 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	publisher = {IEEE Press},
	author = {Pang, Su and Morris, Daniel and Radha, Hayder},
	month = oct,
	year = {2020},
	pages = {10386--10393},
	file = {Pang et al_2020_CLOCs.pdf:/Users/dplane/Zotero/storage/H4P9JDYX/Pang et al_2020_CLOCs.pdf:application/pdf},
}

@inproceedings{shan2020,
	title = {{LIO}-{SAM}: {Tightly}-coupled {Lidar} {Inertial} {Odometry} via {Smoothing} and {Mapping}},
	shorttitle = {{LIO}-{SAM}},
	url = {https://ieeexplore.ieee.org/document/9341176},
	doi = {10.1109/IROS45743.2020.9341176},
	abstract = {We propose a framework for tightly-coupled lidar inertial odometry via smoothing and mapping, LIO-SAM, that achieves highly accurate, real-time mobile robot trajectory estimation and map-building. LIO-SAM formulates lidar-inertial odometry atop a factor graph, allowing a multitude of relative and absolute measurements, including loop closures, to be incorporated from different sources as factors into the system. The estimated motion from inertial measurement unit (IMU) pre-integration de-skews point clouds and produces an initial guess for lidar odometry optimization. The obtained lidar odometry solution is used to estimate the bias of the IMU. To ensure high performance in real-time, we marginalize old lidar scans for pose optimization, rather than matching lidar scans to a global map. Scan-matching at a local scale instead of a global scale significantly improves the real-time performance of the system, as does the selective introduction of keyframes, and an efficient sliding window approach that registers a new keyframe to a fixed-size set of prior "sub-keyframes." The proposed method is extensively evaluated on datasets gathered from three platforms over various scales and environments.},
	urldate = {2025-03-16},
	booktitle = {2020 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Shan, Tixiao and Englot, Brendan and Meyers, Drew and Wang, Wei and Ratti, Carlo and Rus, Daniela},
	month = oct,
	year = {2020},
	note = {ISSN: 2153-0866},
	keywords = {Optimization, Real-time systems, Laser radar, Trajectory, Three-dimensional displays, Registers, Smoothing methods},
	pages = {5135--5142},
	file = {IEEE Xplore Abstract Record:/Users/dplane/Zotero/storage/PM83DYPZ/9341176.html:text/html;Shan et al_2020_LIO-SAM.pdf:/Users/dplane/Zotero/storage/AREX9CDQ/Shan et al_2020_LIO-SAM.pdf:application/pdf},
}

@inproceedings{shi2019,
	title = {{PointRCNN}: {3D} {Object} {Proposal} {Generation} and {Detection} {From} {Point} {Cloud}},
	shorttitle = {{PointRCNN}},
	url = {https://openaccess.thecvf.com/content_CVPR_2019/html/Shi_PointRCNN_3D_Object_Proposal_Generation_and_Detection_From_Point_Cloud_CVPR_2019_paper.html},
	urldate = {2025-03-16},
	author = {Shi, Shaoshuai and Wang, Xiaogang and Li, Hongsheng},
	year = {2019},
	pages = {770--779},
	file = {Shi et al_2019_PointRCNN.pdf:/Users/dplane/Zotero/storage/TXCK35WL/Shi et al_2019_PointRCNN.pdf:application/pdf},
}

@inproceedings{tzeng2017,
	title = {Adversarial {Discriminative} {Domain} {Adaptation}},
	url = {https://openaccess.thecvf.com/content_cvpr_2017/html/Tzeng_Adversarial_Discriminative_Domain_CVPR_2017_paper.html},
	urldate = {2025-03-16},
	author = {Tzeng, Eric and Hoffman, Judy and Saenko, Kate and Darrell, Trevor},
	year = {2017},
	pages = {7167--7176},
	file = {Tzeng et al_2017_Adversarial Discriminative Domain Adaptation.pdf:/Users/dplane/Zotero/storage/5PKKI2P9/Tzeng et al_2017_Adversarial Discriminative Domain Adaptation.pdf:application/pdf},
}

@inproceedings{vaswani2017,
	title = {Attention is {All} you {Need}},
	volume = {30},
	url = {https://papers.nips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
	abstract = {The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.},
	urldate = {2025-03-16},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Ł ukasz and Polosukhin, Illia},
	year = {2017},
	keywords = {nn\_core, attention, transformers},
	file = {Vaswani et al_2017_Attention is All you Need.pdf:/Users/dplane/Zotero/storage/4RWCHYZN/Vaswani et al_2017_Attention is All you Need.pdf:application/pdf},
}

@article{li2020a,
	title = {Compact {Antenna} for {Picosatellites} {Using} a {Meandered} {Folded}-{Shorted} {Patch} {Array}},
	volume = {19},
	issn = {1548-5757},
	url = {https://ieeexplore.ieee.org/document/8957242},
	doi = {10.1109/LAWP.2020.2966088},
	abstract = {The design and operation of a compact antenna array offering circularly polarized (CP) radiation for picosat and other small satellites for communication applications is presented. The proposed array combines the folded-shorted patches (FSPs) and meandering for antenna miniaturization. Realization of CP is achieved by a compact and planar feed circuit consisting of a network of meander-shaped 90° and 180° hybrid couplers, which can provide quadrature feeding of the FSP elements and can be integrated onto the backside of the antenna ground plane, which is only 5 × 5 cm2. Agreement in terms of the simulations and measurements is observed with realized gain values of more than 3 dBic at 1.065 GHz and with an antenna size of 0.17λ0 × 0.17λ0.},
	number = {3},
	urldate = {2025-03-16},
	journal = {IEEE Antennas and Wireless Propagation Letters},
	author = {Li, Yuepei and Podilchak, Symon K. and Anagnostou, Dimitris E. and Constantinides, Constantin and Walkinshaw, Tom},
	month = mar,
	year = {2020},
	note = {Conference Name: IEEE Antennas and Wireless Propagation Letters},
	keywords = {Antenna arrays, Antenna feeds, Antenna measurements, Circular polarisation (CP), Delays, folded-shorted patch (FSP) antenna, Gain, meandering, Resonant frequency, satellite communications, sequentially rotated arrays},
	pages = {477--481},
	file = {IEEE Xplore Abstract Record:/Users/dplane/Zotero/storage/GQI48D7Q/8957242.html:text/html;Li et al_2020_Compact Antenna for Picosatellites Using a Meandered Folded-Shorted Patch Array.pdf:/Users/dplane/Zotero/storage/Q35TZ7P8/Li et al_2020_Compact Antenna for Picosatellites Using a Meandered Folded-Shorted Patch Array.pdf:application/pdf},
}

@article{xu2023,
	title = {{FusionRCNN}: {LiDAR}-{Camera} {Fusion} for {Two}-{Stage} {3D} {Object} {Detection}},
	volume = {15},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2072-4292},
	shorttitle = {{FusionRCNN}},
	url = {https://www.mdpi.com/2072-4292/15/7/1839},
	doi = {10.3390/rs15071839},
	abstract = {Accurate and reliable perception systems are essential for autonomous driving and robotics. To achieve this, 3D object detection with multi-sensors is necessary. Existing 3D detectors have significantly improved accuracy by adopting a two-stage paradigm that relies solely on LiDAR point clouds for 3D proposal refinement. However, the sparsity of point clouds, particularly for faraway points, makes it difficult for the LiDAR-only refinement module to recognize and locate objects accurately. To address this issue, we propose a novel multi-modality two-stage approach called FusionRCNN. This approach effectively and efficiently fuses point clouds and camera images in the Regions of Interest (RoI). The FusionRCNN adaptively integrates both sparse geometry information from LiDAR and dense texture information from the camera in a unified attention mechanism. Specifically, FusionRCNN first utilizes RoIPooling to obtain an image set with a unified size and gets the point set by sampling raw points within proposals in the RoI extraction step. Then, it leverages an intra-modality self-attention to enhance the domain-specific features, followed by a well-designed cross-attention to fuse the information from two modalities. FusionRCNN is fundamentally plug-and-play and supports different one-stage methods with almost no architectural changes. Extensive experiments on KITTI and Waymo benchmarks demonstrate that our method significantly boosts the performances of popular detectors. Remarkably, FusionRCNN improves the strong SECOND baseline by 6.14\% mAP on Waymo and outperforms competing two-stage approaches.},
	language = {en},
	number = {7},
	urldate = {2025-03-16},
	journal = {Remote Sensing},
	author = {Xu, Xinli and Dong, Shaocong and Xu, Tingfa and Ding, Lihe and Wang, Jie and Jiang, Peng and Song, Liqiang and Li, Jianan},
	month = jan,
	year = {2023},
	note = {Number: 7
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {3D object detection, Fusion, LiDAR-camera fusion, two-stage, RCNN},
	pages = {1839},
	file = {Xu et al_2023_FusionRCNN.pdf:/Users/dplane/Zotero/storage/YMESBATK/Xu et al_2023_FusionRCNN.pdf:application/pdf},
}

@article{yang2024,
	title = {A review of intelligent ship marine object detection based on {RGB} camera},
	volume = {18},
	copyright = {© 2023 The Authors. IET Image Processing published by John Wiley \& Sons Ltd on behalf of The Institution of Engineering and Technology.},
	issn = {1751-9667},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1049/ipr2.12959},
	doi = {10.1049/ipr2.12959},
	abstract = {The article presents a comprehensive summary of Intelligent Ship Marine Object Detection (ISMOD) based on the RGB Camera. Marine object detection plays a pivotal role in enabling intelligent ships to acquire crucial data and security assurances for autonomous navigation. Among the various detection sensors, the RGB Camera is an informative and cost-effective tool with a wide range of civil applications. In the beginning, the ISMOD metrics based on the RGB camera is analyzed from three significant aspects, namely accuracy, speed, and robustness. Subsequently, the latest research status and comparative overview are presented, encompassing three mainstream detection methods: traditional detection, deep learning detection, and sensor fusion detection. Finally, the existing challenges of ISMOD are discussed and future development trends are recommended. The results demonstrate that forthcoming development will predominantly concentrate on deep learning approaches, complemented by other techniques. It is imperative to advance detection performance in domains such as deep fusion, multi-feature extraction, multi-fusion technology, and lightweight detection architecture.},
	language = {en},
	number = {2},
	urldate = {2025-03-16},
	journal = {IET Image Processing},
	author = {Yang, Defu and Solihin, Mahmud Iwan and Zhao, Yawen and Yao, Benchun and Chen, Chaoran and Cai, Bingyu and Machmudah, Affiani},
	year = {2024},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1049/ipr2.12959},
	keywords = {learning (artificial intelligence), object detection, intelligent transportation systems, marine navigation},
	pages = {281--297},
	file = {Snapshot:/Users/dplane/Zotero/storage/59GKT6H7/ipr2.html:text/html;Yang et al_2024_A review of intelligent ship marine object detection based on RGB camera.pdf:/Users/dplane/Zotero/storage/A3QJZBQZ/Yang et al_2024_A review of intelligent ship marine object detection based on RGB camera.pdf:application/pdf},
}

@article{roriz2022,
	title = {Automotive {LiDAR} {Technology}: {A} {Survey}},
	volume = {23},
	issn = {1558-0016},
	shorttitle = {Automotive {LiDAR} {Technology}},
	url = {https://ieeexplore.ieee.org/abstract/document/9455394?casa_token=cQfg67vy5LkAAAAA:aiTkSlkwf8CAeLN5gYFeKEkk9d4JRIJBRNxMefi2mMTEX50AhiqeGyYqjUYdesOOuv36zjfWChE},
	doi = {10.1109/TITS.2021.3086804},
	abstract = {Nowadays, and more than a decade after the first steps towards autonomous driving, we keep heading to achieve fully autonomous vehicles on our roads, with LiDAR sensors being a key instrument for the success of this technology. Such advances trigger the emergence of new players in the automotive industry, and along with car manufacturers, this sector represents a multibillion-dollar market where everyone wants to take a share. To understand recent advances and technologies behind LiDAR, this article presents a survey on LiDAR sensors for the automotive industry. With this work, we show the measurement principles and imaging techniques currently being used, going through a review of commercial systems and development solutions available in the market today. Furthermore, we highlight the current and future challenges, providing insights on how both research and industry can step towards better LiDAR solutions.},
	number = {7},
	urldate = {2025-03-19},
	journal = {IEEE Transactions on Intelligent Transportation Systems},
	author = {Roriz, Ricardo and Cabral, Jorge and Gomes, Tiago},
	month = jul,
	year = {2022},
	note = {Conference Name: IEEE Transactions on Intelligent Transportation Systems},
	keywords = {Autonomous vehicles, Sensors, Laser radar, LiDAR, Three-dimensional displays, 3D imaging, Automobiles, Automotive engineering, ToF, Wavelength measurement},
	pages = {6282--6297},
	file = {IEEE Xplore Abstract Record:/Users/dplane/Zotero/storage/TL2LHVVW/9455394.html:text/html;Roriz et al_2022_Automotive LiDAR Technology.pdf:/Users/dplane/Zotero/storage/H76ZAHCL/Roriz et al_2022_Automotive LiDAR Technology.pdf:application/pdf},
}

@misc{huang2024,
	title = {Multi-modal {Sensor} {Fusion} for {Auto} {Driving} {Perception}: {A} {Survey}},
	shorttitle = {Multi-modal {Sensor} {Fusion} for {Auto} {Driving} {Perception}},
	url = {http://arxiv.org/abs/2202.02703},
	doi = {10.48550/arXiv.2202.02703},
	abstract = {Multi-modal fusion is a fundamental task for the perception of an autonomous driving system, which has recently intrigued many researchers. However, achieving a rather good performance is not an easy task due to the noisy raw data, underutilized information, and the misalignment of multi-modal sensors. In this paper, we provide a literature review of the existing multi-modal-based methods for perception tasks in autonomous driving. Generally, we make a detailed analysis including over 50 papers leveraging perception sensors including LiDAR and camera trying to solve object detection and semantic segmentation tasks. Different from traditional fusion methodology for categorizing fusion models, we propose an innovative way that divides them into two major classes, four minor classes by a more reasonable taxonomy in the view of the fusion stage. Moreover, we dive deep into the current fusion methods, focusing on the remaining problems and open-up discussions on the potential research opportunities. In conclusion, what we expect to do in this paper is to present a new taxonomy of multi-modal fusion methods for the autonomous driving perception tasks and provoke thoughts of the fusion-based techniques in the future.},
	urldate = {2025-03-19},
	publisher = {arXiv},
	author = {Huang, Keli and Shi, Botian and Li, Xiang and Li, Xin and Huang, Siyuan and Li, Yikang},
	month = dec,
	year = {2024},
	note = {arXiv:2202.02703 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Huang et al_2024_Multi-modal Sensor Fusion for Auto Driving Perception.pdf:/Users/dplane/Zotero/storage/3ADB7SG6/Huang et al_2024_Multi-modal Sensor Fusion for Auto Driving Perception.pdf:application/pdf;Snapshot:/Users/dplane/Zotero/storage/3D5U7J8Q/2202.html:text/html},
}

@article{yeong2021,
	title = {Sensor and {Sensor} {Fusion} {Technology} in {Autonomous} {Vehicles}: {A} {Review}},
	volume = {21},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1424-8220},
	shorttitle = {Sensor and {Sensor} {Fusion} {Technology} in {Autonomous} {Vehicles}},
	url = {https://www.mdpi.com/1424-8220/21/6/2140},
	doi = {10.3390/s21062140},
	abstract = {With the significant advancement of sensor and communication technology and the reliable application of obstacle detection techniques and algorithms, automated driving is becoming a pivotal technology that can revolutionize the future of transportation and mobility. Sensors are fundamental to the perception of vehicle surroundings in an automated driving system, and the use and performance of multiple integrated sensors can directly determine the safety and feasibility of automated driving vehicles. Sensor calibration is the foundation block of any autonomous system and its constituent sensors and must be performed correctly before sensor fusion and obstacle detection processes may be implemented. This paper evaluates the capabilities and the technical performance of sensors which are commonly employed in autonomous vehicles, primarily focusing on a large selection of vision cameras, LiDAR sensors, and radar sensors and the various conditions in which such sensors may operate in practice. We present an overview of the three primary categories of sensor calibration and review existing open-source calibration packages for multi-sensor calibration and their compatibility with numerous commercial sensors. We also summarize the three main approaches to sensor fusion and review current state-of-the-art multi-sensor fusion techniques and algorithms for object detection in autonomous driving applications. The current paper, therefore, provides an end-to-end review of the hardware and software methods required for sensor fusion object detection. We conclude by highlighting some of the challenges in the sensor fusion field and propose possible future research directions for automated driving systems.},
	language = {en},
	number = {6},
	urldate = {2025-03-25},
	journal = {Sensors},
	author = {Yeong, De Jong and Velasco-Hernandez, Gustavo and Barry, John and Walsh, Joseph},
	month = jan,
	year = {2021},
	note = {Number: 6
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {autonomous vehicles, sensor fusion, calibration, camera, lidar, obstacle detection, perception, radar, self-driving cars},
	pages = {2140},
	file = {Full Text PDF:/Users/dplane/Zotero/storage/ZMH3DFVN/Yeong et al. - 2021 - Sensor and Sensor Fusion Technology in Autonomous .pdf:application/pdf},
}

@article{sun,
	title = {Sparse {Voxels} {Rasterization}: {Real}-time {High}-ﬁdelity {Radiance} {Field} {Rendering}},
	language = {en},
	author = {Sun, Cheng and Choe, Jaesung and Loop, Charles and Ma, Wei-Chiu and Wang, Yu-Chiang Frank},
	file = {Sun et al. - Sparse Voxels Rasterization Real-time High-ﬁdelit.pdf:/Users/dplane/Zotero/storage/MBQ97JZD/Sun et al. - Sparse Voxels Rasterization Real-time High-ﬁdelit.pdf:application/pdf},
}

@misc{tan2020,
	title = {{EfficientDet}: {Scalable} and {Efficient} {Object} {Detection}},
	shorttitle = {{EfficientDet}},
	url = {http://arxiv.org/abs/1911.09070},
	doi = {10.48550/arXiv.1911.09070},
	abstract = {Model efficiency has become increasingly important in computer vision. In this paper, we systematically study neural network architecture design choices for object detection and propose several key optimizations to improve efficiency. First, we propose a weighted bi-directional feature pyramid network (BiFPN), which allows easy and fast multiscale feature fusion; Second, we propose a compound scaling method that uniformly scales the resolution, depth, and width for all backbone, feature network, and box/class prediction networks at the same time. Based on these optimizations and better backbones, we have developed a new family of object detectors, called EfficientDet, which consistently achieve much better efficiency than prior art across a wide spectrum of resource constraints. In particular, with single model and single-scale, our EfficientDet-D7 achieves state-of-the-art 55.1 AP on COCO test-dev with 77M parameters and 410B FLOPs, being 4x - 9x smaller and using 13x - 42x fewer FLOPs than previous detectors. Code is available at https://github.com/google/automl/tree/master/efficientdet.},
	urldate = {2025-04-24},
	publisher = {arXiv},
	author = {Tan, Mingxing and Pang, Ruoming and Le, Quoc V.},
	month = jul,
	year = {2020},
	note = {arXiv:1911.09070 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
	file = {Snapshot:/Users/dplane/Zotero/storage/Y6Y7A6FU/1911.html:text/html;Tan et al_2020_EfficientDet.pdf:/Users/dplane/Zotero/storage/XG2PXB4U/Tan et al_2020_EfficientDet.pdf:application/pdf},
}

@article{wang2020a,
	title = {Multi-{Sensor} {Fusion} in {Automated} {Driving}: {A} {Survey}},
	volume = {8},
	issn = {2169-3536},
	shorttitle = {Multi-{Sensor} {Fusion} in {Automated} {Driving}},
	url = {https://ieeexplore.ieee.org/document/8943388},
	doi = {10.1109/ACCESS.2019.2962554},
	abstract = {With the significant development of practicability in deep learning and the ultra-high-speed information transmission rate of 5G communication technology will overcome the barrier of data transmission on the Internet of Vehicles, automated driving is becoming a pivotal technology affecting the future industry. Sensors are the key to the perception of the outside world in the automated driving system and whose cooperation performance directly determines the safety of automated driving vehicles. In this survey, we mainly discuss the different strategies of multi-sensor fusion in automated driving in recent years. The performance of conventional sensors and the necessity of multi-sensor fusion are analyzed, including radar, LiDAR, camera, ultrasonic, GPS, IMU, and V2X. According to the differences in the latest studies, we divide the fusion strategies into four categories and point out some shortcomings. Sensor fusion is mainly applied for multi-target tracking and environment reconstruction. We discuss the method of establishing a motion model and data association in multi-target tracking. At the end of the paper, we analyzed the deficiencies in the current studies and put forward some suggestions for further improvement in the future. Through this investigation, we hope to analyze the current situation of multi-sensor fusion in the automated driving process and provide more efficient and reliable fusion strategies.},
	urldate = {2025-05-21},
	journal = {IEEE Access},
	author = {Wang, Zhangjing and Wu, Yu and Niu, Qingqing},
	year = {2020},
	keywords = {deep learning, Cameras, Laser radar, Sensor fusion, Automated driving, data association, environmental reconstruction, intent analysis, multi-sensor fusion strategy, multi-target tracking, Sensor phenomena and characterization, Sensor systems},
	pages = {2847--2868},
	file = {Wang et al_2020_Multi-Sensor Fusion in Automated Driving.pdf:/Users/dplane/Zotero/storage/FLRQEEBL/Wang et al_2020_Multi-Sensor Fusion in Automated Driving.pdf:application/pdf},
}

@article{fischer2022,
	title = {A {RoboStack} {Tutorial}: {Using} the {Robot} {Operating} {System} {Alongside} the {Conda} and {Jupyter} {Data} {Science} {Ecosystems}},
	volume = {29},
	issn = {1558-223X},
	shorttitle = {A {RoboStack} {Tutorial}},
	url = {https://ieeexplore.ieee.org/document/9646255},
	doi = {10.1109/MRA.2021.3128367},
	abstract = {The Robot Operating System (ROS) has become the de facto standard middleware in the robotics community [1]. ROS bundles everything, from low-level drivers to tools that transform among coordinate systems, to state-of-the-art perception and control algorithms. One of ROS’s key merits is the rich ecosystem of standardized tools to build and distribute ROS-based software.},
	number = {2},
	urldate = {2025-05-22},
	journal = {IEEE Robotics \& Automation Magazine},
	author = {Fischer, Tobias and Vollprecht, Wolf and Traversaro, Silvio and Yen, Sean and Herrero, Carlos and Milford, Michael},
	month = jun,
	year = {2022},
	keywords = {Robot kinematics, Control systems, Tutorials, Ecosystems, Middleware, Operating systems, Software engineering},
	pages = {65--74},
	file = {Fischer et al_2022_A RoboStack Tutorial.pdf:/Users/dplane/Zotero/storage/J3J5RRGW/Fischer et al_2022_A RoboStack Tutorial.pdf:application/pdf},
}

@misc{zotero-1860,
	title = {Torc {Pinpoint} {User} {Manual}},
	file = {PDF:/Users/dplane/Zotero/storage/CEUP7K2V/_.pdf:application/pdf;PinPoint User Manual v1.5 DRAFT:/Users/dplane/Zotero/storage/JRXV9LTG/PinPoint User Manual v1.5 DRAFT.pdf:application/pdf},
}

@misc{zotero-1864,
	title = {Livox {Horizon} {LiDAR} {User} {Manual}},
	file = {Livox Horizon user manual v1.0.pdf:/Users/dplane/Zotero/storage/MLFKZ8YW/Livox Horizon user manual v1.0.pdf:application/pdf},
}

@article{yang2024a,
	title = {A review of intelligent ship marine object detection based on {RGB} camera},
	volume = {18},
	copyright = {© 2023 The Authors. IET Image Processing published by John Wiley \& Sons Ltd on behalf of The Institution of Engineering and Technology.},
	issn = {1751-9667},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1049/ipr2.12959},
	doi = {10.1049/ipr2.12959},
	abstract = {The article presents a comprehensive summary of Intelligent Ship Marine Object Detection (ISMOD) based on the RGB Camera. Marine object detection plays a pivotal role in enabling intelligent ships to acquire crucial data and security assurances for autonomous navigation. Among the various detection sensors, the RGB Camera is an informative and cost-effective tool with a wide range of civil applications. In the beginning, the ISMOD metrics based on the RGB camera is analyzed from three significant aspects, namely accuracy, speed, and robustness. Subsequently, the latest research status and comparative overview are presented, encompassing three mainstream detection methods: traditional detection, deep learning detection, and sensor fusion detection. Finally, the existing challenges of ISMOD are discussed and future development trends are recommended. The results demonstrate that forthcoming development will predominantly concentrate on deep learning approaches, complemented by other techniques. It is imperative to advance detection performance in domains such as deep fusion, multi-feature extraction, multi-fusion technology, and lightweight detection architecture.},
	language = {en},
	number = {2},
	urldate = {2025-10-22},
	journal = {IET Image Processing},
	author = {Yang, Defu and Solihin, Mahmud Iwan and Zhao, Yawen and Yao, Benchun and Chen, Chaoran and Cai, Bingyu and Machmudah, Affiani},
	year = {2024},
	note = {\_eprint: https://ietresearch.onlinelibrary.wiley.com/doi/pdf/10.1049/ipr2.12959},
	keywords = {learning (artificial intelligence), object detection, intelligent transportation systems, marine navigation},
	pages = {281--297},
	file = {Full Text PDF:/Users/dplane/Zotero/storage/Q9A8SX2E/Yang et al. - 2024 - A review of intelligent ship marine object detection based on RGB camera.pdf:application/pdf;Snapshot:/Users/dplane/Zotero/storage/I3VSFXUQ/ipr2.html:text/html},
}

@inproceedings{liu2023b,
	title = {{BEVFusion}: {Multi}-{Task} {Multi}-{Sensor} {Fusion} with {Unified} {Bird}'s-{Eye} {View} {Representation}},
	shorttitle = {{BEVFusion}},
	url = {https://ieeexplore.ieee.org/document/10160968/figures},
	doi = {10.1109/ICRA48891.2023.10160968},
	abstract = {Multi-sensor fusion is essential for an accurate and reliable autonomous driving system. Recent approaches are based on point-level fusion: augmenting the LiDAR point cloud with camera features. However, the camera-to-LiDAR projection throws away the semantic density of camera features, hindering the effectiveness of such methods, especially for semantic-oriented tasks (such as 3D scene segmentation). In this paper, we propose BEVFusion, an efficient and generic multi-task multi-sensor fusion framework. It unifies multi-modal features in the shared bird's-eye view (BEV) representation space, which nicely preserves both geometric and semantic information. To achieve this, we diagnose and lift the key efficiency bottlenecks in the view transformation with optimized BEV pooling, reducing latency by more than {\textbackslash}mathbf40{\textbackslash}times. BEVFusion is fundamentally task-agnostic and seamlessly supports different 3D perception tasks with almost no architectural changes. It establishes the new state of the art on the nuScenes benchmark, achieving 1.3\% higher mAP and NDS on 3D object detection and 13.6\% higher mIoU on BEV map segmentation, with 1.9× lower computation cost. Code to reproduce our results is available at https://github.com/mit-han-lab/bevfusion.},
	urldate = {2025-10-22},
	booktitle = {2023 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Liu, Zhijian and Tang, Haotian and Amini, Alexander and Yang, Xinyu and Mao, Huizi and Rus, Daniela L. and Han, Song},
	month = may,
	year = {2023},
	keywords = {Laser radar, Object detection, Sensor fusion, Point cloud compression, Three-dimensional displays, Semantics, Multitasking},
	pages = {2774--2781},
	file = {Full Text PDF:/Users/dplane/Zotero/storage/9YL4R4M7/Liu et al. - 2023 - BEVFusion Multi-Task Multi-Sensor Fusion with Unified Bird's-Eye View Representation.pdf:application/pdf},
}

@misc{liang2022,
	title = {{BEVFusion}: {A} {Simple} and {Robust} {LiDAR}-{Camera} {Fusion} {Framework}},
	shorttitle = {{BEVFusion}},
	url = {http://arxiv.org/abs/2205.13790},
	doi = {10.48550/arXiv.2205.13790},
	abstract = {Fusing the camera and LiDAR information has become a de-facto standard for 3D object detection tasks. Current methods rely on point clouds from the LiDAR sensor as queries to leverage the feature from the image space. However, people discovered that this underlying assumption makes the current fusion framework infeasible to produce any prediction when there is a LiDAR malfunction, regardless of minor or major. This fundamentally limits the deployment capability to realistic autonomous driving scenarios. In contrast, we propose a surprisingly simple yet novel fusion framework, dubbed BEVFusion, whose camera stream does not depend on the input of LiDAR data, thus addressing the downside of previous methods. We empirically show that our framework surpasses the state-of-the-art methods under the normal training settings. Under the robustness training settings that simulate various LiDAR malfunctions, our framework significantly surpasses the state-of-the-art methods by 15.7\% to 28.9\% mAP. To the best of our knowledge, we are the first to handle realistic LiDAR malfunction and can be deployed to realistic scenarios without any post-processing procedure. The code is available at https://github.com/ADLab-AutoDrive/BEVFusion.},
	urldate = {2025-10-22},
	publisher = {arXiv},
	author = {Liang, Tingting and Xie, Hongwei and Yu, Kaicheng and Xia, Zhongyu and Lin, Zhiwei and Wang, Yongtao and Tang, Tao and Wang, Bing and Tang, Zhi},
	month = nov,
	year = {2022},
	note = {arXiv:2205.13790 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:/Users/dplane/Zotero/storage/XG6TXB53/Liang et al. - 2022 - BEVFusion A Simple and Robust LiDAR-Camera Fusion Framework.pdf:application/pdf;Snapshot:/Users/dplane/Zotero/storage/H73XKXTX/2205.html:text/html},
}

@misc{zotero-item-1877,
	title = {[2104.09224] {Multi}-{Modal} {Fusion} {Transformer} for {End}-to-{End} {Autonomous} {Driving}},
	url = {https://arxiv.org/abs/2104.09224},
	urldate = {2025-10-22},
	file = {[2104.09224] Multi-Modal Fusion Transformer for End-to-End Autonomous Driving:/Users/dplane/Zotero/storage/TW94WNS7/2104.html:text/html},
}

@misc{prakash2021,
	title = {Multi-{Modal} {Fusion} {Transformer} for {End}-to-{End} {Autonomous} {Driving}},
	url = {http://arxiv.org/abs/2104.09224},
	doi = {10.48550/arXiv.2104.09224},
	abstract = {How should representations from complementary sensors be integrated for autonomous driving? Geometry-based sensor fusion has shown great promise for perception tasks such as object detection and motion forecasting. However, for the actual driving task, the global context of the 3D scene is key, e.g. a change in traffic light state can affect the behavior of a vehicle geometrically distant from that traffic light. Geometry alone may therefore be insufficient for effectively fusing representations in end-to-end driving models. In this work, we demonstrate that imitation learning policies based on existing sensor fusion methods under-perform in the presence of a high density of dynamic agents and complex scenarios, which require global contextual reasoning, such as handling traffic oncoming from multiple directions at uncontrolled intersections. Therefore, we propose TransFuser, a novel Multi-Modal Fusion Transformer, to integrate image and LiDAR representations using attention. We experimentally validate the efficacy of our approach in urban settings involving complex scenarios using the CARLA urban driving simulator. Our approach achieves state-of-the-art driving performance while reducing collisions by 76\% compared to geometry-based fusion.},
	urldate = {2025-10-22},
	publisher = {arXiv},
	author = {Prakash, Aditya and Chitta, Kashyap and Geiger, Andreas},
	month = apr,
	year = {2021},
	note = {arXiv:2104.09224 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
	file = {Full Text PDF:/Users/dplane/Zotero/storage/3BSK2T39/Prakash et al. - 2021 - Multi-Modal Fusion Transformer for End-to-End Autonomous Driving.pdf:application/pdf;Snapshot:/Users/dplane/Zotero/storage/P8X95D2B/2104.html:text/html},
}

@article{chitta2023,
	title = {{TransFuser}: {Imitation} {With} {Transformer}-{Based} {Sensor} {Fusion} for {Autonomous} {Driving}},
	volume = {45},
	issn = {1939-3539},
	shorttitle = {{TransFuser}},
	url = {https://ieeexplore.ieee.org/document/9863660},
	doi = {10.1109/TPAMI.2022.3200245},
	abstract = {How should we integrate representations from complementary sensors for autonomous driving? Geometry-based fusion has shown promise for perception (e.g., object detection, motion forecasting). However, in the context of end-to-end driving, we find that imitation learning based on existing sensor fusion methods underperforms in complex driving scenarios with a high density of dynamic agents. Therefore, we propose TransFuser, a mechanism to integrate image and LiDAR representations using self-attention. Our approach uses transformer modules at multiple resolutions to fuse perspective view and bird's eye view feature maps. We experimentally validate its efficacy on a challenging new benchmark with long routes and dense traffic, as well as the official leaderboard of the CARLA urban driving simulator. At the time of submission, TransFuser outperforms all prior work on the CARLA leaderboard in terms of driving score by a large margin. Compared to geometry-based fusion, TransFuser reduces the average collisions per kilometer by 48\%.},
	number = {11},
	urldate = {2025-10-22},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Chitta, Kashyap and Prakash, Aditya and Jaeger, Bernhard and Yu, Zehao and Renz, Katrin and Geiger, Andreas},
	month = nov,
	year = {2023},
	keywords = {Autonomous vehicles, Cameras, Laser radar, autonomous driving, Sensor fusion, sensor fusion, Three-dimensional displays, Semantics, Attention, imitation learning, transformers, Transformers},
	pages = {12878--12895},
	file = {Full Text PDF:/Users/dplane/Zotero/storage/WKTL9E52/Chitta et al. - 2023 - TransFuser Imitation With Transformer-Based Sensor Fusion for Autonomous Driving.pdf:application/pdf},
}

@article{gu2024,
	title = {{CLFT}: {Camera}-{LiDAR} {Fusion} {Transformer} for {Semantic} {Segmentation} in {Autonomous} {Driving}},
	issn = {2379-8904, 2379-8858},
	shorttitle = {{CLFT}},
	url = {http://arxiv.org/abs/2404.17793},
	doi = {10.1109/TIV.2024.3454971},
	abstract = {Critical research about camera-and-LiDAR-based semantic object segmentation for autonomous driving significantly benefited from the recent development of deep learning. Specifically, the vision transformer is the novel ground-breaker that successfully brought the multi-head-attention mechanism to computer vision applications. Therefore, we propose a vision-transformer-based network to carry out camera-LiDAR fusion for semantic segmentation applied to autonomous driving. Our proposal uses the novel progressive-assemble strategy of vision transformers on a double-direction network and then integrates the results in a cross-fusion strategy over the transformer decoder layers. Unlike other works in the literature, our camera-LiDAR fusion transformers have been evaluated in challenging conditions like rain and low illumination, showing robust performance. The paper reports the segmentation results over the vehicle and human classes in different modalities: camera-only, LiDAR-only, and camera-LiDAR fusion. We perform coherent controlled benchmark experiments of CLFT against other networks that are also designed for semantic segmentation. The experiments aim to evaluate the performance of CLFT independently from two perspectives: multimodal sensor fusion and backbone architectures. The quantitative assessments show our CLFT networks yield an improvement of up to 10\% for challenging dark-wet conditions when comparing with Fully-Convolutional-Neural-Network-based (FCN) camera-LiDAR fusion neural network. Contrasting to the network with transformer backbone but using single modality input, the all-around improvement is 5-10\%.},
	urldate = {2025-10-22},
	journal = {IEEE Trans. Intell. Veh.},
	author = {Gu, Junyi and Bellone, Mauro and Pivoňka, Tomáš and Sell, Raivo},
	year = {2024},
	note = {arXiv:2404.17793 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
	pages = {1--12},
	file = {Full Text PDF:/Users/dplane/Zotero/storage/45TLVDER/Gu et al. - 2024 - CLFT Camera-LiDAR Fusion Transformer for Semantic Segmentation in Autonomous Driving.pdf:application/pdf;Snapshot:/Users/dplane/Zotero/storage/C2DE3T7Y/2404.html:text/html},
}

@misc{xiang2023,
	title = {{FusionViT}: {Hierarchical} {3D} {Object} {Detection} via {LiDAR}-{Camera} {Vision} {Transformer} {Fusion}},
	shorttitle = {{FusionViT}},
	url = {http://arxiv.org/abs/2311.03620},
	doi = {10.48550/arXiv.2311.03620},
	abstract = {For 3D object detection, both camera and lidar have been demonstrated to be useful sensory devices for providing complementary information about the same scenery with data representations in different modalities, e.g., 2D RGB image vs 3D point cloud. An effective representation learning and fusion of such multi-modal sensor data is necessary and critical for better 3D object detection performance. To solve the problem, in this paper, we will introduce a novel vision transformer-based 3D object detection model, namely FusionViT. Different from the existing 3D object detection approaches, FusionViT is a pure-ViT based framework, which adopts a hierarchical architecture by extending the transformer model to embed both images and point clouds for effective representation learning. Such multi-modal data embedding representations will be further fused together via a fusion vision transformer model prior to feeding the learned features to the object detection head for both detection and localization of the 3D objects in the input scenery. To demonstrate the effectiveness of FusionViT, extensive experiments have been done on real-world traffic object detection benchmark datasets KITTI and Waymo Open. Notably, our FusionViT model can achieve state-of-the-art performance and outperforms not only the existing baseline methods that merely rely on camera images or lidar point clouds, but also the latest multi-modal image-point cloud deep fusion approaches.},
	urldate = {2025-10-22},
	publisher = {arXiv},
	author = {Xiang, Xinhao and Zhang, Jiawei},
	month = nov,
	year = {2023},
	note = {arXiv:2311.03620 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Full Text PDF:/Users/dplane/Zotero/storage/8GBLEW9A/Xiang and Zhang - 2023 - FusionViT Hierarchical 3D Object Detection via LiDAR-Camera Vision Transformer Fusion.pdf:application/pdf;Snapshot:/Users/dplane/Zotero/storage/CAT4W235/2311.html:text/html},
}

@inproceedings{schneider2017,
	title = {{RegNet}: {Multimodal} sensor registration using deep neural networks},
	shorttitle = {{RegNet}},
	url = {https://ieeexplore.ieee.org/document/7995968},
	doi = {10.1109/IVS.2017.7995968},
	abstract = {In this paper, we present RegNet, the first deep convolutional neural network (CNN) to infer a 6 degrees of freedom (DOF) extrinsic calibration between multimodal sensors, exemplified using a scanning LiDAR and a monocular camera. Compared to existing approaches, RegNet casts all three conventional calibration steps (feature extraction, feature matching and global regression) into a single real-time capable CNN. Our method does not require any human interaction and bridges the gap between classical offline and target-less online calibration approaches as it provides both a stable initial estimation as well as a continuous online correction of the extrinsic parameters. During training we randomly decalibrate our system in order to train RegNet to infer the correspondence between projected depth measurements and RGB image and finally regress the extrinsic calibration. Additionally, with an iterative execution of multiple CNNs, that are trained on different magnitudes of decalibration, our approach compares favorably to state-of-the-art methods in terms of a mean calibration error of 0.28° for the rotational and 6 cm for the translation components even for large decalibrations up to 1.5 m and 20°.},
	urldate = {2025-10-22},
	booktitle = {2017 {IEEE} {Intelligent} {Vehicles} {Symposium} ({IV})},
	author = {Schneider, Nick and Piewak, Florian and Stiller, Christoph and Franke, Uwe},
	month = jun,
	year = {2017},
	keywords = {Neural networks, Feature extraction, Cameras, Laser radar, Calibration, Sensor systems},
	pages = {1803--1810},
	file = {Full Text PDF:/Users/dplane/Zotero/storage/LPDUXM52/Schneider et al. - 2017 - RegNet Multimodal sensor registration using deep neural networks.pdf:application/pdf},
}

@inproceedings{iyer2018,
	title = {{CalibNet}: {Geometrically} {Supervised} {Extrinsic} {Calibration} using {3D} {Spatial} {Transformer} {Networks}},
	shorttitle = {{CalibNet}},
	url = {http://arxiv.org/abs/1803.08181},
	doi = {10.1109/IROS.2018.8593693},
	abstract = {3D LiDARs and 2D cameras are increasingly being used alongside each other in sensor rigs for perception tasks. Before these sensors can be used to gather meaningful data, however, their extrinsics (and intrinsics) need to be accurately calibrated, as the performance of the sensor rig is extremely sensitive to these calibration parameters. A vast majority of existing calibration techniques require significant amounts of data and/or calibration targets and human effort, severely impacting their applicability in large-scale production systems. We address this gap with CalibNet: a self-supervised deep network capable of automatically estimating the 6-DoF rigid body transformation between a 3D LiDAR and a 2D camera in real-time. CalibNet alleviates the need for calibration targets, thereby resulting in significant savings in calibration efforts. During training, the network only takes as input a LiDAR point cloud, the corresponding monocular image, and the camera calibration matrix K. At train time, we do not impose direct supervision (i.e., we do not directly regress to the calibration parameters, for example). Instead, we train the network to predict calibration parameters that maximize the geometric and photometric consistency of the input images and point clouds. CalibNet learns to iteratively solve the underlying geometric problem and accurately predicts extrinsic calibration parameters for a wide range of mis-calibrations, without requiring retraining or domain adaptation. The project page is hosted at https://epiception.github.io/CalibNet},
	urldate = {2025-10-22},
	booktitle = {2018 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Iyer, Ganesh and Ram, R. Karnik and Murthy, J. Krishna and Krishna, K. Madhava},
	month = oct,
	year = {2018},
	note = {arXiv:1803.08181 [cs]},
	keywords = {Robot sensing systems, Training, Cameras, Laser radar, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics, Calibration, Three-dimensional displays, Two dimensional displays},
	pages = {1110--1117},
	file = {Full Text PDF:/Users/dplane/Zotero/storage/UPFWJRJF/Iyer et al. - 2018 - CalibNet Geometrically Supervised Extrinsic Calibration using 3D Spatial Transformer Networks.pdf:application/pdf;Preprint PDF:/Users/dplane/Zotero/storage/37GLMX8V/Iyer et al. - 2018 - CalibNet Geometrically Supervised Extrinsic Calibration using 3D Spatial Transformer Networks.pdf:application/pdf;Snapshot:/Users/dplane/Zotero/storage/4NXNDJ37/1803.html:text/html},
}

@article{yuan2020,
	title = {{RGGNet}: {Tolerance} {Aware} {LiDAR}-{Camera} {Online} {Calibration} {With} {Geometric} {Deep} {Learning} and {Generative} {Model}},
	volume = {5},
	issn = {2377-3766},
	shorttitle = {{RGGNet}},
	url = {https://ieeexplore.ieee.org/abstract/document/9206138},
	doi = {10.1109/LRA.2020.3026958},
	abstract = {Accurate LiDAR-camera online calibration is critical for modern autonomous vehicles and robot platforms. Dominant methods heavily rely on hand-crafted features, which are not scalable in practice. With the increasing popularity of deep learning (DL), a few recent efforts have demonstrated the advantages of DL for feature extraction on this task. However, their reported performances are not sufficiently satisfying yet. We believe one improvement can be the problem formulation with proper consideration of the underneath geometry. Besides, existing online calibration methods focus on optimizing the calibration error while overlooking the tolerance within the error bounds. To address the research gap, a DL-based LiDAR-camera calibration method, named as the RGGNet, is proposed by considering the Riemannian geometry and utilizing a deep generative model to learn an implicit tolerance model. When evaluated on KITTI benchmark datasets, the proposed method demonstrates superior performances to the state-of-the-art DL-based methods. The code will be publicly available at https://github.com/KleinYuan/RGGNet.},
	number = {4},
	urldate = {2025-10-22},
	journal = {IEEE Robotics and Automation Letters},
	author = {Yuan, Kaiwen and Guo, Zhenyu and Wang, Z. Jane},
	month = oct,
	year = {2020},
	keywords = {Deep learning, Feature extraction, deep learning, Cameras, Laser radar, LiDAR, Calibration, Three-dimensional displays, Geometry, Extrinsic calibration, generative model, point cloud, riemannian geometry},
	pages = {6956--6963},
	file = {Full Text PDF:/Users/dplane/Zotero/storage/AMUBA8NL/Yuan et al. - 2020 - RGGNet Tolerance Aware LiDAR-Camera Online Calibration With Geometric Deep Learning and Generative.pdf:application/pdf},
}

@inproceedings{shi2020,
	title = {{CalibRCNN}: {Calibrating} {Camera} and {LiDAR} by {Recurrent} {Convolutional} {Neural} {Network} and {Geometric} {Constraints}},
	shorttitle = {{CalibRCNN}},
	url = {https://ieeexplore.ieee.org/document/9341147},
	doi = {10.1109/IROS45743.2020.9341147},
	abstract = {In this paper, we present Calibration Recurrent Convolutional Neural Network (CalibRCNN) to infer a 6 degrees of freedom (DOF) rigid body transformation between 3D LiDAR and 2D camera. Different from the existing methods, our 3D-2D CalibRCNN not only uses the LSTM network to extract the temporal features between 3D point clouds and RGB images of consecutive frames, but also uses the geometric loss and photometric loss obtained by the interframe constraint to refine the calibration accuracy of the predicted transformation parameters. The CalibRCNN aims at inferring the correspondence between projected depth image and RGB image to learn the underlying geometry of 2D-3D calibration. Thus, the proposed calibration model achieves a good generalization ability to adapt to unknown initial calibration error ranges, and other 3D LiDAR and 2D camera pairs with different intrinsic parameters from the training dataset. Extensive experiments have demonstrated that our CalibRCNN can achieve state-of-the-art accuracy by comparison with other CNN based methods.},
	urldate = {2025-10-22},
	booktitle = {2020 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Shi, Jieying and Zhu, Ziheng and Zhang, Jianhua and Liu, Ruyu and Wang, Zhenhua and Chen, Shengyong and Liu, Honghai},
	month = oct,
	year = {2020},
	note = {ISSN: 2153-0866},
	keywords = {Feature extraction, Cameras, Laser radar, Adaptation models, Calibration, Three-dimensional displays, Two dimensional displays},
	pages = {10197--10202},
	file = {Full Text PDF:/Users/dplane/Zotero/storage/F362DZIZ/Shi et al. - 2020 - CalibRCNN Calibrating Camera and LiDAR by Recurrent Convolutional Neural Network and Geometric Cons.pdf:application/pdf},
}

@misc{xiao2024,
	title = {{CalibFormer}: {A} {Transformer}-based {Automatic} {LiDAR}-{Camera} {Calibration} {Network}},
	shorttitle = {{CalibFormer}},
	url = {http://arxiv.org/abs/2311.15241},
	doi = {10.48550/arXiv.2311.15241},
	abstract = {The fusion of LiDARs and cameras has been increasingly adopted in autonomous driving for perception tasks. The performance of such fusion-based algorithms largely depends on the accuracy of sensor calibration, which is challenging due to the difficulty of identifying common features across different data modalities. Previously, many calibration methods involved specific targets and/or manual intervention, which has proven to be cumbersome and costly. Learning-based online calibration methods have been proposed, but their performance is barely satisfactory in most cases. These methods usually suffer from issues such as sparse feature maps, unreliable cross-modality association, inaccurate calibration parameter regression, etc. In this paper, to address these issues, we propose CalibFormer, an end-to-end network for automatic LiDAR-camera calibration. We aggregate multiple layers of camera and LiDAR image features to achieve high-resolution representations. A multi-head correlation module is utilized to identify correlations between features more accurately. Lastly, we employ transformer architectures to estimate accurate calibration parameters from the correlation information. Our method achieved a mean translation error of \$0.8751 {\textbackslash}mathrm\{cm\}\$ and a mean rotation error of \$0.0562 {\textasciicircum}\{{\textbackslash}circ\}\$ on the KITTI dataset, surpassing existing state-of-the-art methods and demonstrating strong robustness, accuracy, and generalization capabilities.},
	urldate = {2025-10-22},
	publisher = {arXiv},
	author = {Xiao, Yuxuan and Li, Yao and Meng, Chengzhen and Li, Xingchen and Ji, Jianmin and Zhang, Yanyong},
	month = mar,
	year = {2024},
	note = {arXiv:2311.15241 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
	file = {Full Text PDF:/Users/dplane/Zotero/storage/8G9MAX95/Xiao et al. - 2024 - CalibFormer A Transformer-based Automatic LiDAR-Camera Calibration Network.pdf:application/pdf;Snapshot:/Users/dplane/Zotero/storage/KWYT9JCV/2311.html:text/html},
}

@article{wu2021,
	title = {This {Is} the {Way}: {Sensors} {Auto}-{Calibration} {Approach} {Based} on {Deep} {Learning} for {Self}-{Driving} {Cars}},
	volume = {21},
	issn = {1558-1748},
	shorttitle = {This {Is} the {Way}},
	url = {https://ieeexplore.ieee.org/document/9599702},
	doi = {10.1109/JSEN.2021.3124788},
	abstract = {The technological advancement of sensors and computational power has opened a new chapter in machine learning for robotics applications, especially in image classification, segmentation, object detection, and self-driving cars. One of the challenges among these applications is improving the systems perception reliability and accuracy through sensors fusion. Hence, the focus on using Stereo-cameras and LiDARs as a complement to its accurate distance measurement. However, the calibration process of the sensors is mandatory before deployment. Some may use the conventional methods, including checkerboards, specific pattern labels, or even human labeling, which is labor-intensive and repetitive as it involves doing the same calibration process every time before using. In this work, we have proposed NetCalib – an auto-calibration methodology based on a deep neural network. This research aims to utilize the power of machine learning to find the geometric transformation between stereo cameras and LiDAR automatically. From the experiments, our method manages to find the transformations from randomly sampled artificial errors and outperforms the linear optimization-based ICP algorithm. Furthermore, this research work is open-sourced to the community to fully use the advances of the methodology and initiate collaboration and innovation in this field.},
	number = {24},
	urldate = {2025-10-22},
	journal = {IEEE Sensors Journal},
	author = {Wu, Shan and Hadachi, Amnir and Vivet, Damien and Prabhakar, Yadu},
	month = dec,
	year = {2021},
	keywords = {Deep learning, Computational modeling, Sensors, Cameras, Laser radar, LiDAR, Calibration, Three-dimensional displays, self-driving cars, auto-calibration, linear optimization, sensors fusion, stereo-camera},
	pages = {27779--27788},
	file = {Full Text PDF:/Users/dplane/Zotero/storage/EA5PZ5KU/Wu et al. - 2021 - This Is the Way Sensors Auto-Calibration Approach Based on Deep Learning for Self-Driving Cars.pdf:application/pdf},
}

@misc{zotero-item-1911,
	title = {Taxonomy and {Definitions} for {Terms} {Related} to {Driving} {Automation} {Systems} for {On}-{Road} {Motor} {Vehicles} {J3016}\_202104},
	url = {https://www.sae.org/standards/j3016_202104-taxonomy-definitions-terms-related-driving-automation-systems-road-motor-vehicles},
	urldate = {2025-10-22},
	keywords = {SAE J3016\_202104},
	file = {Taxonomy and Definitions for Terms Related to Driving Automation Systems for On-Road Motor Vehicles J3016_202104:/Users/dplane/Zotero/storage/TIFBWQ6B/j3016_202104-taxonomy-definitions-terms-related-driving-automation-systems-road-motor-vehicles.html:text/html},
}

@article{huang,
	title = {Autonomy {Levels} for {Unmanned} {Systems} ({ALFUS}) {Framework} {Volume} {I}: {Terminology} {Version} 1.},
	language = {en},
	journal = {NIST Special Publication},
	author = {Huang, Hui-Min},
	file = {PDF:/Users/dplane/Zotero/storage/JWKGNNEA/Huang - Autonomy Levels for Unmanned Systems (ALFUS) Framework Volume I Terminology Version 1..pdf:application/pdf},
}

@article{yeong2021a,
	title = {Sensor and {Sensor} {Fusion} {Technology} in {Autonomous} {Vehicles}: {A} {Review}},
	volume = {21},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1424-8220},
	shorttitle = {Sensor and {Sensor} {Fusion} {Technology} in {Autonomous} {Vehicles}},
	url = {https://www.mdpi.com/1424-8220/21/6/2140},
	doi = {10.3390/s21062140},
	abstract = {With the significant advancement of sensor and communication technology and the reliable application of obstacle detection techniques and algorithms, automated driving is becoming a pivotal technology that can revolutionize the future of transportation and mobility. Sensors are fundamental to the perception of vehicle surroundings in an automated driving system, and the use and performance of multiple integrated sensors can directly determine the safety and feasibility of automated driving vehicles. Sensor calibration is the foundation block of any autonomous system and its constituent sensors and must be performed correctly before sensor fusion and obstacle detection processes may be implemented. This paper evaluates the capabilities and the technical performance of sensors which are commonly employed in autonomous vehicles, primarily focusing on a large selection of vision cameras, LiDAR sensors, and radar sensors and the various conditions in which such sensors may operate in practice. We present an overview of the three primary categories of sensor calibration and review existing open-source calibration packages for multi-sensor calibration and their compatibility with numerous commercial sensors. We also summarize the three main approaches to sensor fusion and review current state-of-the-art multi-sensor fusion techniques and algorithms for object detection in autonomous driving applications. The current paper, therefore, provides an end-to-end review of the hardware and software methods required for sensor fusion object detection. We conclude by highlighting some of the challenges in the sensor fusion field and propose possible future research directions for automated driving systems.},
	language = {en},
	number = {6},
	urldate = {2025-10-22},
	journal = {Sensors},
	author = {Yeong, De Jong and Velasco-Hernandez, Gustavo and Barry, John and Walsh, Joseph},
	month = jan,
	year = {2021},
	note = {Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {autonomous vehicles, sensor fusion, calibration, camera, lidar, obstacle detection, perception, radar, self-driving cars},
	pages = {2140},
	file = {Full Text PDF:/Users/dplane/Zotero/storage/DDVG6N5N/Yeong et al. - 2021 - Sensor and Sensor Fusion Technology in Autonomous Vehicles A Review.pdf:application/pdf},
}

@misc{huang2024a,
	title = {Multi-modal {Sensor} {Fusion} for {Auto} {Driving} {Perception}: {A} {Survey}},
	shorttitle = {Multi-modal {Sensor} {Fusion} for {Auto} {Driving} {Perception}},
	url = {http://arxiv.org/abs/2202.02703},
	doi = {10.48550/arXiv.2202.02703},
	abstract = {Multi-modal fusion is a fundamental task for the perception of an autonomous driving system, which has recently intrigued many researchers. However, achieving a rather good performance is not an easy task due to the noisy raw data, underutilized information, and the misalignment of multi-modal sensors. In this paper, we provide a literature review of the existing multi-modal-based methods for perception tasks in autonomous driving. Generally, we make a detailed analysis including over 50 papers leveraging perception sensors including LiDAR and camera trying to solve object detection and semantic segmentation tasks. Different from traditional fusion methodology for categorizing fusion models, we propose an innovative way that divides them into two major classes, four minor classes by a more reasonable taxonomy in the view of the fusion stage. Moreover, we dive deep into the current fusion methods, focusing on the remaining problems and open-up discussions on the potential research opportunities. In conclusion, what we expect to do in this paper is to present a new taxonomy of multi-modal fusion methods for the autonomous driving perception tasks and provoke thoughts of the fusion-based techniques in the future.},
	urldate = {2025-10-22},
	publisher = {arXiv},
	author = {Huang, Keli and Shi, Botian and Li, Xiang and Li, Xin and Huang, Siyuan and Li, Yikang},
	month = dec,
	year = {2024},
	note = {arXiv:2202.02703 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:/Users/dplane/Zotero/storage/QWDNIHVE/Huang et al. - 2024 - Multi-modal Sensor Fusion for Auto Driving Perception A Survey.pdf:application/pdf;Snapshot:/Users/dplane/Zotero/storage/LZUBI5SR/2202.html:text/html},
}

@article{ptp2008,
	title = {{IEEE} {Standard} for a {Precision} {Clock} {Synchronization} {Protocol} for {Networked} {Measurement} and {Control} {Systems}},
	url = {https://ieeexplore.ieee.org/document/4579760},
	doi = {10.1109/IEEESTD.2008.4579760},
	abstract = {A protocol is provided in this standard that enables precise synchronization of clocks in measurement and control systems implemented with technologies such as network communication, local computing, and distributed objects. The protocol is applicable to systems communicating via packet networks. Heterogeneous systems are enabled that include clocks of various inherent precision, resolution, and stability to synchronize. System-wide synchronization accuracy and precision in the sub-microsecond range are supported with minimal network and local clock computing resources. Simple systems are installed and operated without requiring the management attention of users because the default behavior of the protocol allows for it.},
	urldate = {2025-10-22},
	journal = {IEEE Std 1588-2008 (Revision of IEEE Std 1588-2002)},
	month = jul,
	year = {2008},
	keywords = {1588-2008, boundary clock, Boundary Clock, clock, Clocks, distributed system, Grandmaster Clock, IEEE 1588, IEEE Standards, management, master clock, measurement and control system, Ordinary Clock, Patents, Protocols, real-time clock, security, Security, Standards, synchronization, Synchronization, synchronized clock, Trademarks, transparent clock, Transparent Clock},
	pages = {1--269},
	file = {Full Text PDF:/Users/dplane/Zotero/storage/DFF4IHWT/2008 - IEEE Standard for a Precision Clock Synchronization Protocol for Networked Measurement and Control S.pdf:application/pdf;Full Text PDF:/Users/dplane/Zotero/storage/HGT5G7MU/2020 - IEEE Standard for a Precision Clock Synchronization Protocol for Networked Measurement and Control S.pdf:application/pdf},
}

@inproceedings{liu2021,
	title = {Brief {Industry} {Paper}: {The} {Matter} of {Time} — {A} {General} and {Efficient} {System} for {Precise} {Sensor} {Synchronization} in {Robotic} {Computing}},
	shorttitle = {Brief {Industry} {Paper}},
	url = {https://ieeexplore.ieee.org/document/9470470},
	doi = {10.1109/RTAS52030.2021.00040},
	abstract = {Time synchronization is a critical task in robotic computing such as autonomous driving. In the past few years, as we developed advanced robotic applications, our synchronization system has evolved as well. In this paper, we first introduce the time synchronization problem and explain the challenges of time synchronization, especially in robotic workloads. Summarizing these challenges, we then present a general hardware synchronization system for robotic computing, which delivers high synchronization accuracy while maintaining low energy and resource consumption. The proposed hardware synchronization system is a key building block in our future robotic products.},
	urldate = {2025-10-22},
	booktitle = {2021 {IEEE} 27th {Real}-{Time} and {Embedded} {Technology} and {Applications} {Symposium} ({RTAS})},
	author = {Liu, Shaoshan and Yu, Bo and Liu, Yahui and Zhang, Kunai and Qiao, Yisong and Li, Thomas Yuang and Tang, Jie and Zhu, Yuhao},
	month = may,
	year = {2021},
	note = {ISSN: 2642-7346},
	keywords = {Task analysis, Robot sensing systems, Real-time systems, Synchronization, Hardware, Industries, Service robots},
	pages = {413--416},
	file = {Full Text PDF:/Users/dplane/Zotero/storage/9QV3A5KI/Liu et al. - 2021 - Brief Industry Paper The Matter of Time — A General and Efficient System for Precise Sensor Synchro.pdf:application/pdf},
}

@inproceedings{furgale2013,
	title = {Unified temporal and spatial calibration for multi-sensor systems},
	url = {https://ieeexplore.ieee.org/document/6696514/},
	doi = {10.1109/IROS.2013.6696514},
	abstract = {In order to increase accuracy and robustness in state estimation for robotics, a growing number of applications rely on data from multiple complementary sensors. For the best performance in sensor fusion, these different sensors must be spatially and temporally registered with respect to each other. To this end, a number of approaches have been developed to estimate these system parameters in a two stage process, first estimating the time offset and subsequently solving for the spatial transformation between sensors. In this work, we present on a novel framework for jointly estimating the temporal offset between measurements of different sensors and their spatial displacements with respect to each other. The approach is enabled by continuous-time batch estimation and extends previous work by seamlessly incorporating time offsets within the rigorous theoretical framework of maximum likelihood estimation. Experimental results for a camera to inertial measurement unit (IMU) calibration prove the ability of this framework to accurately estimate time offsets up to a fraction of the smallest measurement period.},
	urldate = {2025-10-22},
	booktitle = {2013 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems}},
	author = {Furgale, Paul and Rehder, Joern and Siegwart, Roland},
	month = nov,
	year = {2013},
	note = {ISSN: 2153-0866},
	keywords = {Estimation, Splines (mathematics), Sensors, Cameras, Time measurement, Calibration, Measurement uncertainty},
	pages = {1280--1286},
	file = {Full Text PDF:/Users/dplane/Zotero/storage/IIVABFGT/Furgale et al. - 2013 - Unified temporal and spatial calibration for multi-sensor systems.pdf:application/pdf},
}

@inproceedings{zhou2018,
	address = {Salt Lake City, UT, USA},
	title = {{VoxelNet}: {End}-to-{End} {Learning} for {Point} {Cloud} {Based} {3D} {Object} {Detection}},
	isbn = {978-1-5386-6420-9},
	shorttitle = {{VoxelNet}},
	url = {https://ieeexplore.ieee.org/document/8578570/},
	doi = {10.1109/CVPR.2018.00472},
	abstract = {Accurate detection of objects in 3D point clouds is a central problem in many applications, such as autonomous navigation, housekeeping robots, and augmented/virtual reality. To interface a highly sparse LiDAR point cloud with a region proposal network (RPN), most existing efforts have focused on hand-crafted feature representations, for example, a bird’s eye view projection. In this work, we remove the need of manual feature engineering for 3D point clouds and propose VoxelNet, a generic 3D detection network that uniﬁes feature extraction and bounding box prediction into a single stage, end-to-end trainable deep network. Speciﬁcally, VoxelNet divides a point cloud into equally spaced 3D voxels and transforms a group of points within each voxel into a uniﬁed feature representation through the newly introduced voxel feature encoding (VFE) layer. In this way, the point cloud is encoded as a descriptive volumetric representation, which is then connected to a RPN to generate detections. Experiments on the KITTI car detection benchmark show that VoxelNet outperforms the state-of-the-art LiDAR based 3D detection methods by a large margin. Furthermore, our network learns an effective discriminative representation of objects with various geometries, leading to encouraging results in 3D detection of pedestrians and cyclists, based on only LiDAR.},
	language = {en},
	urldate = {2025-10-22},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Zhou, Yin and Tuzel, Oncel},
	month = jun,
	year = {2018},
	pages = {4490--4499},
	file = {PDF:/Users/dplane/Zotero/storage/RPVEU2JM/Zhou and Tuzel - 2018 - VoxelNet End-to-End Learning for Point Cloud Based 3D Object Detection.pdf:application/pdf},
}

@article{bae2023,
	title = {Survey on the {Developments} of {Unmanned} {Marine} {Vehicles}: {Intelligence} and {Cooperation}},
	volume = {23},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1424-8220},
	shorttitle = {Survey on the {Developments} of {Unmanned} {Marine} {Vehicles}},
	url = {https://www.mdpi.com/1424-8220/23/10/4643},
	doi = {10.3390/s23104643},
	abstract = {With the recent development of artificial intelligence (AI) and information and communication technology, manned vehicles operated by humans used on the ground, air, and sea are evolving into unmanned vehicles (UVs) that operate without human intervention. In particular, unmanned marine vehicles (UMVs), including unmanned underwater vehicles (UUVs) and unmanned surface vehicles (USVs), have the potential to complete maritime tasks that are unachievable for manned vehicles, lower the risk of man power, raise the power required to carry out military missions, and reap huge economic benefits. The aim of this review is to identify past and current trends in UMV development and present insights into future UMV development. The review discusses the potential benefits of UMVs, including completing maritime tasks that are unachievable for manned vehicles, lowering the risk of human intervention, and increasing power for military missions and economic benefits. However, the development of UMVs has been relatively tardy compared to that of UVs used on the ground and in the air due to adverse environments for UMV operation. This review highlights the challenges in developing UMVs, particularly in adverse environments, and the need for continued advancements in communication and networking technologies, navigation and sound exploration technologies, and multivehicle mission planning technologies to improve UMV cooperation and intelligence. Furthermore, the review identifies the importance of incorporating AI and machine learning technologies in UMVs to enhance their autonomy and ability to perform complex tasks. Overall, this review provides insights into the current state and future directions for UMV development.},
	language = {en},
	number = {10},
	urldate = {2025-10-22},
	journal = {Sensors},
	author = {Bae, Inyeong and Hong, Jungpyo},
	month = jan,
	year = {2023},
	note = {Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {artificial intelligence, cooperation, swarm, unmanned marine vehicle},
	pages = {4643},
	file = {Full Text PDF:/Users/dplane/Zotero/storage/4JR8CP7N/Bae and Hong - 2023 - Survey on the Developments of Unmanned Marine Vehicles Intelligence and Cooperation.pdf:application/pdf},
}

@article{brantner2021,
	title = {Controlling {Ocean} {One}: {Human}–robot collaboration for deep-sea manipulation},
	volume = {38},
	copyright = {© 2020 Wiley Periodicals LLC},
	issn = {1556-4967},
	shorttitle = {Controlling {Ocean} {One}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/rob.21960},
	doi = {10.1002/rob.21960},
	abstract = {Deploying robots to explore venues that are inaccessible to humans, or simply inhospitable, has been a longstanding ambition of scientists, engineers, and explorers across numerous fields. The deep sea exemplifies an environment that is largely uncharted and denies human presence. Central to exploration is the capacity to deliver dexterous robotic manipulation to this unstructured environment. Unmanned underwater vehicles (UUVs) are successful in providing passive solutions for observation and mapping but currently are far from capable of delivering human-level dexterity. The ones providing manipulation typically are UUVs coupled with position-controlled hydraulic arms using disjoint controllers for navigation and manipulation that require expert operators. Ocean One is a humanoid underwater robot designed specifically for underwater manipulation. In this paper, we present Ocean One's control architecture that, through a collaboration between this humanoid robot and a human pilot, enables the deployment of dexterous robotic manipulation to the deep sea. We provide detailed descriptions of this architecture's two main components: first, a whole-body controller that creates functional autonomy by coordinating manipulation, posture, and constraint tasks, and second, a set of haptic and visual human interfaces that enable intimate interaction while avoiding micromanagement. We test the presented methods in simulation and validate them in pool experiments and in two field deployments. On its maiden mission into the Mediterranean Sea, Ocean One explored the Lune, a French naval vessel that sank in 1664 off the coast of Toulon, France. In its second expedition, Ocean One assisted human divers in investigating underwater volcanic structures at Santorini, Greece.},
	language = {en},
	number = {1},
	urldate = {2025-10-22},
	journal = {Journal of Field Robotics},
	author = {Brantner, Gerald and Khatib, Oussama},
	year = {2021},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/rob.21960},
	keywords = {control, extreme environments, human–robot interaction, marinerobotics, mobile manipulation},
	pages = {28--51},
	file = {Full Text PDF:/Users/dplane/Zotero/storage/TJYEFVWT/Brantner and Khatib - 2021 - Controlling Ocean One Human–robot collaboration for deep-sea manipulation.pdf:application/pdf;Snapshot:/Users/dplane/Zotero/storage/ENNP46MM/rob.html:text/html},
}

@article{ferreira2022,
	title = {Editorial: {Navigation} and {Perception} for {Autonomous} {Surface} {Vessels}},
	volume = {9},
	issn = {2296-9144},
	shorttitle = {Editorial},
	url = {https://pmc.ncbi.nlm.nih.gov/articles/PMC9163955/},
	doi = {10.3389/frobt.2022.918464},
	urldate = {2025-11-01},
	journal = {Front Robot AI},
	author = {Ferreira, Fausto and Quattrini Li, Alberto and Rødseth, Ørnulf J.},
	month = may,
	year = {2022},
	pmid = {35669289},
	pmcid = {PMC9163955},
	pages = {918464},
	file = {Full Text PDF:/Users/dplane/Zotero/storage/W6JPGS6K/Ferreira et al. - 2022 - Editorial Navigation and Perception for Autonomous Surface Vessels.pdf:application/pdf},
}

@article{zorlu2025,
	title = {A {Comprehensive} {Bibliometric} {Review} of {Autonomous} {Vehicle} {Research}: {Trends}, {Disciplines}, and {Future} {Directions}},
	shorttitle = {A {Comprehensive} {Bibliometric} {Review} of {Autonomous} {Vehicle} {Research}},
	url = {https://www.acadlore.com/article/MITS/2025_4_2/mits040205},
	doi = {10.56578/mits040205},
	urldate = {2025-11-01},
	author = {Zorlu, Eray Can and Çiftçi, Muhammed Eyüp and Zorlu, Eray Can and Çiftçi, Muhammed Eyüp and Aydin, Metin Mutlu},
	month = may,
	year = {2025},
	file = {Full Text PDF:/Users/dplane/Zotero/storage/Y92ZX3F2/Zorlu et al. - 2025 - A Comprehensive Bibliometric Review of Autonomous Vehicle Research Trends, Disciplines, and Future.pdf:application/pdf},
}

@article{castano-londono2024,
	title = {Evolution of {Algorithms} and {Applications} for {Unmanned} {Surface} {Vehicles} in the {Context} of {Small} {Craft}: {A} {Systematic} {Review}},
	volume = {14},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2076-3417},
	shorttitle = {Evolution of {Algorithms} and {Applications} for {Unmanned} {Surface} {Vehicles} in the {Context} of {Small} {Craft}},
	url = {https://www.mdpi.com/2076-3417/14/21/9693},
	doi = {10.3390/app14219693},
	abstract = {The development of autonomous vessels and unmanned surface vehicles (USVs) has generated great interest in the scientific community due to their potential and advantages for use in various environments and applications. Several literature review papers have been produced from different perspectives, contributing to a better understanding of the topic and to the analysis of advances, challenges, and trends. We hypothesize that the greatest attention has been focused on the development of high-impact applications in the maritime sector. Additionally, we depart from the need to investigate the potential and advances of USVs in fluvial environments, which involve particular operating conditions, where there are different socio-environmental conditions and restrictions in terms of access to conventional energy sources and communication systems. In this sense, the main objective of this work is to study USVs in the particular context of small craft. The search for records was conducted in Scopus and Web of Science databases, covering studies published from 2000 to 16 May 2024. The methodology employed was based on the PRISMA 2020 guidelines, which is a widely recognized protocol that ensures quality and rigor in systematic reviews and bibliometric analyses. To optimize the data collection and selection process, the semaphore technique was additionally implemented, allowing for an efficient categorization of the studies found. This combined methodological approach facilitated a systematic and transparent evaluation of the literature. This study was developed based on three research questions about the evolution of research topics, areas of application, and types of algorithms related to USVs. The study of the evolution of works on USVs was carried out based on the results of the meta-analysis generated with the Bibliometrix tool. The study of applications and developments was carried out based on information obtained from the papers for six study categories: application environment, level of autonomy, application area, algorithm typology, methods, and electronic devices used. For each of the 387 papers identified in the databases, labeling was performed for the 359 screened papers with six study categories according to the availability of information in the title and abstract. In the categories application sector, autonomy level, application area and algorithm type/task, it was identified that most studies are oriented toward the maritime sector, the developments to achieve full autonomy for USVs, the development of designs or algorithms at the modeling and simulation level, and the development and implementation of algorithms for the GNC subsystems. Nevertheless, this research has revealed a much wider range of environments and applications beyond maritime, military, and commercial sectors. In addition, from the mapping of the types of algorithms used in the GNC architecture, the study provides information that can be used to guide the design of the subsystems that enable USV autonomy for civilian use in restricted environments.},
	language = {en},
	number = {21},
	urldate = {2025-11-01},
	journal = {Applied Sciences},
	author = {Castano-Londono, Luis and Marrugo Llorente, Stefany del Pilar and Paipa-Sanabria, Edwin and Orozco-Lopez, María Belén and Fuentes Montaña, David Ignacio and Gonzalez Montoya, Daniel},
	month = jan,
	year = {2024},
	note = {Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {autonomy, bibliometric analysis, GNC architecture, unmanned surface vehicles},
	pages = {9693},
	file = {Full Text PDF:/Users/dplane/Zotero/storage/ILBC4I3S/Castano-Londono et al. - 2024 - Evolution of Algorithms and Applications for Unmanned Surface Vehicles in the Context of Small Craft.pdf:application/pdf},
}

@article{xue2025,
	title = {A bibliometric analysis of the development of autonomous ships in inland waterway transport},
	volume = {12},
	issn = {2296-7745},
	url = {https://www.frontiersin.org/journals/marine-science/articles/10.3389/fmars.2025.1624596/full},
	doi = {10.3389/fmars.2025.1624596},
	abstract = {Inland waterway transport (IWT) plays a critical role in global logistics, offering large-capacity, long-distance transport at a lower cost. Recently, the advent of autonomous ships has promised to revolutionize efficiency and sustainability within the shipping industry. While existing research predominantly targets maritime settings, the distinct challenges of inland waterways such as fluctuating water depths, varying river currents, and confined channels demand tailored technological solutions. This study provides a thorough bibliometric analysis of autonomous ships in inland waterway transport, based on 163 publications from the Web of Science (WoS) core collection. This study identifies key technological milestones in this field and highlights the research gaps of adapting maritime autonomous ship technologies to inland waterways. The pressing need for customized solutions is also discussed. By reviewing the current landscape, this study contributes to the field as a beneficial reference for researchers, industry professionals, and policymakers, promoting the development of autonomous ship technology in inland waterways.},
	language = {English},
	urldate = {2025-11-01},
	journal = {Front. Mar. Sci.},
	author = {Xue, Jie and Li, Qianbing and Song, Yuanming and Yang, Peijie and Feng, Yuanjun and Hu, Hao},
	month = jul,
	year = {2025},
	note = {Publisher: Frontiers},
	keywords = {autonomous ship, bibliometric analysis, evolutionary trends, inland waterway transport, VOSviewer},
	file = {Full Text PDF:/Users/dplane/Zotero/storage/B83VP59J/Xue et al. - 2025 - A bibliometric analysis of the development of autonomous ships in inland waterway transport.pdf:application/pdf},
}

@article{huang2025,
	title = {Surface {Vessels} {Detection} and {Tracking} {Method} and {Datasets} with {Multi}-{Source} {Data} {Fusion} in {Real}-{World} {Complex} {Scenarios}},
	volume = {25},
	issn = {1424-8220},
	url = {https://pmc.ncbi.nlm.nih.gov/articles/PMC11990929/},
	doi = {10.3390/s25072179},
	abstract = {Environment sensing plays an important role for the safe autonomous navigation of intelligent ships. However, the inherent limitations of sensors, such as the low frequency of the automatic identification system (AIS), blind zone of the marine radar, and lack of depth information in visible images, make it difficult to achieve accurate sensing with a single modality of sensor data. To overcome this limitation, we propose a new multi-source data fusion framework and technologies that integrate AIS, radar, and visible data. This framework leverages the complementary strengths of these different types of sensors to enhance sensing performance, especially in real complex scenarios where single-modality data are significantly affected by blind zone and adverse weather conditions. We first design a multi-stage detection and tracking method (named MSTrack). By feeding the historical fusion results back to earlier tracking stages, the proposed method identifies and refines potential missing detections from the layered detection and tracking processes of radar and visible images. Then, a cascade association matching method is proposed to realize the association between multi-source trajectories. It first performs pairwise association in a high-accuracy aligned coordinate system, followed by association in a low-accuracy coordinate system and integrated matching between multi-source data. Through these association operations, the method can effectively reduce the association errors caused by measurement noise and projection system errors. Furthermore, we develop the first multi-source fusion dataset for intelligent vessel (WHUT-MSFVessel), and validate our methods. The experimental results show that our multi-source data fusion methods significantly improve the sensing accuracy and identity consistency of tracking, achieving average MOTA scores of 0.872 and 0.938 on the radar and visible images, respectively, and IDF1 scores of 0.811 and 0.929. Additionally, the fusion accuracy reaches up to 0.9, which can provide vessels with a comprehensive perception of the navigation environment for safer navigation.},
	number = {7},
	urldate = {2025-11-01},
	journal = {Sensors (Basel)},
	author = {Huang, Wenbin and Feng, Hui and Xu, Haixiang and Liu, Xu and He, Jianhua and Gan, Langxiong and Wang, Xiaoqian and Wang, Shanshan},
	month = mar,
	year = {2025},
	pmid = {40218690},
	pmcid = {PMC11990929},
	pages = {2179},
	file = {Full Text PDF:/Users/dplane/Zotero/storage/WMQJM424/Huang et al. - 2025 - Surface Vessels Detection and Tracking Method and Datasets with Multi-Source Data Fusion in Real-Wor.pdf:application/pdf},
}

@misc{ren2016,
	title = {Faster {R}-{CNN}: {Towards} {Real}-{Time} {Object} {Detection} with {Region} {Proposal} {Networks}},
	shorttitle = {Faster {R}-{CNN}},
	url = {http://arxiv.org/abs/1506.01497},
	doi = {10.48550/arXiv.1506.01497},
	abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
	urldate = {2025-11-01},
	publisher = {arXiv},
	author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
	month = jan,
	year = {2016},
	note = {arXiv:1506.01497 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Full Text PDF:/Users/dplane/Zotero/storage/S4QNCDWE/Ren et al. - 2016 - Faster R-CNN Towards Real-Time Object Detection with Region Proposal Networks.pdf:application/pdf;Snapshot:/Users/dplane/Zotero/storage/TVDF96JN/1506.html:text/html},
}

@misc{lin2015,
	title = {Microsoft {COCO}: {Common} {Objects} in {Context}},
	shorttitle = {Microsoft {COCO}},
	url = {http://arxiv.org/abs/1405.0312},
	doi = {10.48550/arXiv.1405.0312},
	abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
	urldate = {2025-11-01},
	publisher = {arXiv},
	author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Bourdev, Lubomir and Girshick, Ross and Hays, James and Perona, Pietro and Ramanan, Deva and Zitnick, C. Lawrence and Dollár, Piotr},
	month = feb,
	year = {2015},
	note = {arXiv:1405.0312 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, COCO},
	file = {Preprint PDF:/Users/dplane/Zotero/storage/WSM6TZS7/Lin et al. - 2015 - Microsoft COCO Common Objects in Context.pdf:application/pdf;Snapshot:/Users/dplane/Zotero/storage/7DQMX4WK/1405.html:text/html},
}
