\documentclass{erauthesis}
\department{Mechanical Engineering}
\chair{Eric Coyle, Ph.D}
\dean{James Gregory, Ph.D.}
\dgc{Lon Moeller, J.D.}
\depchair{Patrick Currier, Ph.D.}
\advisortitle{Committee chair}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{array}
\usepackage{enumitem}
% \usepackage[style=authoryear]{biblatex} % or numeric, apa, etc.
% \addbibresource{Dissertation.bib}         % your Zotero export

% acronyms
\usepackage{acronym} 

\title{A STUDY IN OBJECT DETECTION AND CLASSIFICATION
PERFORMANCE BY SENSING MODALITY FOR AUTONOMOUS
SURFACE VESSELS} % the title should be included here
\author{Daniel P. Lane} 
\graduation{December}{2025}
\advisor {Eric Coyle} %Committe chair


\coadvisor{Subhradeep Roy} % If you do not have a co-advisor, delete this whole command

\committememzero{Xxxx X. Xxxxxxxxx, Ph.D.} % If you have a co-advisor, do not edit this member name
%% Enter the name of the committee members
\committememone{Patrick Currier}
\committememtwo{Monica Garcia}
\committememthree{Jianhua Liu}
\committememfour{TBD}



%\signaturepush{-2.0}									

\begin{document}

\frontmatter

\maketitle

% \makesignature
\makeatletter 
\advance\fau@frontstage by 1  % Skip signature page but maintain counter
% \makeanother
\input{acronyms}

\begin{acknowledgements}

	% \raggedright XXxxxx xxxxx xxxxx xxxxx xxxxx xxxxx xxxxx.  Xxxxx xxxxx xxxxx xxxxx xxxxx xxxxx xxxxx xxxxx xxxxx xxxxx xxxxx xxxxx xxxxx xxxxx xxxxx xxxxx xxxxx xxxxx.\\Xxxxx xxxxx xxxxx xxxxx xxxxx xxxxx xxxxx xxxxx xxxxx xxxxx xxxxx xxxxx xxxxx xxxxx xxxxx xxxxx xxxxx xxxxx xxxxx xxxxx.  Xxxxx xxxxx xxxxx xxxxx xxxxx xxxxx xxxxx xxxxx xxxxx xxxxx xxxxx xxxxx xxxxx xxxxx xxxxx.\\
    \raggedright In addition to any personal statements of acknowledgement, be sure to include any acknowledgement statements required by research sponsors.\\{[Single page limit]} 

    \raggedright This research was sponsored in part by the Department of the Navy, Office of Naval Research through ONR N00014-17-1-2492, and the Naval Engineering Education Consortium (NEEC) through grants N00174-19-1-0018 and N00174-22-1-0012, sponsored by NSWC Carderock and NUWC Keyport respectively. Any opinions, findings, conclusions, or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the Department of the Navy or Office of Naval Research.
\end{acknowledgements}

\begin{abstract}
	\raggedright Researcher: Daniel P. Lane
 \\Title: A study in object detection and classification performance by sensing modality for autonomous surface vessels \\Institution:	Embry-Riddle Aeronautical University\\Degree:	Doctor of Philosophy in Mechanical Engineering\\Year:	2025 \\
 This research addresses the critical gap in quantitative performance comparison between \ac{LiDAR} and vision-based sensing for real-time maritime object detection on autonomous surface vessels.
 Using \ac{ERAU}'s Minion platform and 2024 Maritime RobotX Challenge data, this study evaluates \ac{GB-CACHE} \ac{LiDAR} processing against \ac{YOLO} vision detection across six maritime object categories. The methodology encompasses real-time performance analysis, multi-sensor calibration, and sensor fusion for bounding box confidence integration.
 Performance metrics include precision, recall, \ac{mAP}, training requirements, and computational efficiency. 
 % Results demonstrate [key performance finding] and establish [fusion outcome]. 
 The research provides quantitative baselines for maritime sensing modality selection and validated calibration procedures enabling improved autonomous navigation in complex maritime environments.
 
 % Lorem ipsum dolor sit amet... This is a summative abstract, not just a list of topics.  Include relevant information including conclusions and recommendations.  Limit to 150 words; spell out abbreviations; citations not needed.

\end{abstract}
\pagetableofcontents
\clearpage
\listoftables					% Or \nolistoftables if there are no 
\clearpage
\listoffigures					% Or \nolistoffigures if there are no 



\mainmatter
\newpage
\chapter{Introduction}

\section{Introduction}

\subsection{Significance of Study}

The development of \acp{ASV} represents a significant technological advancement requiring sophisticated perception systems capable of detecting and classifying maritime objects with high accuracy and reliability in real-time operational environments. As \ac{ASV} technology advances toward practical deployment in commercial and research applications, understanding the comparative performance characteristics of different sensing modalities and their integration through sensor fusion methodologies becomes critical for effective system design and operational safety assurance.

The advancement of autonomous surface vessel technology has gained significant momentum through comprehensive research programs and competitive evaluation platforms such as the Maritime RobotX Challenge, where multidisciplinary teams develop and test \ac{ASV} platforms under realistic maritime operational conditions. These research platforms serve as essential testbeds for evaluating perception technologies under actual environmental constraints, providing valuable empirical insights into sensor performance characteristics, real-time processing capabilities, and complex system integration challenges that cannot be adequately assessed through simulation alone.

Research platforms provide critical opportunities for real-world validation of perception algorithms under actual maritime conditions with operational time constraints. These platforms enable comprehensive comparative analysis opportunities for evaluating different sensor modalities and advanced sensor fusion approaches under controlled yet realistic testing scenarios. Furthermore, research platforms facilitate essential technology transfer pathways from experimental research systems to operational \ac{ASV} deployments requiring robust real-time performance guarantees. Finally, these platforms enable systematic performance benchmarking that supports rigorous evaluation of detection accuracy, classification reliability, and sensor fusion effectiveness across diverse maritime operational scenarios.

Current \ac{ASV} development faces a significant knowledge gap regarding the quantitative performance characteristics of different sensing modalities and their integration methodologies in complex maritime environments. While individual sensor technologies, particularly \ac{LiDAR} systems and vision-based cameras, have been extensively studied and validated in terrestrial applications, their comparative performance characteristics and sensor fusion integration capabilities in maritime contexts lack systematic quantitative analysis with emphasis on real-time processing constraints and computational efficiency requirements.

Comprehensive performance analysis requires systematic detection accuracy comparison between \ac{LiDAR}-based systems utilizing \ac{GB-CACHE} processing and vision-based systems implementing \ac{YOLO} object detection algorithms. Additionally, rigorous classification performance evaluation across diverse maritime object types using advanced machine learning algorithms must be conducted to establish performance baselines. Assessment of training requirements for machine learning-based classification systems, particularly focusing on data efficiency and convergence characteristics, represents another critical analytical requirement. Real-time processing capabilities and computational efficiency evaluation under operational constraints must be systematically analyzed to ensure practical deployment feasibility. Moreover, sensor fusion effectiveness evaluation, specifically examining bounding box confidence integration methodologies, requires comprehensive analysis to determine optimal multi-modal processing approaches. Finally, environmental robustness evaluation across varying maritime conditions, including different weather states, lighting conditions, and sea states, must be conducted to ensure reliable operational performance.

\ac{ASV} perception systems must reliably detect and classify a diverse range of maritime objects that are critical for safe autonomous navigation, requiring robust algorithms capable of real-time processing under challenging environmental conditions. The maritime environment presents unique detection challenges due to varying lighting conditions, wave-induced platform motion, and the diverse physical characteristics of navigation-critical objects that must be accurately identified and classified. Navigation buoys, including Polyform A-2 and A-3 buoys available in various colors for channel marking and navigation guidance, represent primary detection targets requiring high accuracy classification. Regulatory markers, specifically Sur-Mark tall buoys designed for navigation reference and hazard identification, present distinct detection challenges due to their geometric characteristics and operational deployment contexts. Light towers serving as active navigation aids provide both visual and electronic guidance signals, requiring detection algorithms capable of handling variable illumination and signaling states. Various vessels, including recreational boats and commercial watercraft, represent dynamic detection targets with diverse size, shape, and motion characteristics that complicate reliable classification. Maritime infrastructure elements, including docks, piers, and other fixed navigational hazards, require robust detection capabilities to ensure safe autonomous navigation in complex harbor and coastal environments.

% \subsection{Problem Statement}
\subsection{Problem Statement: Performance Comparison Gap}

Despite the growing operational importance of autonomous surface vessels and the significant maturation of individual sensor technologies over the past decade, there exists a critical and well-documented gap in quantitative performance comparison between different sensing modalities specifically applied to maritime object detection and classification tasks. Current \ac{ASV} development efforts lack systematic analytical frameworks for evaluating how \ac{LiDAR}-based systems utilizing advanced point cloud processing algorithms perform relative to vision-based systems implementing state-of-the-art deep learning approaches when deployed in realistic maritime operational environments.

Existing research efforts in maritime object detection and classification have primarily focused on individual sensor implementations and algorithm development without conducting comprehensive comparative analysis that would inform optimal sensor selection and integration strategies for operational \ac{ASV} systems. Contemporary research demonstrates a predominant focus on \ac{LiDAR}-only implementations that emphasize point cloud processing methodologies and clustering algorithms specifically adapted for maritime environments, yet these studies typically lack comparative evaluation against alternative sensing modalities. Similarly, vision-only system research emphasizes deep learning approaches for maritime object recognition, particularly convolutional neural network architectures, but generally operates in isolation without systematic comparison to \ac{LiDAR}-based approaches. The limited cross-modal comparison studies that do exist provide insufficient quantitative performance metrics and lack standardized evaluation frameworks necessary for meaningful comparative analysis. Furthermore, there exists a notable absence of standardized evaluation frameworks specifically designed for maritime perception systems, hindering systematic comparison and technology advancement across research groups and commercial developers.

The absence of comprehensive quantitative performance analysis leaves fundamental technical and operational questions unanswered for \ac{ASV} system designers and engineers responsible for developing reliable autonomous navigation systems. Critical questions regarding detection and classification performance remain inadequately addressed in current research literature. Specifically, the comparative detection accuracy performance of \ac{LiDAR}-based systems utilizing \ac{GB-CACHE} processing versus vision-based systems implementing \ac{YOLO} algorithms for specific maritime object types requires systematic investigation. The precision and recall characteristics of each sensing modality across different object classes under varying environmental conditions need quantitative evaluation to inform sensor selection decisions. Training requirements, including data volume, computational resources, and convergence time, differ significantly between \ac{LiDAR} feature-based approaches and vision-based deep learning methodologies, yet these differences lack systematic quantification. Computational overhead associated with each sensing modality for real-time operation, including processing latency and resource utilization, requires comprehensive analysis to ensure practical deployment feasibility. Additionally, the effectiveness of sensor fusion methodologies, particularly bounding box confidence integration approaches, needs rigorous evaluation to determine optimal multi-modal processing strategies for enhanced detection performance.

% \subsection{Problem Statement}
\subsection{Problem Statement: Sensor Fusion Challenges}

Autonomous surface vessels require precise integration of multiple sensing modalities to achieve reliable object detection and classification performance that meets operational safety standards for autonomous navigation. A fundamental and persistent challenge in \ac{ASV} perception system development lies in establishing and maintaining accurate spatial and temporal calibration between \ac{LiDAR} and camera systems under dynamic maritime operational conditions that present unique environmental challenges not encountered in terrestrial applications.

Multi-sensor \ac{ASV} platforms face unique spatial calibration requirements that differ significantly from terrestrial applications due to the dynamic nature of maritime environments and the continuous mechanical stresses imposed by marine operational conditions. Environmental factors affecting calibration present ongoing challenges for maintaining sensor alignment accuracy. Platform motion induced by wave action creates continuous roll, pitch, and yaw movements that affect sensor alignment and require robust calibration maintenance strategies. Vibration and mechanical stress inherent in marine environments cause gradual calibration drift that can degrade sensor fusion performance over extended operational periods. Temperature variations in maritime environments affect sensor mounting structures and optical characteristics, potentially introducing systematic calibration errors that must be compensated through adaptive calibration procedures. Saltwater corrosion presents long-term challenges by potentially altering sensor mounting hardware characteristics over extended deployment periods, requiring regular calibration validation and maintenance protocols.

Precision requirements for effective sensor fusion establish demanding performance specifications for calibration maintenance systems that must operate reliably under dynamic maritime conditions. Sub-pixel accuracy calibration is essential for accurate \ac{LiDAR} point cloud projection onto camera images, enabling effective correlation between sensor modalities for object detection applications and ensuring that spatial relationships between sensors remain consistent across operational scenarios. Millimeter-level precision in spatial calibration is required for effective object detection correlation between \ac{LiDAR} and vision systems, particularly for small maritime objects such as navigation buoys where detection accuracy directly impacts navigation safety. Consistent calibration maintenance across varying operational conditions, including different sea states and weather conditions, requires robust calibration validation procedures that can adapt to changing environmental parameters. Real-time calibration validation capabilities are necessary for detecting calibration degradation during operation and implementing corrective measures to maintain sensor fusion performance without interrupting autonomous navigation operations.

\subsection{Limitations and Assumptions}

This research investigation is conducted using \ac{ERAU}'s Minion \ac{ASV} research platform and utilizes sensor data collected during the 2024 Maritime RobotX Challenge competition, which establishes specific operational and methodological constraints that define the scope and applicability of research findings.

Platform-specific limitations inherent in this research approach must be acknowledged and considered when interpreting results and their broader applicability. Research findings are specifically applicable to \ac{ERAU}'s Minion research platform design, including its particular sensor mounting configuration, platform dynamics characteristics, and operational capabilities, which may not directly translate to other \ac{ASV} platform designs. The competition environment context, where primary data collection occurred during RobotX challenge events, may not represent the full spectrum of maritime operational conditions encountered in commercial or military \ac{ASV} deployments. Geographic constraints imposed by conducting testing in competition and training areas introduce environmental characteristics that may not be representative of other maritime operational regions. Operational scenarios focused on RobotX challenge tasks may not encompass the complete range of potential \ac{ASV} mission requirements and environmental conditions encountered in practical autonomous vessel operations.

This research focuses on specific maritime object categories that are relevant to RobotX competition scenarios and representative of typical \ac{ASV} navigation challenges, while acknowledging that maritime environments contain additional object types not addressed in this investigation. The research addresses six primary object classes that represent critical navigation elements in maritime environments. Tall buoys, specifically Sur-Mark regulatory buoys with standardized dimensions of 39-inch height, 10-inch column diameter, and 18-inch ring diameter, represent regulatory navigation markers requiring reliable detection and classification. Polyform A-2 buoys, measuring 14.5 inches by 19.5 inches and available in red, green, blue, and black color variants, serve as channel markers and navigation references requiring color-specific classification capabilities. Polyform A-3 buoys, with dimensions of 17 inches by 23 inches and identical color availability, represent larger navigation buoys requiring robust detection across varying distances and environmental conditions. Light towers function as active navigation aids incorporating electronic and visual signaling capabilities, presenting detection challenges due to variable illumination states and complex geometric structures. Jon boats, characterized as flat-bottom chase boats utilized in competition and training scenarios, represent small vessel detection targets with distinct geometric and motion characteristics. Sailboats, including recreational sailing vessels commonly encountered in competition environments, represent larger vessel detection targets with variable configuration due to sail and rigging arrangements.

\textbf{Definitions of Terms}

\begin{itemize}[label={}]
    \item\textbf{Autonomous Surface Vessel (ASV)} An unmanned watercraft capable of independent navigation and task execution without direct human control, utilizing onboard sensors and computational systems for environmental perception and decision-making.
    
    \item\textbf{Clustering} A computational technique that groups data points with similar characteristics or spatial proximity to identify distinct objects or regions within complex datasets.

    \item\textbf{Grid-Based Clustering} A spatial data organization methodology that partitions three-dimensional point cloud data into regular grid structures to facilitate efficient clustering analysis and object identification within defined spatial regions.
    
    \item\textbf{Concave Hull} A geometric boundary that closely follows the shape of a point set by allowing inward curves, providing a more accurate representation of object boundaries compared to convex hull approaches.
    
    \item\textbf{You Only Look Once (YOLO)} A real-time object detection algorithm that processes entire images in a single forward pass through a convolutional neural network, simultaneously predicting bounding boxes and class probabilities for detected objects.
    
    \item\textbf{Sensor Fusion} The computational process of combining data from multiple sensors to produce more accurate, reliable, or comprehensive information than could be achieved using individual sensors independently.
    
    \item\textbf{Bounding Box} A rectangular region that defines the spatial boundaries of a detected object within an image or three-dimensional space, typically specified by corner coordinates or center point with width and height dimensions.
    
    \item\textbf{Confidence Integration} A methodology for combining detection results from multiple sensors by evaluating and integrating the confidence scores associated with object predictions to improve overall detection reliability.
    
    \item\textbf{Maritime RobotX Challenge} An international autonomous surface vessel competition that provides standardized testing scenarios and performance evaluation frameworks for ASV perception, navigation, and manipulation capabilities.
    
    \item\textbf{Real-time Processing} Computational processing that guarantees response within specified time constraints, typically requiring completion of detection and classification tasks within predetermined latency limits suitable for autonomous navigation safety requirements.
    
    \item\textbf{Light Detection and Ranging (LiDAR)} A remote sensing technology that uses laser pulses to measure distances and create detailed three-dimensional point cloud representations of environmental features and objects.
    
    \item\textbf{Point Cloud} A collection of data points in three-dimensional space representing the external surface of objects, typically generated by LiDAR sensors through distance measurements to environmental features.
\end{itemize}

\textbf{List of Acronyms}

\begin{itemize}[label={}]
    \item\textbf{ASV} Autonomous Surface Vessel
    \item\textbf{ERAU} Embry-Riddle Aeronautical University  
    \item\textbf{GB-CACHE} Grid-Based Clustering and Concave Hull Extraction
    \item\textbf{LiDAR} Light Detection and Ranging
    \item\textbf{RGB} Red, Green, Blue
    \item\textbf{ROS} Robot Operating System
    \item\textbf{YOLO} You Only Look Once
    \item\textbf{IoU} Intersection over Union
    \item\textbf{mAP} mean Average Precision
    \item\textbf{ROI} Region of Interest
    \item\textbf{GPS} Global Positioning System
    \item\textbf{IMU} Inertial Measurement Unit
\end{itemize}

\chapter{Review of the Relevant Literature}

% Note: All in-text citations should appear as \cite{einstein}

\acp{USV} have emerged as essential platforms capable of performing dangerous, dirty, and cumbersome tasks that exceed human capability. These vessels are pivotal in various maritime operations, including environmental monitoring, search and rescue missions, and resource exploration \cite{liebergall, eckstein2024}.% [1], [2]. 
Their ability to operate independently with minimal human intervention has significantly enhanced operational efficiency and safety at sea \cite{bai2022}.%[3].

Autonomous vehicles use a variety of sensors to perceive their surroundings but primarily rely on some combination of visual information through a camera and spatial data provided by \ac{LiDAR} \cite{yeong2021}.%[4].
Each sensing modality offers distinct advantages: visual data provides rich color and texture information, while \ac{LiDAR} delivers precise spatial measurements of the surrounding environment.
Real-time object detection methods have been developed for both sensing modalities, leveraging deep learning architectures.
Object detection with visual data often employs transfer learning on pre-trained convolutional neural networks such as ResNet \cite{he2016} and \ac{YOLO} \cite{ultralytics}.%[6].
Similarly, \ac{LiDAR}-based object detection can be performed using point-based algorithms like PointNet \cite{garcia-garcia2016}, voxel-based methods such as VoxelNet \cite{zhou2018a}, or hybrid approaches like PV-RCNN \cite{shi2021}.%[9].
Despite these advancements, each modality has inherent limitations—vision-based systems struggle with poor lighting conditions and occlusions, while \ac{LiDAR} data can be sparse and affected by water reflections.

To address these limitations, sensor fusion techniques have been explored as a means of combining the strengths of both modalities. 
Research into sensor fusion methods dates back to military applications in the 1980s and has gained significant traction in the last 15 years, particularly due to interest from the automotive industry in autonomous driving technologies. However, no unified approach has been established for optimal sensor fusion, with ongoing debates regarding the best fusion strategies (e.g., early, mid, or late fusion) and their trade-offs concerning computational efficiency and accuracy.

While research in the automotive sector has contributed significantly to sensor fusion methodologies \cite{yeong2021,clunie2021,roriz2022,cui2022,das2022,liu2023a}, direct application to maritime environments remains challenging due to fundamental environmental differences. 
Automotive environments are highly structured, with well-defined lanes, uniform object classes, and relatively predictable lighting conditions. 
In contrast, the maritime environment introduces additional complexities, including dynamic vehicle motion induced by wind and waves, variable scene density (ranging from sparse open waters to congested littoral zones), and specular reflections on the water surface that can interfere with both vision-based \cite{liu2023a} and \ac{LiDAR}-based object detection \cite{ahmed2024}.%[15]. 
These factors necessitate domain-specific adaptations of sensor fusion architectures to ensure robust real-time object detection for \acp{USV}. 
However, the lack of available maritime-specific datasets \cite{jun-hwa2022,su2023,thompson2023} creates an additional challenge.

Given these challenges, further research is needed to enhance sensor fusion methodologies for maritime applications. 
Key areas of investigation include efficient feature selection tailored to maritime object classes, the development of lightweight fusion architectures suited for real-time processing, and an evaluation of computational requirements for deployment on \ac{USV} hardware. 
Addressing these research gaps will contribute to the advancement of autonomous maritime perception, enhancing the operational capabilities of \acp{USV} in complex and dynamic environments.

\chapter{Sensing Platform}

    \section{USV Platform}

        \subsection{Sensors}
        
            \subsubsection{Perception Geometry}

We designed the Minion's sensor layout to maximize overlap between vision and \ac{LiDAR}—both in space and time—which is what you need for reliable multi-modal object detection and sensor fusion.
The geometry focuses on forward-facing coverage, which makes sense for maritime navigation since objects of interest mostly appear ahead of the vessel during transit.

Our approach combines sensors with complementary strengths while compensating for each one's limitations.
The \ac{HDR} cameras give us dense pixel-level information with rich texture and color, while \ac{LiDAR} provides precise 3D spatial measurements regardless of lighting.
By arranging the sensors so they observe the same spatial volume, we meet the basic requirement for late fusion to work properly.

We use three Leopard Imaging IMX490 \ac{HDR} cameras arranged with overlapping horizontal coverage.
Each camera has a 65-degree horizontal field of view, and we positioned them with strategic overlap to create a composite forward-facing envelope.
This setup gives us thorough visual coverage of the navigation corridor ahead while maintaining enough overlap for stereoscopic depth estimation if we need it later.
To complement the cameras, we added three Livox Horizon \ac{LiDAR} units that provide spatial coverage designed to overlap with the camera field of view.
Each Livox has an 81.7° × 25.1° (horizontal × vertical) field of view—substantially wider than the cameras in azimuth.
The three units positioned together deliver about 165° × 25.1° of combined coverage, which completely overlaps the camera system horizontally and covers roughly 77\% vertically \cite{thompson2023}.

We chose forward-facing \ac{LiDAR} sensors over traditional 360-degree rotating units because of our focus on sensor fusion performance.
Mechanically rotating systems like the Velodyne HDL-32E spread their returns across the full horizontal plane, but the Livox units concentrate sampling density within the forward camera field of view.
This gives us better point cloud density where we actually need it, improving spatial resolution for object detection.
The Livox scan pattern also works differently than traditional spinning \ac{LiDAR}.
Instead of sampling fixed vertical rings that repeat with each rotation, the Livox units trace a rosette pattern that progressively covers the entire field of view over a one-second period \cite{thompson2023}.
This non-repetitive pattern gives us much higher spatial sampling when we aggregate multiple scans over time, as we'll detail in Section 3.1.1.3.

We register all sensor measurements to a common spatial reference frame centered on the vessel.
Extrinsic calibration procedures (detailed in Section 3.2.1) establish precise geometric relationships between individual sensors.
The \ac{LiDAR} units came with factory-configured inter-sensor calibration, so all three Livox devices report points from a unified origin at the center sensor position.
A Pinpoint \ac{GPS}/\ac{INS} system tracks the platform's position and orientation, letting us transform sensor observations between the vessel frame and world coordinates.
This matters for temporal aggregation of \ac{LiDAR} observations during vessel motion, as we describe in Section 3.2.2.

The perception geometry reflects several priorities specific to our maritime research goals.
Maximum spatial overlap between vision and \ac{LiDAR} lets us directly compare detection performance and validate late fusion approaches.
The forward-facing emphasis acknowledges that maritime collision avoidance and navigation planning need detailed perception of what's ahead—360-degree coverage just isn't necessary for this work.
Concentrating \ac{LiDAR} sampling within the camera field of view gives us higher point cloud density than omnidirectional sensors with similar point budgets.
And the Livox scan pattern combined with \ac{GPS}/\ac{INS} integration lets us accumulate observations during motion, further boosting effective spatial resolution.
This sensor configuration provides the geometric foundation for our comparative object detection analysis.
The following subsections cover individual sensor specifications in detail.

            \subsubsection{HDR Camera}

We use three Leopard Imaging LI-USB30-IMX490-GW5400-GMSL2-065H \ac{HDR} cameras for the vision component of Minion's perception system.
We picked these cameras specifically because maritime environments throw some tough lighting challenges at you—bright sky, reflective water, and shadowed objects all in the same scene—and you need extended dynamic range to preserve visual detail across that whole luminance spectrum \cite{thompson2023}.

The Leopard Imaging IMX490 builds on Sony's automotive-grade image sensor, which was originally designed for advanced driver assistance systems that face similar environmental challenges.
Table 3.1 shows the camera's key specifications.

\begin{table}[h]
\centering
\caption{HDR Camera Specifications}
\begin{tabular}{ll}
\hline
\multicolumn{2}{c}{HDR Camera}\\
% HDR & Camera\\
\hline
% \textbf{Parameter} & \textbf{Value} \\
\hline
% Make & Leopard Imaging \\
Leopard Imaging & LI-USB30-IMX490-GW5400-GMSL2-065H \\
Horizontal Resolution & 2880 pixels \\
Vertical Resolution & 1860 pixels \\
Aspect Ratio & 1.55:1 \\
Maximum Frame Rate & 25 fps \\
Horizontal Field of View & 65° \\
Interface & USB 3.0 (UVC) \\
HDR Capability & 120 dB \\
\hline
\end{tabular}
\label{tab:hdr_camera_specs}
\end{table}

The 2880×1860 resolution gives us enough spatial detail for detecting objects at the ranges we care about in autonomous vessel navigation, and the 25 fps frame rate is more than adequate for typical vessel speeds and maneuvering.
When we deploy the three cameras with overlapping 65-degree fields of view, we get the wide forward-facing coverage described in Section 3.1.1.1.

The IMX490 uses a two-stage interface design that separates the camera head from image processing.
The sensor communicates via Gigabit Multimedia Serial Link 2 to a secondary box with dedicated image processing hardware.
This image processor from Leopard Imaging handles debayering, white balance, gamma correction, and \ac{HDR} tone mapping in a fixed pipeline—there aren't any user-configurable parameters \cite{thompson2023}.
The processor box outputs USB 3.0 configured as a Universal Video Class device, which means it works with standard OS video interfaces without needing proprietary drivers.
On our Ubuntu Linux setup, we access the cameras through Video for Linux version 2, the standard kernel interface for video capture.

We capture video from the \ac{HDR} cameras using GStreamer, an industry-standard pipeline-based framework for media processing.
GStreamer's modular design lets you build complex video pipelines from individual functional blocks connected in directed graphs.
The \texttt{v4l2src} element talks to the Video for Linux kernel to grab raw frames, which then flow through a pipeline with format conversion, encoding, and streaming.
Here's the problem though: standard GStreamer pipelines use timestamps relative to stream start, which doesn't work for multi-sensor fusion where you need absolute time references for temporal alignment.
We solved this by developing a custom GStreamer plugin that injects absolute system timestamps into the video stream.
Section 3.2.2.2 describes how this timestamp plugin works and the Supplemental Enhancement Information embedding method we use.

The custom plugin works as a GStreamer pad probe attached to the \texttt{v4l2src} output \cite{thompson2023}.
When each new image buffer comes from the camera, the plugin samples system time using the POSIX \texttt{gettimeofday()} function, grabbing the Unix epoch timestamp in milliseconds.
We then associate this timestamp with the video frame in one of two ways: external CSV logging (writing frame number and timestamp to a file for lookup later) or in-band embedding (putting the timestamp directly in the video bitstream as metadata).
The in-band approach (detailed in Section 3.2.2.2) works better because timing information stays with the image data through recording, playback, and network transmission—you can't accidentally separate them.
This matters for the temporal drift analysis and correction we describe in Section 3.2.2.3.

For accurate timestamps, the system clock needs to stay synchronized with the global time reference that all sensors share.
Our \ac{HDR} cameras run on the NVIDIA Jetson Xavier embedded computer, which syncs its clock via Network Time Protocol to the Atlas PC.
Atlas gets its time from the Pinpoint \ac{GPS} receiver, creating the hierarchical time distribution we detail in Section 3.2.2.1.
Before we start the video pipeline, the camera software checks that Network Time Protocol sync is established and the clock matches \ac{GPS} time.
Only after this verification do we begin streaming, which means all timestamps reflect \ac{GPS}-disciplined time from the first frame onward.
This check prevents recording data with bad timestamps that would mess up temporal alignment later.

The maritime environment shaped both our camera choice and deployment approach.
The \ac{HDR} capability handles extreme brightness differences between sky, water, and shadowed structures, while the automotive-grade sensor gives us environmental sealing and a temperature range that works outdoors.
The USB 3.0 interface lets us put the camera in weatherproof housing away from the computing hardware—the spec supports cable runs up to 5 meters.
We mounted the three cameras in individual weatherproof boxes integrated into the camera enclosure assembly (Section 3.1.2.2), which handles power distribution, environmental protection, and network connectivity while maintaining the geometric relationships we need.

The IMX490's native \ac{HDR} combines multiple exposures in a single frame period, giving us 120 dB dynamic range without sequential multi-exposure capture.
This simultaneous approach avoids motion artifacts you'd get from temporal bracketing—important when your platform moves and targets move too.
We cover image quality validation and intrinsic calibration in Section 3.2.1.1, including lens distortion characterization and camera matrix estimation.
The combination of wide dynamic range, moderate resolution, and calibrated geometry gives us the image quality we need for training and evaluating vision-based detection models in Chapter 5.

Thompson et al. \cite{thompson2023} originally developed this camera hardware and integration approach, providing a solid foundation for maritime perception work.
Our main addition was in-band timestamp embedding and temporal drift correction (Section 3.2.2), which we needed for rigorous multi-modal sensor fusion analysis.

            \subsubsection{Livox Horizon}

For spatial sensing, we use three Livox Horizon solid-state \ac{LiDAR} units.
We picked these to replace the existing Velodyne mechanical spinning sensors because they give us better point cloud density within the camera field of view, plus a non-repetitive scan pattern that improves coverage when we aggregate over time \cite{thompson2023}.

Initially we planned to use the Velodyne HDL-32E and VLP-16 sensors already on the Minion platform.
But when we analyzed point cloud density within the camera field of view, we found some fundamental problems with mechanically rotating \ac{LiDAR} for sensor fusion.
Traditional spinning sensors like the Velodyne HDL-32E use a fixed vertical array of laser emitters that rotates horizontally at constant speed.
The HDL-32E has 32 laser channels arranged vertically across about 40 degrees of elevation.
This array spins at 10 Hz, creating 32 discrete horizontal rings of returns spread across the full 360-degree plane \cite{thompson2023}.

While 360-degree coverage gives you comprehensive spatial awareness, it creates problems for camera-\ac{LiDAR} fusion.
First, sparse azimuthal sampling—on a stationary or slow-moving platform, those 32 lasers hit the same positions on every rotation, so you don't get any increase in sampling density over time.
Second, inefficient field of view use—spreading returns across 360 degrees means only a fraction lands in the forward-facing camera view.
These issues pushed us toward a different \ac{LiDAR} technology optimized for forward-facing perception with temporal aggregation.

The Livox Horizon uses a non-repetitive rosette scan pattern that progressively covers its field of view over time instead of repeatedly hitting the same spots.
It has an 81.7° × 25.1° (horizontal × vertical) field of view—much wider in azimuth than our 65-degree cameras.
We deployed three Livox units together to get combined coverage of about 165° × 25.1°, which completely overlaps the camera array horizontally and covers roughly 77\% vertically \cite{thompson2023}.
Table 3.2 shows the Livox Horizon specs.

\begin{table}[h]
\centering
\caption{LiDAR Specifications}
\begin{tabular}{ll}
\hline
\multicolumn{2}{c}{Livox Horizon}\\
\hline
% \textbf{Parameter} & \textbf{Value} \\
\hline
Model & Livox Horizon \\
Horizontal Field of View & 81.7° \\
Vertical Field of View & 25.1° \\
Range & 260 m @ 80\% reflectivity \\
Point Rate (Single Return) & 240,000 pts/sec \\
Point Rate (Dual Return) & 480,000 pts/sec \\
Range Precision & ±2 cm \\
Wavelength & 905 nm \\
Scan Pattern & Non-repetitive rosette \\
Interface & UDP (Ethernet) \\
Operating Frequency & 100 Hz \\
\hline
\end{tabular}
\label{tab:livox_horizon_specs}
\end{table}

The 905 nm near-infrared wavelength gets absorbed by water, so we get few or no returns from water surfaces—this actually helps with maritime object detection by reducing clutter from waves that would otherwise trigger false positives in clustering algorithms.

The key difference between the Livox Horizon and traditional spinning \ac{LiDAR} is the scan pattern.
A mechanically spinning sensor with N lasers produces N discrete horizontal rings that repeat identically each rotation.
For a vessel moving at typical speeds (2-5 m/s), the platform doesn't move much between rotations relative to object size, so you get near-perfect overlap of consecutive scans.
The Livox works differently—it uses a two-dimensional scanning mechanism that traces a non-repetitive rosette pattern across the field of view.
Two orthogonal mirrors operating at slightly different frequencies make the laser beam trace a complex Lissajous-like path that progressively fills the field of view without repeating \cite{thompson2023}.

The scan pattern coverage builds over time—longer integration periods give you denser spatial sampling.
At the standard 100 Hz output rate, each UDP message has about 10 milliseconds of returns.
Over a one-second aggregation, the rosette pattern gives you much higher spatial coverage than a spinning sensor's fixed ring geometry.
Figure 3.7 in \cite{thompson2023} shows this advantage by projecting one second of Livox data onto an \ac{HDR} camera image—you get dense coverage across the image with few gaps.
This density matters for the grid-based clustering we use for \ac{LiDAR} object detection in Chapter 5.

When you compare point cloud density within the camera field of view, the Livox advantage becomes clear.
A Velodyne HDL-32E in dual-return mode puts out about 1.4 million points per second spread across the full 360-degree plane.
The camera field of view covers roughly 165 degrees of that, so the effective point rate in our region of interest is:

$$\text{Effective HDL-32E rate} = 1.4 \times 10^6 \times \frac{165°}{360°} \approx 641,000 \text{ pts/sec}$$

Three Livox Horizon units in dual-return mode together produce:

$$\text{Combined Livox rate} = 3 \times 480,000 = 1.44 \times 10^6 \text{ pts/sec}$$

That's a 2.25× increase in point returns within the camera view, and this doesn't even account for the better vertical sampling from the non-repetitive pattern versus fixed 32-ring geometry \cite{thompson2023}.

The non-repetitive scan pattern really shines when you aggregate multiple scans over time, building up points over an integration window to boost spatial sampling.
On a moving platform, you need to compensate for motion during that accumulation period.
Our \ac{LiDAR} aggregation approach (detailed in Chapter 5) uses the \ac{GPS}/\ac{INS} to track platform pose over time.
We transform each \ac{LiDAR} point from sensor coordinates to a world reference frame using the platform pose at that point's timestamp.
Then we transform all accumulated points from world coordinates to the target camera image frame, which lets us overlay a dense point cloud on the image.
For this work, we picked a four-second integration period based on testing the tradeoff between point cloud density and motion blur.
Each camera image sampled at 1 Hz gets all the \ac{LiDAR} points from the preceding four seconds, transformed to the image capture time.
This gives us the spatial resolution we need for reliable detection while keeping motion compensation accurate enough for our typical vessel speeds.

The Livox Horizon sensors talk via UDP over Ethernet, sending point cloud data as binary messages at 100 Hz.
Livox provides an open-source \ac{ROS} SDK that handles network communication, message parsing, and point cloud publication to \ac{ROS} topics.
Integrating the Livox SDK into our Minion software didn't require much work—mostly network configuration and tweaking \ac{ROS} launch files.
We run the SDK on the Atlas PC, where it receives UDP messages from all three \ac{LiDAR} units simultaneously over the local network (Section 3.1.2.3).

To fuse data from multiple \ac{LiDAR} units accurately, you need to know exactly how the sensors relate geometrically.
The Livox Horizon units can store extrinsic calibration parameters onboard, letting each sensor transform its measurements to a common reference frame before transmission.
Our \ac{LiDAR}-to-\ac{LiDAR} extrinsic calibration (Section 3.2.1.3) determines the six-DOF transformation—translation and rotation—that relates each sensor's local coordinates to the designated primary sensor.
We write these parameters to non-volatile memory on each Livox, configuring them to automatically apply the transformation so all point clouds come out in the center \ac{LiDAR}'s reference frame \cite{thompson2023}.
This factory setup simplifies downstream processing—no need for software-based registration of the three point clouds.
Then we relate the unified cloud to camera coordinates through camera-to-\ac{LiDAR} extrinsic calibration (Section 3.2.1.2).

For temporal alignment of \ac{LiDAR} with camera images and \ac{GPS}/\ac{INS} pose, all \ac{LiDAR} points need timestamps referenced to a common global time base.
The Livox Horizon supports IEEE 1588 Precision Time Protocol, which gives sub-microsecond clock sync when you have a PTP master on the network.
In our setup, the NVIDIA Jetson Xavier in the camera box acts as the PTP master, syncing the three \ac{LiDAR} units.
The Jetson itself syncs to \ac{GPS} time via the Network Time Protocol hierarchy we detail in Section 3.2.2.1.
This cascaded sync means \ac{LiDAR} timestamps share the same \ac{GPS}-disciplined time reference as camera frames, giving us frame-accurate alignment for fusion.
We verify PTP sync status using the Livox Viewer software, which shows sync state and time source for each sensor.
We confirm proper PTP lock before collecting data, ensuring all timestamps are accurate.

One nice feature of the 905 nm wavelength: water absorbs it strongly.
This means minimal \ac{LiDAR} returns from water surfaces, unlike vision sensors that clearly image water texture and waves.
No water returns means less point cloud clutter and simpler object detection—we don't need to filter ground plane points for the water surface.
You can see this in point cloud visualizations overlaid on camera images: dense \ac{LiDAR} coverage on vessels, buoys, and other solid objects, but few or no returns from water.
For our grid-based clustering, this helps prevent false positives from wave crests or foam patterns.

The Livox Horizon setup gives us several capabilities we need for comparing object detection performance.
We get 2.25× more points within the camera view compared to omnidirectional spinning sensors, and the non-repetitive scan pattern boosts sampling density through temporal aggregation.
Per-point timestamps and \ac{GPS}/\ac{INS} integration let us aggregate accurately during motion, while PTP sync provides sub-microsecond timing accuracy relative to cameras and \ac{GPS}.
Factory-configured multi-sensor calibration unifies point clouds from three sensors before transmission, and the 905 nm wavelength reduces maritime clutter by rejecting water returns.
Combined with the software integration and sync infrastructure in Section 3.2, this provides what we need for rigorous \ac{LiDAR} detection evaluation and comparison with vision-based approaches in Chapters 5 and 6.

            \subsubsection{Pinpoint GPS/INS}

The Pinpoint \ac{GPS}/\ac{INS} does two critical jobs in the Minion perception system: it provides precise global time for synchronizing all sensors, and it tracks the vessel's position and orientation for correcting \ac{LiDAR} data during motion.
The \ac{GPS} receiver acts as the master clock for the entire synchronization hierarchy described in Section 3.2.2.1, while the integrated \ac{IMU} enables high-rate pose updates between \ac{GPS} fixes.

Global Navigation Satellite System signals carry highly accurate timing information by their very nature—satellite positioning works by precisely measuring signal delay from multiple satellites with synchronized atomic clocks.
The Pinpoint receiver extracts this timing and serves it over the vessel's network using Network Time Protocol.
We assigned the \ac{GPS} unit a static IP address of 201.7.90.30 on the Minion network and configured it to broadcast \ac{GPS}-disciplined time via Network Time Protocol.
The Atlas PC main computer syncs its clock to this \ac{GPS} source with typical accuracy in the tens to hundreds of nanoseconds, limited more by network latency than by \ac{GPS} accuracy itself.

This \ac{GPS} time reference flows through the synchronization hierarchy to every computing resource on the platform.
Atlas operates as a Network Time Protocol server for the NVIDIA Jetson Xavier camera computer, which then serves as a Precision Time Protocol master for the Livox Horizon \ac{LiDAR} sensors.
The cascaded approach means timestamps from all sensors—cameras, \ac{LiDAR}, and \ac{GPS}/\ac{INS}—share a common time reference traceable to \ac{GPS} time.

Beyond timekeeping, the \ac{GPS}/\ac{INS} system continuously tracks platform position and orientation, which we need for \ac{LiDAR} temporal aggregation.
As described in Section 3.1.1.3, accumulating \ac{LiDAR} points over several seconds requires compensating for vessel motion during that period.
The Pinpoint system fuses \ac{GPS} position measurements with \ac{IMU} acceleration and rotation data through an Extended Kalman Filter to produce pose estimates at rates far higher than \ac{GPS} alone could provide.
\ac{GPS} fixes arrive at 1-10 Hz depending on satellite visibility, while the \ac{IMU} runs at hundreds of hertz.
The fusion algorithm propagates pose estimates forward using \ac{IMU} integration between \ac{GPS} updates, then corrects accumulated drift when new \ac{GPS} measurements arrive.

The resulting pose solution gives us six-degree-of-freedom state estimates—three-dimensional position and three-dimensional orientation—at high enough temporal resolution to interpolate platform pose to the exact timestamp of each \ac{LiDAR} point.
This supports the transformation methodology in Chapter 5, where we transform \ac{LiDAR} points acquired at different times during motion to a common reference frame before projecting them onto camera images.

The Pinpoint \ac{GPS}/\ac{INS} connects to the Minion computing infrastructure via Ethernet, using both Network Time Protocol for time distribution and User Datagram Protocol for pose data.
\ac{ROS} driver nodes on the Atlas PC receive and parse the binary \ac{GPS}/\ac{INS} messages, publishing pose estimates as standard \ac{ROS} geometry messages.
These messages include position (latitude, longitude, altitude), orientation (roll, pitch, yaw), uncertainty estimates, and \ac{GPS} fix quality flags.
The \ac{ROS} tf2 library consumes these pose estimates to maintain the time-varying transformation tree relating sensor frames to platform and world frames, enabling automated coordinate transformation for sensor fusion.

The \ac{GPS}/\ac{INS} defines the inertial reference frame for the platform, with position in geographic coordinates (WGS84 datum) and orientation relative to true north and local gravity.
This world-referenced system serves as the common reference for transforming observations from multiple sensors acquired at different times during motion.
Transforming from sensor-local frames to the \ac{GPS}/\ac{INS} world frame requires knowing the sensor-to-platform extrinsic calibration parameters.
We determine these through calibration procedures described in Section 3.2.1.2 for cameras and Section 3.2.1.3 for \ac{LiDAR} sensors.
Once we have extrinsic parameters, the \ac{GPS}/\ac{INS} pose solution lets us transform any sensor observation to world coordinates or to any other sensor's reference frame.

Maritime environments create challenges for \ac{GPS}/\ac{INS} systems that differ from land vehicles.
Water surface reflections cause multipath interference in \ac{GPS} signals, while vessel pitch and roll motion exercises the full range of the \ac{IMU}.
The Pinpoint system was designed for marine applications and incorporates algorithms to handle these factors while maintaining accuracy during vessel maneuvering.
\ac{GPS} accuracy degrades in areas with limited satellite visibility—near tall structures, bridges, or fjord-like coastal terrain.
The \ac{IMU} provides short-term pose continuity during \ac{GPS} outages, though accumulated drift limits how long pure inertial navigation remains accurate.
For our data collection, we operated in open water with good satellite visibility, ensuring consistent \ac{GPS} fix quality throughout recording sessions.

The \ac{GPS}/\ac{INS} system enables two functions essential for multi-modal sensor fusion research.
First, \ac{GPS}-disciplined time distributed to all sensors means timestamps from cameras, \ac{LiDAR}, and \ac{GPS} itself reference the same time base, giving us frame-accurate temporal association across modalities.
Second, continuous pose estimates let us transform temporally distributed observations to common reference frames, supporting \ac{LiDAR} aggregation during motion and registration of sensor fields of view for fusion operations.
Combined with the synchronization verification and drift correction procedures in Section 3.2.2, these capabilities provide the temporal and spatial foundation for rigorous object detection performance evaluation across sensing modalities.
The \ac{GPS}/\ac{INS} thus serves not just as a navigation sensor but as critical infrastructure enabling the multi-modal perception analysis central to this research.

        \subsection{Compute and LAN}
        
    \section{Data Collection}
    
        \subsection{Calibration}
        
            \subsubsection{Camera Intrinsics}
            
            \subsubsection{Camera Extrinsics}
            
            \subsubsection{Livox Intrinsics}
            
        \subsection{Synchronization}
        
            \subsubsection{Clock Synchronization}
            
            \subsubsection{Video Pipeline}
            
            \subsubsection{Temporal Drift}
            
    \section{Data Output}

\chapter{Dataset}

\chapter{Real-time Object Detection}

\chapter{Late Fusion}

\chapter{Conclusions}

% This chapter will synthesize findings from all three research objectives:
% - Summary of comparative performance results between LiDAR and vision systems
% - Calibration and synchronization framework effectiveness
% - Real-time processing capability validation
% - Implications for ASV perception system design
% - Contribution to maritime autonomous systems knowledge

\section{Research Objective Achievement Summary}
% Placeholder for objective completion summary

\section{Performance Comparison Findings}
% Placeholder for key comparative analysis conclusions

\section{Implications for ASV System Design}
% Placeholder for practical design guidance conclusions

\chapter{Recommendations and Future Work}

% This chapter will address:
% - Recommendations for ASV perception system design based on findings
% - Sensor selection guidance for maritime applications
% - Future research directions for maritime sensor fusion
% - Technology transfer opportunities to operational systems

\section{ASV Perception System Design Recommendations}
% Placeholder for design guidance recommendations

\section{Future Research Directions}
% Placeholder for future work recommendations

\section{Technology Transfer Opportunities}
% Placeholder for practical application recommendations




% \printbibliography
\bibliographystyle{plainnat}
% \bibliography{References}
\bibliography{Dissertation}

\backmatter

\chapter{A Test of the Appendix System}

Tables of Results

\chapter{Another Test of the Appendix System}
Supplemental Figures.
\end{document}

