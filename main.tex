\documentclass{erauthesis}
\department{Mechanical Engineering}
\chair{Eric Coyle, Ph.D}
\dean{James Gregory, Ph.D.}
\dgc{Lon Moeller, J.D.}
\depchair{Patrick Currier, Ph.D.}
\advisortitle{Committee chair}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{array}
\usepackage{enumitem}
% \usepackage[style=authoryear]{biblatex} % or numeric, apa, etc.
% \addbibresource{Dissertation.bib}         % your Zotero export

% acronyms
\usepackage{acronym} 

\title{A STUDY IN OBJECT DETECTION AND CLASSIFICATION
PERFORMANCE BY SENSING MODALITY FOR AUTONOMOUS
SURFACE VESSELS} % the title should be included here
\author{Daniel P. Lane} 
\graduation{December}{2025}
\advisor {Eric Coyle} %Committe chair


\coadvisor{Subhradeep Roy} % If you do not have a co-advisor, delete this whole command

\committememzero{Xxxx X. Xxxxxxxxx, Ph.D.} % If you have a co-advisor, do not edit this member name
%% Enter the name of the committee members
\committememone{Patrick Currier}
\committememtwo{Monica Garcia}
\committememthree{Jianhua Liu}
\committememfour{TBD}



%\signaturepush{-2.0}									

\begin{document}

\frontmatter

\maketitle

% \makesignature
\makeatletter 
\advance\fau@frontstage by 1  % Skip signature page but maintain counter
% \makeanother
\input{acronyms}

\begin{acknowledgements}

	% \raggedright XXxxxx xxxxx xxxxx xxxxx xxxxx xxxxx xxxxx.  Xxxxx xxxxx xxxxx xxxxx xxxxx xxxxx xxxxx xxxxx xxxxx xxxxx xxxxx xxxxx xxxxx xxxxx xxxxx xxxxx xxxxx xxxxx.\\Xxxxx xxxxx xxxxx xxxxx xxxxx xxxxx xxxxx xxxxx xxxxx xxxxx xxxxx xxxxx xxxxx xxxxx xxxxx xxxxx xxxxx xxxxx xxxxx xxxxx.  Xxxxx xxxxx xxxxx xxxxx xxxxx xxxxx xxxxx xxxxx xxxxx xxxxx xxxxx xxxxx xxxxx xxxxx xxxxx.\\
    \raggedright In addition to any personal statements of acknowledgement, be sure to include any acknowledgement statements required by research sponsors.\\{[Single page limit]} 

    \raggedright This research was sponsored in part by the Department of the Navy, Office of Naval Research through ONR N00014-17-1-2492, and the Naval Engineering Education Consortium (NEEC) through grants N00174-19-1-0018 and N00174-22-1-0012, sponsored by NSWC Carderock and NUWC Keyport respectively. Any opinions, findings, conclusions, or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the Department of the Navy or Office of Naval Research.
\end{acknowledgements}

\begin{abstract}
	\raggedright Researcher: Daniel P. Lane
 \\Title: A study in object detection and classification performance by sensing modality for autonomous surface vessels \\Institution:	Embry-Riddle Aeronautical University\\Degree:	Doctor of Philosophy in Mechanical Engineering\\Year:	2025 \\
 This research addresses the critical gap in quantitative performance comparison between \ac{LiDAR} and vision-based sensing for real-time maritime object detection on autonomous surface vessels.
 Using \ac{ERAU}'s Minion platform and 2024 Maritime RobotX Challenge data, this study evaluates \ac{GB-CACHE} \ac{LiDAR} processing against \ac{YOLO} vision detection across six maritime object categories. The methodology encompasses real-time performance analysis, multi-sensor calibration, and sensor fusion for bounding box confidence integration.
 Performance metrics include precision, recall, \ac{mAP}, training requirements, and computational efficiency. 
 % Results demonstrate [key performance finding] and establish [fusion outcome]. 
 The research provides quantitative baselines for maritime sensing modality selection and validated calibration procedures enabling improved autonomous navigation in complex maritime environments.
 
 % Lorem ipsum dolor sit amet... This is a summative abstract, not just a list of topics.  Include relevant information including conclusions and recommendations.  Limit to 150 words; spell out abbreviations; citations not needed.

\end{abstract}
\pagetableofcontents
\clearpage
\listoftables					% Or \nolistoftables if there are no 
\clearpage
\listoffigures					% Or \nolistoffigures if there are no 



\mainmatter
\newpage
\chapter{Introduction}

\section{Introduction}

\subsection{Significance of Study}

The development of \acp{ASV} represents a significant technological advancement requiring sophisticated perception systems capable of detecting and classifying maritime objects with high accuracy and reliability in real-time operational environments. As \ac{ASV} technology advances toward practical deployment in commercial and research applications, understanding the comparative performance characteristics of different sensing modalities and their integration through sensor fusion methodologies becomes critical for effective system design and operational safety assurance.

The advancement of autonomous surface vessel technology has gained significant momentum through comprehensive research programs and competitive evaluation platforms such as the Maritime RobotX Challenge, where multidisciplinary teams develop and test \ac{ASV} platforms under realistic maritime operational conditions. These research platforms serve as essential testbeds for evaluating perception technologies under actual environmental constraints, providing valuable empirical insights into sensor performance characteristics, real-time processing capabilities, and complex system integration challenges that cannot be adequately assessed through simulation alone.

Research platforms provide critical opportunities for real-world validation of perception algorithms under actual maritime conditions with operational time constraints. These platforms enable comprehensive comparative analysis opportunities for evaluating different sensor modalities and advanced sensor fusion approaches under controlled yet realistic testing scenarios. Furthermore, research platforms facilitate essential technology transfer pathways from experimental research systems to operational \ac{ASV} deployments requiring robust real-time performance guarantees. Finally, these platforms enable systematic performance benchmarking that supports rigorous evaluation of detection accuracy, classification reliability, and sensor fusion effectiveness across diverse maritime operational scenarios.

Current \ac{ASV} development faces a significant knowledge gap regarding the quantitative performance characteristics of different sensing modalities and their integration methodologies in complex maritime environments. While individual sensor technologies, particularly \ac{LiDAR} systems and vision-based cameras, have been extensively studied and validated in terrestrial applications, their comparative performance characteristics and sensor fusion integration capabilities in maritime contexts lack systematic quantitative analysis with emphasis on real-time processing constraints and computational efficiency requirements.

Comprehensive performance analysis requires systematic detection accuracy comparison between \ac{LiDAR}-based systems utilizing \ac{GB-CACHE} processing and vision-based systems implementing \ac{YOLO} object detection algorithms. Additionally, rigorous classification performance evaluation across diverse maritime object types using advanced machine learning algorithms must be conducted to establish performance baselines. Assessment of training requirements for machine learning-based classification systems, particularly focusing on data efficiency and convergence characteristics, represents another critical analytical requirement. Real-time processing capabilities and computational efficiency evaluation under operational constraints must be systematically analyzed to ensure practical deployment feasibility. Moreover, sensor fusion effectiveness evaluation, specifically examining bounding box confidence integration methodologies, requires comprehensive analysis to determine optimal multi-modal processing approaches. Finally, environmental robustness evaluation across varying maritime conditions, including different weather states, lighting conditions, and sea states, must be conducted to ensure reliable operational performance.

\ac{ASV} perception systems must reliably detect and classify a diverse range of maritime objects that are critical for safe autonomous navigation, requiring robust algorithms capable of real-time processing under challenging environmental conditions. The maritime environment presents unique detection challenges due to varying lighting conditions, wave-induced platform motion, and the diverse physical characteristics of navigation-critical objects that must be accurately identified and classified. Navigation buoys, including Polyform A-2 and A-3 buoys available in various colors for channel marking and navigation guidance, represent primary detection targets requiring high accuracy classification. Regulatory markers, specifically Sur-Mark tall buoys designed for navigation reference and hazard identification, present distinct detection challenges due to their geometric characteristics and operational deployment contexts. Light towers serving as active navigation aids provide both visual and electronic guidance signals, requiring detection algorithms capable of handling variable illumination and signaling states. Various vessels, including recreational boats and commercial watercraft, represent dynamic detection targets with diverse size, shape, and motion characteristics that complicate reliable classification. Maritime infrastructure elements, including docks, piers, and other fixed navigational hazards, require robust detection capabilities to ensure safe autonomous navigation in complex harbor and coastal environments.

% \subsection{Problem Statement}
\subsection{Problem Statement: Performance Comparison Gap}

Despite the growing operational importance of autonomous surface vessels and the significant maturation of individual sensor technologies over the past decade, there exists a critical and well-documented gap in quantitative performance comparison between different sensing modalities specifically applied to maritime object detection and classification tasks. Current \ac{ASV} development efforts lack systematic analytical frameworks for evaluating how \ac{LiDAR}-based systems utilizing advanced point cloud processing algorithms perform relative to vision-based systems implementing state-of-the-art deep learning approaches when deployed in realistic maritime operational environments.

Existing research efforts in maritime object detection and classification have primarily focused on individual sensor implementations and algorithm development without conducting comprehensive comparative analysis that would inform optimal sensor selection and integration strategies for operational \ac{ASV} systems. Contemporary research demonstrates a predominant focus on \ac{LiDAR}-only implementations that emphasize point cloud processing methodologies and clustering algorithms specifically adapted for maritime environments, yet these studies typically lack comparative evaluation against alternative sensing modalities. Similarly, vision-only system research emphasizes deep learning approaches for maritime object recognition, particularly convolutional neural network architectures, but generally operates in isolation without systematic comparison to \ac{LiDAR}-based approaches. The limited cross-modal comparison studies that do exist provide insufficient quantitative performance metrics and lack standardized evaluation frameworks necessary for meaningful comparative analysis. Furthermore, there exists a notable absence of standardized evaluation frameworks specifically designed for maritime perception systems, hindering systematic comparison and technology advancement across research groups and commercial developers.

The absence of comprehensive quantitative performance analysis leaves fundamental technical and operational questions unanswered for \ac{ASV} system designers and engineers responsible for developing reliable autonomous navigation systems. Critical questions regarding detection and classification performance remain inadequately addressed in current research literature. Specifically, the comparative detection accuracy performance of \ac{LiDAR}-based systems utilizing \ac{GB-CACHE} processing versus vision-based systems implementing \ac{YOLO} algorithms for specific maritime object types requires systematic investigation. The precision and recall characteristics of each sensing modality across different object classes under varying environmental conditions need quantitative evaluation to inform sensor selection decisions. Training requirements, including data volume, computational resources, and convergence time, differ significantly between \ac{LiDAR} feature-based approaches and vision-based deep learning methodologies, yet these differences lack systematic quantification. Computational overhead associated with each sensing modality for real-time operation, including processing latency and resource utilization, requires comprehensive analysis to ensure practical deployment feasibility. Additionally, the effectiveness of sensor fusion methodologies, particularly bounding box confidence integration approaches, needs rigorous evaluation to determine optimal multi-modal processing strategies for enhanced detection performance.

% \subsection{Problem Statement}
\subsection{Problem Statement: Sensor Fusion Challenges}

Autonomous surface vessels require precise integration of multiple sensing modalities to achieve reliable object detection and classification performance that meets operational safety standards for autonomous navigation. A fundamental and persistent challenge in \ac{ASV} perception system development lies in establishing and maintaining accurate spatial and temporal calibration between \ac{LiDAR} and camera systems under dynamic maritime operational conditions that present unique environmental challenges not encountered in terrestrial applications.

Multi-sensor \ac{ASV} platforms face unique spatial calibration requirements that differ significantly from terrestrial applications due to the dynamic nature of maritime environments and the continuous mechanical stresses imposed by marine operational conditions. Environmental factors affecting calibration present ongoing challenges for maintaining sensor alignment accuracy. Platform motion induced by wave action creates continuous roll, pitch, and yaw movements that affect sensor alignment and require robust calibration maintenance strategies. Vibration and mechanical stress inherent in marine environments cause gradual calibration drift that can degrade sensor fusion performance over extended operational periods. Temperature variations in maritime environments affect sensor mounting structures and optical characteristics, potentially introducing systematic calibration errors that must be compensated through adaptive calibration procedures. Saltwater corrosion presents long-term challenges by potentially altering sensor mounting hardware characteristics over extended deployment periods, requiring regular calibration validation and maintenance protocols.

Precision requirements for effective sensor fusion establish demanding performance specifications for calibration maintenance systems that must operate reliably under dynamic maritime conditions. Sub-pixel accuracy calibration is essential for accurate \ac{LiDAR} point cloud projection onto camera images, enabling effective correlation between sensor modalities for object detection applications and ensuring that spatial relationships between sensors remain consistent across operational scenarios. Millimeter-level precision in spatial calibration is required for effective object detection correlation between \ac{LiDAR} and vision systems, particularly for small maritime objects such as navigation buoys where detection accuracy directly impacts navigation safety. Consistent calibration maintenance across varying operational conditions, including different sea states and weather conditions, requires robust calibration validation procedures that can adapt to changing environmental parameters. Real-time calibration validation capabilities are necessary for detecting calibration degradation during operation and implementing corrective measures to maintain sensor fusion performance without interrupting autonomous navigation operations.

\subsection{Limitations and Assumptions}

This research investigation is conducted using \ac{ERAU}'s Minion \ac{ASV} research platform and utilizes sensor data collected during the 2024 Maritime RobotX Challenge competition, which establishes specific operational and methodological constraints that define the scope and applicability of research findings.

Platform-specific limitations inherent in this research approach must be acknowledged and considered when interpreting results and their broader applicability. Research findings are specifically applicable to \ac{ERAU}'s Minion research platform design, including its particular sensor mounting configuration, platform dynamics characteristics, and operational capabilities, which may not directly translate to other \ac{ASV} platform designs. The competition environment context, where primary data collection occurred during RobotX challenge events, may not represent the full spectrum of maritime operational conditions encountered in commercial or military \ac{ASV} deployments. Geographic constraints imposed by conducting testing in competition and training areas introduce environmental characteristics that may not be representative of other maritime operational regions. Operational scenarios focused on RobotX challenge tasks may not encompass the complete range of potential \ac{ASV} mission requirements and environmental conditions encountered in practical autonomous vessel operations.

This research focuses on specific maritime object categories that are relevant to RobotX competition scenarios and representative of typical \ac{ASV} navigation challenges, while acknowledging that maritime environments contain additional object types not addressed in this investigation. The research addresses six primary object classes that represent critical navigation elements in maritime environments. Tall buoys, specifically Sur-Mark regulatory buoys with standardized dimensions of 39-inch height, 10-inch column diameter, and 18-inch ring diameter, represent regulatory navigation markers requiring reliable detection and classification. Polyform A-2 buoys, measuring 14.5 inches by 19.5 inches and available in red, green, blue, and black color variants, serve as channel markers and navigation references requiring color-specific classification capabilities. Polyform A-3 buoys, with dimensions of 17 inches by 23 inches and identical color availability, represent larger navigation buoys requiring robust detection across varying distances and environmental conditions. Light towers function as active navigation aids incorporating electronic and visual signaling capabilities, presenting detection challenges due to variable illumination states and complex geometric structures. Jon boats, characterized as flat-bottom chase boats utilized in competition and training scenarios, represent small vessel detection targets with distinct geometric and motion characteristics. Sailboats, including recreational sailing vessels commonly encountered in competition environments, represent larger vessel detection targets with variable configuration due to sail and rigging arrangements.

\textbf{Definitions of Terms}

\begin{itemize}[label={}]
    \item\textbf{Autonomous Surface Vessel (ASV)} An unmanned watercraft capable of independent navigation and task execution without direct human control, utilizing onboard sensors and computational systems for environmental perception and decision-making.
    
    \item\textbf{Clustering} A computational technique that groups data points with similar characteristics or spatial proximity to identify distinct objects or regions within complex datasets.

    \item\textbf{Grid-Based Clustering} A spatial data organization methodology that partitions three-dimensional point cloud data into regular grid structures to facilitate efficient clustering analysis and object identification within defined spatial regions.
    
    \item\textbf{Concave Hull} A geometric boundary that closely follows the shape of a point set by allowing inward curves, providing a more accurate representation of object boundaries compared to convex hull approaches.
    
    \item\textbf{You Only Look Once (YOLO)} A real-time object detection algorithm that processes entire images in a single forward pass through a convolutional neural network, simultaneously predicting bounding boxes and class probabilities for detected objects.
    
    \item\textbf{Sensor Fusion} The computational process of combining data from multiple sensors to produce more accurate, reliable, or comprehensive information than could be achieved using individual sensors independently.
    
    \item\textbf{Bounding Box} A rectangular region that defines the spatial boundaries of a detected object within an image or three-dimensional space, typically specified by corner coordinates or center point with width and height dimensions.
    
    \item\textbf{Confidence Integration} A methodology for combining detection results from multiple sensors by evaluating and integrating the confidence scores associated with object predictions to improve overall detection reliability.
    
    \item\textbf{Maritime RobotX Challenge} An international autonomous surface vessel competition that provides standardized testing scenarios and performance evaluation frameworks for ASV perception, navigation, and manipulation capabilities.
    
    \item\textbf{Real-time Processing} Computational processing that guarantees response within specified time constraints, typically requiring completion of detection and classification tasks within predetermined latency limits suitable for autonomous navigation safety requirements.
    
    \item\textbf{Light Detection and Ranging (LiDAR)} A remote sensing technology that uses laser pulses to measure distances and create detailed three-dimensional point cloud representations of environmental features and objects.
    
    \item\textbf{Point Cloud} A collection of data points in three-dimensional space representing the external surface of objects, typically generated by LiDAR sensors through distance measurements to environmental features.
\end{itemize}

\textbf{List of Acronyms}

\begin{itemize}[label={}]
    \item\textbf{ASV} Autonomous Surface Vessel
    \item\textbf{ERAU} Embry-Riddle Aeronautical University  
    \item\textbf{GB-CACHE} Grid-Based Clustering and Concave Hull Extraction
    \item\textbf{LiDAR} Light Detection and Ranging
    \item\textbf{RGB} Red, Green, Blue
    \item\textbf{ROS} Robot Operating System
    \item\textbf{YOLO} You Only Look Once
    \item\textbf{IoU} Intersection over Union
    \item\textbf{mAP} mean Average Precision
    \item\textbf{ROI} Region of Interest
    \item\textbf{GPS} Global Positioning System
    \item\textbf{IMU} Inertial Measurement Unit
\end{itemize}

\chapter{Review of the Relevant Literature}

% Note: All in-text citations should appear as \cite{einstein}

\acp{USV} have emerged as essential platforms capable of performing dangerous, dirty, and cumbersome tasks that exceed human capability. These vessels are pivotal in various maritime operations, including environmental monitoring, search and rescue missions, and resource exploration \cite{liebergall, eckstein2024}.% [1], [2]. 
Their ability to operate independently with minimal human intervention has significantly enhanced operational efficiency and safety at sea \cite{bai2022}.%[3].

Autonomous vehicles use a variety of sensors to perceive their surroundings but primarily rely on some combination of visual information through a camera and spatial data provided by \ac{LiDAR} \cite{yeong2021}.%[4].
Each sensing modality offers distinct advantages: visual data provides rich color and texture information, while \ac{LiDAR} delivers precise spatial measurements of the surrounding environment.
Real-time object detection methods have been developed for both sensing modalities, leveraging deep learning architectures.
Object detection with visual data often employs transfer learning on pre-trained convolutional neural networks such as ResNet \cite{he2016} and \ac{YOLO} \cite{ultralytics}.%[6].
Similarly, \ac{LiDAR}-based object detection can be performed using point-based algorithms like PointNet \cite{garcia-garcia2016}, voxel-based methods such as VoxelNet \cite{zhou2018a}, or hybrid approaches like PV-RCNN \cite{shi2021}.%[9].
Despite these advancements, each modality has inherent limitations—vision-based systems struggle with poor lighting conditions and occlusions, while \ac{LiDAR} data can be sparse and affected by water reflections.

To address these limitations, sensor fusion techniques have been explored as a means of combining the strengths of both modalities. 
Research into sensor fusion methods dates back to military applications in the 1980s and has gained significant traction in the last 15 years, particularly due to interest from the automotive industry in autonomous driving technologies. However, no unified approach has been established for optimal sensor fusion, with ongoing debates regarding the best fusion strategies (e.g., early, mid, or late fusion) and their trade-offs concerning computational efficiency and accuracy.

While research in the automotive sector has contributed significantly to sensor fusion methodologies \cite{yeong2021,clunie2021,roriz2022,cui2022,das2022,liu2023a}, direct application to maritime environments remains challenging due to fundamental environmental differences. 
Automotive environments are highly structured, with well-defined lanes, uniform object classes, and relatively predictable lighting conditions. 
In contrast, the maritime environment introduces additional complexities, including dynamic vehicle motion induced by wind and waves, variable scene density (ranging from sparse open waters to congested littoral zones), and specular reflections on the water surface that can interfere with both vision-based \cite{liu2023a} and \ac{LiDAR}-based object detection \cite{ahmed2024}.%[15]. 
These factors necessitate domain-specific adaptations of sensor fusion architectures to ensure robust real-time object detection for \acp{USV}. 
However, the lack of available maritime-specific datasets \cite{jun-hwa2022,su2023,thompson2023} creates an additional challenge.

Given these challenges, further research is needed to enhance sensor fusion methodologies for maritime applications. 
Key areas of investigation include efficient feature selection tailored to maritime object classes, the development of lightweight fusion architectures suited for real-time processing, and an evaluation of computational requirements for deployment on \ac{USV} hardware. 
Addressing these research gaps will contribute to the advancement of autonomous maritime perception, enhancing the operational capabilities of \acp{USV} in complex and dynamic environments.

\chapter{Sensing Platform}

    \section{USV Platform}

        \subsection{Sensors}
        
            \subsubsection{Perception Geometry}

The sensor configuration on the Minion \ac{ASV} was designed to maximize spatial and temporal overlap between vision and \ac{LiDAR} sensing modalities, enabling robust multi-modal object detection and sensor fusion.
The perception geometry emphasizes forward-facing coverage optimized for maritime navigation scenarios, where objects of interest primarily appear ahead of the vessel during transit operations.

The multi-modal perception system integrates complementary sensor types to leverage the strengths of each modality while compensating for individual limitations.
High dynamic range cameras provide dense pixel-level information with rich texture and color features, while \ac{LiDAR} sensors deliver precise three-dimensional spatial measurements independent of lighting conditions.
The geometric arrangement ensures that both sensor types observe the same spatial volume, a prerequisite for effective late fusion methodologies.

The perception system employs three Leopard Imaging IMX490 \ac{HDR} cameras arranged to provide overlapping horizontal coverage.
Each camera features a 65-degree horizontal field of view, positioned with strategic overlap to create a composite forward-facing perception envelope.
This configuration yields comprehensive visual coverage of the navigation corridor ahead of the vessel while maintaining sufficient overlap for stereoscopic depth estimation if required in future work.
Complementing the camera array, three Livox Horizon \ac{LiDAR} units provide spatial measurement coverage designed to maximize overlap with the camera field of view.
Each Livox Horizon features an 81.7° × 25.1° (horizontal × vertical) field of view.
The three units are positioned to deliver a combined horizontal coverage of approximately 165° × 25.1°, resulting in complete horizontal overlap with the camera system and approximately 77\% vertical overlap \cite{thompson2023}.

The decision to employ forward-facing \ac{LiDAR} sensors rather than traditional 360-degree rotating units reflects the research focus on sensor fusion performance.
Unlike mechanically rotating \ac{LiDAR} systems such as the Velodyne HDL-32E, which distribute returns across the full horizontal plane, the Livox Horizon units concentrate their sampling density within the forward-facing camera field of view.
This targeted approach delivers higher effective point cloud density in the region of interest, improving the spatial resolution available for object detection algorithms.
The Livox Horizon scan pattern differs fundamentally from traditional spinning \ac{LiDAR} designs.
Rather than sampling fixed vertical rings that repeat with each rotation, the Livox units employ a rosette-pattern scanning approach that progressively covers the entire field of view over a one-second integration period \cite{thompson2023}.
This non-repetitive scan pattern yields substantially higher spatial sampling density when multiple scans are temporally aggregated, as detailed in Section 3.1.1.3.

All sensor measurements are registered to a common spatial reference frame centered on the vessel platform.
Precise geometric relationships between individual sensors are established through extrinsic calibration procedures detailed in Section 3.2.1.
The \ac{LiDAR} units additionally benefit from factory-configured inter-sensor calibration, allowing all three Livox devices to report points from a unified origin corresponding to the center sensor position.
Position and orientation of the platform itself is provided by a Pinpoint \ac{GPS}/\ac{INS} system, enabling transformation of sensor observations between the vessel reference frame and inertial (world) coordinates.
This capability proves essential for temporal aggregation of \ac{LiDAR} observations during vessel motion, as described in the synchronization methodology of Section 3.2.2.

The perception geometry reflects several key design priorities specific to maritime autonomous navigation research.
Maximum spatial overlap between vision and \ac{LiDAR} modalities enables direct comparison of detection performance and validates late fusion approaches, while the forward-facing emphasis addresses the reality that maritime collision avoidance and navigation planning require detailed perception of the forward corridor, making 360-degree coverage unnecessary for this research application.
Concentrating \ac{LiDAR} sampling within the camera field of view provides higher effective point cloud density compared to omnidirectional sensors with equivalent point return rates.
Furthermore, the Livox scan pattern combined with \ac{GPS}/\ac{INS} integration enables temporal accumulation of observations during vessel motion, further increasing effective spatial resolution.
The resulting sensor configuration provides the geometric foundation for the comparative object detection performance analysis that forms the core of this research.
Detailed specifications of individual sensor components are presented in the following subsections.

            \subsubsection{HDR Camera}

The vision component of the Minion perception system employs three Leopard Imaging LI-USB30-IMX490-GW5400-GMSL2-065H \ac{HDR} cameras.
These cameras were selected for their capability to handle the challenging lighting conditions characteristic of maritime environments, where high-contrast scenes with bright sky, reflective water surfaces, and shadowed objects demand extended dynamic range to preserve visual detail across the luminance spectrum \cite{thompson2023}.

The Leopard Imaging IMX490 camera system is built around Sony's automotive-grade image sensor, designed for advanced driver assistance systems applications with similar environmental challenges to maritime perception.
Table 3.1 presents the key specifications of the camera hardware.

\begin{table}[h]
\centering
\caption{Leopard Imaging IMX490 HDR Camera Specifications}
\begin{tabular}{ll}
\hline
\textbf{Parameter} & \textbf{Value} \\
\hline
Model & LI-USB30-IMX490-GW5400-GMSL2-065H \\
Horizontal Resolution & 2880 pixels \\
Vertical Resolution & 1860 pixels \\
Aspect Ratio & 1.55:1 \\
Maximum Frame Rate & 25 fps \\
Horizontal Field of View & 65° \\
Interface & USB 3.0 (UVC) \\
HDR Capability & 120 dB \\
\hline
\end{tabular}
\label{tab:hdr_camera_specs}
\end{table}

The 2880×1860 pixel resolution provides sufficient spatial detail for object detection at ranges relevant to autonomous surface vessel navigation, while the 25 frames-per-second maximum frame rate exceeds the temporal sampling requirements for typical vessel speeds and maneuverability constraints.
The 65-degree horizontal field of view, when deployed across three cameras with overlapping coverage, delivers the wide forward-facing perception envelope described in Section 3.1.1.1.

The IMX490 camera employs a two-stage interface architecture that separates the camera head from image signal processing.
The camera sensor communicates via a Gigabit Multimedia Serial Link 2 interface to a secondary enclosure containing dedicated image signal processing hardware.
This image signal processing unit, manufactured by Leopard Imaging, performs debayering, white balance, gamma correction, and \ac{HDR} tone mapping in a fixed processing pipeline with no end-user configurable parameters \cite{thompson2023}.
The image signal processing enclosure provides a USB 3.0 interface configured to operate as a Universal Video Class device.
Universal Video Class compliance ensures compatibility with standard operating system video capture interfaces without requiring proprietary drivers.
In the Ubuntu Linux environment deployed on the Minion platform, the cameras are accessed through the Video for Linux version 2 subsystem, which provides a standardized kernel-level interface for video capture devices.

Video capture from the \ac{HDR} cameras is implemented using the GStreamer multimedia framework, an industry-standard pipeline-based media processing library.
GStreamer's modular architecture enables construction of complex video processing pipelines from individual functional elements connected in directed graphs.
The \texttt{v4l2src} element interfaces with the Video for Linux version 2 kernel subsystem to acquire raw image frames from the camera, which are then processed through a pipeline incorporating format conversion, encoding, and streaming elements.
The standard GStreamer video pipeline operates with timestamps relative to the start of the stream, which is insufficient for multi-sensor fusion applications requiring absolute time references for temporal alignment.
To address this limitation, a custom GStreamer plugin was developed to inject absolute system timestamps into the video stream.
This timestamp plugin is described in detail in Section 3.2.2.2, where the Supplemental Enhancement Information timestamp embedding methodology is presented.

The custom timestamp plugin operates as a GStreamer pad probe attached to the source pad of the \texttt{v4l2src} element \cite{thompson2023}.
Upon receiving each new image buffer from the camera, the plugin samples the system time via the POSIX \texttt{gettimeofday()} function, capturing the Unix epoch timestamp in milliseconds.
This timestamp is then associated with the corresponding video frame through external comma-separated value logging, where the frame number and timestamp are written to a file creating a frame-to-time lookup table external to the video stream, or through in-band embedding, where the timestamp is embedded directly into the video bitstream as metadata that persists through encoding and streaming operations.
The in-band timestamp embedding approach, detailed in Section 3.2.2.2, provides superior robustness by ensuring that timing information cannot be separated from the associated image data during recording, playback, or network transmission.
This capability proves essential for the temporal drift analysis and correction procedures described in Section 3.2.2.3.

Accurate timestamps require that the system clock from which they are derived maintains synchronization with the global time reference shared by all sensors on the platform.
The \ac{HDR} camera system operates on the NVIDIA Jetson Xavier embedded computer, which synchronizes its system clock via Network Time Protocol with the Atlas PC main computing platform.
The Atlas PC, in turn, derives time from the Pinpoint \ac{GPS} receiver, establishing a hierarchical time distribution architecture detailed in Section 3.2.2.1.
Before launching the video capture pipeline, the camera enclosure software verifies that Network Time Protocol synchronization is established and that the system clock has been updated to \ac{GPS} time.
Only after successful verification does the system begin video streaming, ensuring that all timestamps reflect \ac{GPS}-disciplined time from the first captured frame.
This verification procedure prevents the recording of data with uncalibrated timestamps, which would compromise subsequent temporal alignment operations.

The maritime operating environment presents several challenges that informed the camera selection and deployment strategy.
The high dynamic range capability addresses the extreme brightness variations between sky, water, and shadowed vessel structures, while the automotive-grade sensor provides environmental sealing and extended operating temperature range suitable for outdoor deployment.
The USB 3.0 interface enables placement of the camera sensor in weatherproof housing separate from computing hardware, with up to 5-meter cable runs supported by the USB 3.0 specification.
The three cameras are mounted in individual weatherproof enclosures integrated into the camera enclosure assembly described in Section 3.1.2.2.
This assembly provides power distribution, environmental protection, and network connectivity for all three cameras while maintaining the geometric relationships required for the perception geometry configuration.

The IMX490 sensor's native \ac{HDR} capability combines multiple exposures within a single frame acquisition period, delivering 120 dB dynamic range without requiring sequential multi-exposure capture.
This simultaneous \ac{HDR} approach avoids motion artifacts that would occur with temporal exposure bracketing, particularly important for a mobile platform operating in dynamic maritime environments with moving targets.
Image quality validation and intrinsic calibration procedures are described in Section 3.2.1.1, where lens distortion characterization and camera matrix estimation are documented.
The combination of wide dynamic range, moderate resolution, and calibrated geometric parameters provides the image quality necessary for training and evaluating vision-based object detection models as presented in Chapter 5.

The camera hardware and integration methodology described in this section, originally developed by \cite{thompson2023}, provides a proven foundation for maritime perception research.
The addition of in-band timestamp embedding and temporal drift correction, detailed in Section 3.2.2, represents the primary extension to this baseline system required for rigorous multi-modal sensor fusion analysis.

            \subsubsection{Livox Horizon}

The spatial sensing component of the Minion perception system employs three Livox Horizon solid-state \ac{LiDAR} units.
These sensors were selected to replace the existing Velodyne mechanical spinning \ac{LiDAR} sensors based on superior point cloud density within the camera field of view and a non-repetitive scan pattern that improves spatial coverage through temporal aggregation \cite{thompson2023}.

Initial planning for the multi-modal perception system anticipated utilizing the Velodyne HDL-32E and VLP-16 sensors already installed on the Minion platform.
However, analysis of the point cloud density achievable within the camera field of view revealed fundamental limitations of the mechanically rotating \ac{LiDAR} architecture when applied to sensor fusion applications.
Traditional spinning \ac{LiDAR} sensors such as the Velodyne HDL-32E employ a fixed vertical array of laser emitters that rotates horizontally at constant angular velocity.
The HDL-32E, for example, features 32 laser channels arranged in a vertical array spanning approximately 40 degrees of elevation.
This array rotates at 10 Hz, producing 32 discrete horizontal rings of point returns distributed across the full 360-degree azimuthal range \cite{thompson2023}.

While this omnidirectional coverage provides comprehensive spatial awareness, it presents significant limitations for camera-\ac{LiDAR} fusion.
Sparse azimuthal sampling means that for a stationary or slow-moving platform, each of the 32 lasers repeatedly samples the same azimuthal positions on successive rotations, yielding no increase in spatial sampling density over time.
Furthermore, inefficient field of view utilization occurs because the full 360-degree coverage distributes the sensor's point return budget across the entire horizontal plane, allocating only a fraction of total returns to the forward-facing camera field of view.
These limitations motivated the selection of an alternative \ac{LiDAR} technology optimized for forward-facing perception with temporal aggregation capabilities.

The Livox Horizon employs a non-repetitive rosette scan pattern that progressively covers its entire field of view over an integration period rather than repeatedly sampling fixed positions.
The sensor features an 81.7° × 25.1° (horizontal × vertical) field of view, substantially wider in azimuth than the 65-degree horizontal field of view of individual \ac{HDR} cameras.
Three Livox Horizon units are deployed to provide a combined horizontal coverage of approximately 165° × 25.1°, encompassing the full camera array field of view with complete horizontal overlap and approximately 77\% vertical overlap \cite{thompson2023}.
Table 3.2 presents the key specifications of the Livox Horizon \ac{LiDAR} system.

\begin{table}[h]
\centering
\caption{Livox Horizon LiDAR Specifications}
\begin{tabular}{ll}
\hline
\textbf{Parameter} & \textbf{Value} \\
\hline
Model & Livox Horizon \\
Horizontal Field of View & 81.7° \\
Vertical Field of View & 25.1° \\
Range & 260 m @ 80\% reflectivity \\
Point Rate (Single Return) & 240,000 pts/sec \\
Point Rate (Dual Return) & 480,000 pts/sec \\
Range Precision & ±2 cm \\
Wavelength & 905 nm \\
Scan Pattern & Non-repetitive rosette \\
Interface & UDP (Ethernet) \\
Operating Frequency & 100 Hz \\
\hline
\end{tabular}
\label{tab:livox_horizon_specs}
\end{table}

The 905 nm near-infrared wavelength is absorbed by water, resulting in few or no returns from water surfaces—a characteristic that proves advantageous for maritime object detection by reducing clutter from wave surfaces that would otherwise generate false positives in clustering-based detection algorithms.

The fundamental distinction between the Livox Horizon and traditional spinning \ac{LiDAR} lies in the scan pattern geometry.
A mechanically spinning \ac{LiDAR} with N laser channels produces N discrete horizontal rings that repeat identically with each rotation period.
For a vessel traveling at typical autonomous surface vehicle speeds (2-5 m/s), the platform displacement between successive rotations remains small relative to object dimensions, resulting in near-perfect overlap of consecutive scans within the platform reference frame.
The Livox Horizon, by contrast, employs a two-dimensional scanning mechanism that traces a non-repetitive rosette pattern across the field of view.
This pattern is generated by two orthogonal scanning mirrors operating at slightly incommensurate frequencies, causing the laser beam to trace a complex Lissajous-like trajectory that progressively fills the sensor's field of view without repeating \cite{thompson2023}.

The scan pattern coverage evolves over time, with denser spatial sampling achieved through longer integration periods.
At the standard 100 Hz data output rate, each UDP message contains approximately 10 milliseconds of \ac{LiDAR} returns.
Over a one-second aggregation period, the rosette pattern provides substantially higher spatial coverage than achievable with a spinning \ac{LiDAR}'s fixed ring geometry.
Figure 3.7 in \cite{thompson2023} demonstrates this density advantage by projecting one second of aggregated Livox point cloud data onto an \ac{HDR} camera image, revealing dense coverage across the image plane with few gaps.
This density proves critical for the grid-based clustering algorithm employed for \ac{LiDAR} object detection, as described in Chapter 5.

Quantitative comparison of effective point cloud density within the camera field of view reveals the advantage of the Livox configuration.
A Velodyne HDL-32E operating in dual-return mode generates approximately 1.4 million points per second distributed across the full 360-degree horizontal plane.
With the camera field of view occupying roughly 165 degrees of this range, the effective point rate within the region of interest is:

$$\text{Effective HDL-32E rate} = 1.4 \times 10^6 \times \frac{165°}{360°} \approx 641,000 \text{ pts/sec}$$

Three Livox Horizon units operating in dual-return mode collectively produce:

$$\text{Combined Livox rate} = 3 \times 480,000 = 1.44 \times 10^6 \text{ pts/sec}$$

This represents a 2.25× increase in point returns within the camera field of view, even before accounting for the superior vertical sampling density afforded by the non-repetitive scan pattern versus fixed 32-ring geometry \cite{thompson2023}.

The non-repetitive scan pattern delivers maximum benefit when multiple scans are temporally aggregated, accumulating points over an integration window to increase spatial sampling density.
For a moving platform, this aggregation requires compensation for vehicle motion during the accumulation period.
The \ac{LiDAR} aggregation methodology, detailed in Chapter 5, employs the \ac{GPS}/\ac{INS} system to track platform pose as a function of time.
Each \ac{LiDAR} point is transformed from the sensor reference frame to an inertial world reference frame using the platform pose at the point's acquisition timestamp.
Points accumulated over the integration window are then transformed from the inertial frame to the reference frame of the target camera image, enabling dense point cloud overlay on the corresponding image.
For this research, a four-second integration period was selected based on empirical evaluation of point cloud density versus motion blur effects.
Each camera image sampled at 1 Hz is accompanied by all \ac{LiDAR} points acquired during the preceding four seconds, transformed to the image capture time reference frame.
This aggregation strategy provides the spatial resolution necessary for reliable object detection while maintaining acceptable motion compensation accuracy given the platform's typical velocity profile.

The Livox Horizon sensors communicate via User Datagram Protocol over Ethernet, transmitting point cloud data as binary messages at 100 Hz.
Livox provides an open-source \ac{ROS} software development kit that handles network communication, message parsing, and point cloud publication to \ac{ROS} topics.
Integration of the Livox SDK into the Minion software architecture required minimal modification, primarily involving configuration of network parameters and adjustment of \ac{ROS} launch files.
The SDK was deployed on the Atlas PC main computing platform, where it receives UDP messages from all three \ac{LiDAR} units simultaneously over the local area network described in Section 3.1.2.3.

Accurate fusion of data from multiple \ac{LiDAR} units requires precise knowledge of the geometric relationship between sensors.
The Livox Horizon units support onboard storage of extrinsic calibration parameters, enabling each sensor to transform its measurements to a common reference frame before transmission.
The \ac{LiDAR}-to-\ac{LiDAR} extrinsic calibration procedure, described in Section 3.2.1.3, determines the six-degree-of-freedom transformation (three-dimensional translation and three-dimensional rotation) relating each sensor's local coordinate system to that of the designated primary sensor.
These transformation parameters are written to non-volatile memory on each Livox unit, configuring the sensors to automatically apply the transformation such that all point clouds are broadcast in the reference frame of the center \ac{LiDAR} \cite{thompson2023}.
This factory-configured transformation simplifies downstream processing by eliminating the need for software-based point cloud registration of data from the three individual sensors.
The unified point cloud is then related to the camera coordinate systems through camera-to-\ac{LiDAR} extrinsic calibration, as detailed in Section 3.2.1.2.

Temporal alignment of \ac{LiDAR} measurements with camera images and \ac{GPS}/\ac{INS} pose data requires that all \ac{LiDAR} points be timestamped with reference to a common global time base.
The Livox Horizon units support IEEE 1588 Precision Time Protocol, enabling sub-microsecond clock synchronization when operating on a network with a Precision Time Protocol master clock source.
In the Minion architecture, the NVIDIA Jetson Xavier embedded computer in the camera enclosure operates as the Precision Time Protocol master clock, providing synchronization to the three \ac{LiDAR} units.
The Jetson itself synchronizes to \ac{GPS} time via a hierarchical Network Time Protocol architecture, as detailed in Section 3.2.2.1.
This cascaded synchronization ensures that \ac{LiDAR} timestamps share the same \ac{GPS}-disciplined time reference as camera frame timestamps, enabling frame-accurate temporal alignment for sensor fusion.
Verification of Precision Time Protocol synchronization status is performed using the Livox Viewer software application, which displays the synchronization state and time source information for each connected sensor.
Proper Precision Time Protocol lock is confirmed before data collection operations, ensuring temporal accuracy of all recorded data.

An important characteristic of the 905 nm wavelength employed by the Livox Horizon is strong absorption by water.
This physical property results in minimal \ac{LiDAR} returns from water surfaces, in contrast to vision sensors that clearly image water texture and wave patterns.
The absence of water surface returns reduces point cloud clutter and simplifies object detection by eliminating the need to filter ground plane points corresponding to the water surface.
This behavior is visible in point cloud visualizations overlaid on camera images, where dense \ac{LiDAR} coverage appears on vessels, buoys, and other solid objects while the water surface yields few or no returns.
For the grid-based clustering algorithm employed in this research, the absence of water returns proves advantageous by preventing false positive detections of wave crests or foam patterns as discrete objects.

The Livox Horizon \ac{LiDAR} configuration provides several key capabilities that enable the comparative object detection performance analysis central to this research.
The system delivers 2.25× more points within the camera field of view compared to omnidirectional spinning \ac{LiDAR} alternatives, while the non-repetitive scan pattern increases sampling density through temporal aggregation.
Per-point timestamps and \ac{GPS}/\ac{INS} integration enable accurate aggregation during platform motion, while Precision Time Protocol synchronization provides sub-microsecond timing accuracy relative to camera and \ac{GPS} sensors.
Factory-configured multi-sensor calibration with onboard extrinsic parameters unifies point clouds from three sensors before transmission, and the 905 nm wavelength characteristics reduce clutter in maritime environments by rejecting water surface returns.
These characteristics, combined with the software integration and synchronization infrastructure described in Section 3.2, provide the foundation for rigorous evaluation of \ac{LiDAR}-based object detection performance and comparison with vision-based approaches as presented in Chapters 5 and 6.

            \subsubsection{Pinpoint GPS/INS}
            
        \subsection{Compute and LAN}
        
    \section{Data Collection}
    
        \subsection{Calibration}
        
            \subsubsection{Camera Intrinsics}
            
            \subsubsection{Camera Extrinsics}
            
            \subsubsection{Livox Intrinsics}
            
        \subsection{Synchronization}
        
            \subsubsection{Clock Synchronization}
            
            \subsubsection{Video Pipeline}
            
            \subsubsection{Temporal Drift}
            
    \section{Data Output}

\chapter{Dataset}

\chapter{Real-time Object Detection}

\chapter{Late Fusion}

\chapter{Conclusions}

% This chapter will synthesize findings from all three research objectives:
% - Summary of comparative performance results between LiDAR and vision systems
% - Calibration and synchronization framework effectiveness
% - Real-time processing capability validation
% - Implications for ASV perception system design
% - Contribution to maritime autonomous systems knowledge

\section{Research Objective Achievement Summary}
% Placeholder for objective completion summary

\section{Performance Comparison Findings}
% Placeholder for key comparative analysis conclusions

\section{Implications for ASV System Design}
% Placeholder for practical design guidance conclusions

\chapter{Recommendations and Future Work}

% This chapter will address:
% - Recommendations for ASV perception system design based on findings
% - Sensor selection guidance for maritime applications
% - Future research directions for maritime sensor fusion
% - Technology transfer opportunities to operational systems

\section{ASV Perception System Design Recommendations}
% Placeholder for design guidance recommendations

\section{Future Research Directions}
% Placeholder for future work recommendations

\section{Technology Transfer Opportunities}
% Placeholder for practical application recommendations




% \printbibliography
\bibliographystyle{plainnat}
% \bibliography{References}
\bibliography{Dissertation}

\backmatter

\chapter{A Test of the Appendix System}

Tables of Results

\chapter{Another Test of the Appendix System}
Supplemental Figures.
\end{document}

